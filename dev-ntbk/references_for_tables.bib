@inproceedings{001saha2019lastmeter,
author = {Saha, Manaswi and Fiannaca, Alexander J. and Kneisel, Melanie and Cutrell, Edward and Morris, Meredith Ringel},
title = {Closing the Gap: Designing for the Last-Few-Meters Wayfinding Problem for People with Visual Impairments},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353776},
doi = {10.1145/3308561.3353776},
abstract = {Despite the major role of Global Positioning Systems (GPS) as a navigation tool for people with visual impairments (VI), a crucial missing aspect of point-to-point navigation with these systems is the last-few-meters wayfinding problem. Due to GPS inaccuracy and inadequate map data, systems often bring a user to the vicinity of a destination but not to the exact location, causing challenges such as difficulty locating building entrances or a specific storefront from a series of stores. In this paper, we study this problem space in two parts: (1) A formative study (N=22) to understand challenges, current resolution techniques, and user needs; and (2) A design probe study (N=13) using a novel, vision-based system called Landmark AI to understand how technology can better address aspects of this problem. Based on these investigations, we articulate a design space for systems addressing this challenge, along with implications for future systems to support precise navigation for people with VI.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {222–235},
numpages = {14},
keywords = {wayfinding, landmarks, blindness, accessibility},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{002yang2011spatialstudy,
author = {Yang, Rayoung and Park, Sangmi and Mishra, Sonali R. and Hong, Zhenan and Newsom, Clint and Joo, Hyeon and Hofer, Erik and Newman, Mark W.},
title = {Supporting spatial awareness and independent wayfinding for pedestrians with visual impairments},
year = {2011},
isbn = {9781450309202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2049536.2049544},
doi = {10.1145/2049536.2049544},
abstract = {Much of the information designed to help people navigate the built environment is conveyed through visual channels, which means it is not accessible to people with visual impairments. Due to this limitation, travelers with visual impairments often have difficulty navigating and discovering locations in unfamiliar environments, which reduces their sense of independence with respect to traveling by foot. In this paper, we examine how mobile location-based computing systems can be used to increase the feeling of independence in travelers with visual impairments. A set of formative interviews with people with visual impairments showed that increasing one's general spatial awareness is the key to greater independence. This insight guided the design of Talking Points 3 (TP3), a mobile location-aware system for people with visual impairments that seeks to increase the legibility of the environment for its users in order to facilitate navigating to desired locations, exploration, serendipitous discovery, and improvisation. We conducted studies with eight legally blind participants in three campus buildings in order to explore how and to what extent TP3 helps promote spatial awareness for its users. The results shed light on how TP3 helped users find destinations in unfamiliar environments, but also allowed them to discover new points of interest, improvise solutions to problems encountered, develop personalized strategies for navigating, and, in general, enjoy a greater sense of independence.},
booktitle = {The Proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {27–34},
numpages = {8},
keywords = {wayfinding, accessibility},
location = {Dundee, Scotland, UK},
series = {ASSETS '11}
}

@inproceedings{003cheraghi2019cityguide,
author = {Cheraghi, Seyed Ali and Almadan, Ali and Namboodiri, Vinod},
title = {CityGuide: A Seamless Indoor-Outdoor Wayfinding System for People With Vision Impairments},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3354621},
doi = {10.1145/3308561.3354621},
abstract = {GPS accuracy is poor in indoor environments and around buildings. Thus, reading and following signs still remains the most common mechanism for providing and receiving wayfinding information in such spaces. This puts individuals who are blind or visually impaired (BVI) at a great disadvantage. This work designs, implements, and evaluates a wayfinding system and smartphone application called CityGuide that can be used by BVI individuals to navigate their surroundings beyond what is possible with just a GPS-based system. CityGuide enables an individual to query and get turn-by-turn shortest route directions from an indoor location to an outdoor location. CityGuide leverages recently developed indoor wayfinding solutions in conjunction with GPS signals to provide a seamless indoor-outdoor navigation and wayfinding system that guides a BVI individual to their desired destination through the shortest route. Evaluations of CityGuide with BVI human subjects navigating between an indoor starting point to an outdoor destination within an unfamiliar university campus scenario showed it to be effective in reducing end-to-end navigation times and distances of almost all participants.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {542–544},
numpages = {3},
keywords = {vision impairments, navigation and wayfinding, accessibility},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@incollection{004zimmermannjanschitz2019gis,
author = {Susanne Zimmermann-Janschitz},
title = {The Application of Geographic Information Systems to Support Wayfinding for People with Visual Impairments or Blindness},
booktitle = {Visual Impairment and Blindness},
publisher={IntechOpen London, UK},
address = {Rijeka},
year = {2019},
pages = {301-319},
editor = {Giuseppe Lo Giudice and Angel Catalá},
chapter = {18},
doi = {10.5772/intechopen.89308},
url = {https://doi.org/10.5772/intechopen.89308}
}

@Article{005ko2017situation,
AUTHOR = {Ko, Eunjeong and Kim, Eun Yi},
TITLE = {A Vision-Based Wayfinding System for Visually Impaired People Using Situation Awareness and Activity-Based Instructions},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {8},
ARTICLE-NUMBER = {1882},
NUMPAGES = {34},
URL = {https://www.mdpi.com/1424-8220/17/8/1882},
PubMedID = {28813033},
ISSN = {1424-8220},
ABSTRACT = {A significant challenge faced by visually impaired people is ‘wayfinding’, which is the ability to find one’s way to a destination in an unfamiliar environment. This study develops a novel wayfinding system for smartphones that can automatically recognize the situation and scene objects in real time. Through analyzing streaming images, the proposed system first classifies the current situation of a user in terms of their location. Next, based on the current situation, only the necessary context objects are found and interpreted using computer vision techniques. It estimates the motions of the user with two inertial sensors and records the trajectories of the user toward the destination, which are also used as a guide for the return route after reaching the destination. To efficiently convey the recognized results using an auditory interface, activity-based instructions are generated that guide the user in a series of movements along a route. To assess the effectiveness of the proposed system, experiments were conducted in several indoor environments: the sit in which the situation awareness accuracy was 90% and the object detection false alarm rate was 0.016. In addition, our field test results demonstrate that users can locate their paths with an accuracy of 97%.},
DOI = {10.3390/s17081882}
}

@Article{006prandi2023mapping,
author={Prandi, Catia and Barricelli, Barbara Rita and Mirri, Silvia and Fogli, Daniela},
title={Accessible wayfinding and navigation: a systematic mapping study},
journal={Universal Access in the Information Society},
year={2023},
month={Mar},
day={01},
volume={22},
number={1},
pages={185-212},
abstract={Urban environments, university campuses, and public and private buildings often present architectural barriers that prevent people with disabilities and special needs to move freely and independently. This paper presents a systematic mapping study of the scientific literature proposing devices, and software applications aimed at fostering accessible wayfinding and navigation in indoor and outdoor environments. We selected 111 out of 806 papers published in the period 2009--2020, and we analyzed them according to different dimensions: at first, we surveyed which solutions have been proposed to address the considered problem; then, we analyzed the selected papers according to five dimensions: context of use, target users, hardware/software technologies, type of data sources, and user role in system design and evaluation. Our findings highlight trends and gaps related to these dimensions. The paper finally presents a reflection on challenges and open issues that must be taken into consideration for the design of future accessible places and of related technologies and applications aimed at facilitating wayfinding and navigation.},
issn={1615-5297},
doi={10.1007/s10209-021-00843-x},
url={https://doi.org/10.1007/s10209-021-00843-x},
}

@article{007chana2017travelaid,
author = {Piyush Chanana and Rohan Paul and M Balakrishnan and PVM Rao},
title ={Assistive technology solutions for aiding travel of pedestrians with visual impairment},
journal = {Journal of Rehabilitation and Assistive Technologies Engineering},
volume = {4},
number = {},
pages = {2055668317725993},
year = {2017},
doi = {10.1177/2055668317725993},
note ={PMID: 31186934},
URL = { https://doi.org/10.1177/2055668317725993 },
eprint = { https://doi.org/10.1177/205566817725993 },
abstract = { This work systematically reviews the assistive technology solutions for pedestrians with visual impairment and reveals that most of the existing solutions address a specific part of the travel problem. Technology-centered approach with limited focus on the user needs is one of the major concerns in the design of most of the systems. State-of-the-art sensor technology and processing techniques are being used to capture details of the surrounding environment. The real challenge is in conveying this information in a simplified and understandable form especially when the alternate senses of hearing, touch, and smell have much lesser perception bandwidth than that of vision. A lot of systems are at prototyping stages and need to be evaluated and validated by the real users. Conveying the required information promptly through the preferred interface to ensure safety, orientation, and independent mobility is still an unresolved problem. Based on observations and detailed review of available literature, the authors proposed that holistic solutions need to be developed with the close involvement of users from the initial to the final validation stages. Analysis reveals that several factors need serious consideration in the design of such assistive technology solutions. },
}

@ARTICLE{008parker2021/survey,
AUTHOR={Parker, Amy T. and Swobodzinski, Martin and Wright, Julie D. and Hansen, Kyrsten and Morton, Becky and Schaller, Elizabeth},   
TITLE={Wayfinding Tools for People With Visual Impairments in Real-World Settings: A Literature Review of Recent Studies},      
JOURNAL={Frontiers in Education},      
VOLUME={6},
ARTICLE-NUMBER={723816},
NUMPAGES={23},
YEAR={2021},      
URL={https://www.frontiersin.org/articles/10.3389/feduc.2021.723816},       
DOI={10.3389/feduc.2021.723816},      
ISSN={2504-284X},   
ABSTRACT={A review of 35 peer reviewed articles dated from 2016 to February, 2021 was conducted to identify and describe the types of wayfinding devices that people who are blind, visually impaired or deafblind use while navigating indoors and/or outdoors in dynamic travel contexts. Within this investigation, we discovered some characteristics of participants with visual impairments, routes traveled, and real-world environments that have been included in recent wayfinding research as well as information regarding the institutions, agencies, and funding sources that enable these investigations. Results showed that 33 out of the 35 studies which met inclusionary criteria integrated the use of smart device technology. Many of these devices were supplemented by bluetooth low-energy beacons, and other sensors with more recent studies integrating LIDAR scanning. Identified studies included scant information about participant’s visual acuities or etiologies with a few exceptions, which limits the usability of the findings for this highly heterogeneous population. Themes derived from this study are categorized around the individual traveler’s needs; the wayfinding technologies identified and their perceived efficacy; the contexts and routes for wayfinding tasks; and the institutional support offered for sustaining wayfinding research.}
}

@InProceedings{009giannoumis2018network,
author="Giannoumis, G. A.
and Ferati, M.
and Pandya, U.
and Krivonos, D.
and Pey, T.",
editor="Langdon, Pat
and Lazar, Jonathan
and Heylighen, Ann
and Dong, Hua",
title="Usability of Indoor Network Navigation Solutions for Persons with Visual Impairments",
booktitle="Breaking Down Barriers",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="135--145",
abstract="The United Nations (UN) Convention on the Rights of Persons with Disabilities (CRPD) obligates States Parties to ensure personal mobility and independence for persons with disabilities by promoting access to and the development of assistive technology (AT)---i.e. products and services that enhance daily living and quality of life for persons with disabilities. Research has examined the experiences of persons with different disabilities using ICT and AT for indoor navigation and wayfinding. However, in the last year, ICT developers have made substantial strides in deploying Internet of Things (IoT) devices as part of indoor network navigation solutions (INNS) for persons with visual impairments. This article asks, `To what extent do persons with visual impairments perceive INNS as usable?' Quantitative and qualitative data from an experimental trial conducted with 36 persons with visual impairments shows that persons with visual impairments largely consider INNS as usable for wayfinding in transportation stations. However, the results also suggest that persons with visual impairments experienced barriers using INNS due to the timing of the instructions. Future research should continue to investigate the usability of INNS for persons with visual impairments and focus specifically on reliability and responsivity of the instruction timing.",
isbn="978-3-319-75028-6",
doi="10.1007/978-3-319-75028-6_12"
}

@article{010rodriguezsanchez2014smartphone,
title = {Accessible smartphones for blind users: A case study for a wayfinding system},
journal = {Expert Systems with Applications},
volume = {41},
number = {16},
pages = {7210-7222},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2014.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S095741741400311X},
author = {M.C. Rodriguez-Sanchez and M.A. Moreno-Alvarez and E. Martin and S. Borromeo and J.A. Hernandez-Tamames},
keywords = {Accessibility, Visually impaired users, User interfaces, Touch screens, Wayfinding},
abstract = {While progress on assistive technologies have been made, some blind users still face several problems opening and using basic functionalities when interacting with touch interfaces. Sometimes, people with visual impairments may also have problems navigating autonomously, without personal assistance, especially in unknown environments. This paper presents a complete solution to manage the basic functions of a smartphone and to guide users using a wayfinding application. This way, a blind user could go to work from his home in an autonomous way using an adaptable wayfinding application on his smartphone. The wayfinding application combines text, map, auditory and tactile feedback for providing the information. Eighteen visually impaired users tested the application. Preliminary results from this study show that blind people and limited vision users can effectively use the wayfinding application without help. The evaluation also confirms the usefulness of extending the vibration feedback to convey distance information as well as directional information. The validation was successful for iOS and Android devices.}
}

@inproceedings{011afrooz2012urban, 
author= {Eslami Afrooz, Aida and Hanaee, Toktam and Parolin, Bruno}, 
booktitle= {Proceedings REALCORP 2012 Tagungsband}, 
note= {Respubs id = 201262729 Respubs category = E1}, 
organization= {Schwechat, Austria}, 
pages= {1081--1091}, 
school= {Schwechat, Austria}, 
address={Schwechat},
publisher={REALCORP},
title= {Wayfinding Performance of Visually Impaired Pedestrians in an Urban Area},
year= {2012}, 
abstract= {Movement from an origin to a destination within a city is an inevitable activity for all inhabitants, especially for those who are commuters. For the visually impaired, this movement task is a difficult activity, given inability to use visual properties and hence reliance on hearing and smell for navigation. This study seeks to determine the type of information that is acquired by the visually impaired to navigate from an origin to a destination. In essence the study is attempting to determine the wayfinding process in familiar environments among the visually impaired. An experiment with 12 totally visually impaired and 12 partially visually impaired students was conducted in Mashhad city. Our method is based on an analysis of wayfinding from a school for the visually impaired to a familiar destination in the urban area of the city. Questionnaire survey methods were used to determine reference points, which senses (hearing, touch, smell) were used and problems experienced in reaching the destination by walking. The key findings show that there are differences between the two groups in terms of their use of reference points, use of the senses and problems encountered on the wayfinding trip to the destination. The totally visually impaired displayed a reliance on touch, smell and hearing for gaining information from the environment, as opposed to the partially visually impaired who could rely on sight and other senses for their information. As a result of the study it is suggested that those who design aids for the visually impaired should have stronger experiences of the perceptions of the needs and problems encountered by the visually impaired during the wayfinding process. For urban planners and designers the results suggest the need for greater consideration of the problems and needs of the visually impaired in terms of street layout and pattern, pavement slope and material, and safety and security.}, 
startyear= {2012}, 
startmonth= {May}, 
startday= {14}, 
finishyear= {2012}, 
finishmonth= {May}, 
finishday= {16}, 
keyword= {urban landscape structure}, 
language= {English}, 
conference= {REALCORP 2012 17th International Conference on Urban Planning and Regonal Development in the Information Society},}

@article{012koutsoklenis2014hapticcue,
author = {Athanasios Koutsoklenis and Konstantinos Papadopoulos},
title ={Haptic Cues Used for Outdoor Wayfinding by Individuals with Visual Impairments},
journal = {Journal of Visual Impairment \& Blindness},
volume = {108},
number = {1},
pages = {43-53},
year = {2014},
doi = {10.1177/0145482X1410800105},
URL = {https://doi.org/10.1177/0145482X1410800105},
eprint = { https://doi.org/10.1177/ },
abstract = { IntroductionThe study presented here examines which haptic cues individuals with visual impairments use more frequently and determines which of these cues are deemed by these individuals to be the most important for way-finding in urban environments. It also investigates the ways in which these haptic cues are used by individuals with visual impairments.MethodsTo answer the research questions, both quantitative and qualitative data were collected. The data collection procedure consisted of three parts: a focus-group interview, questionnaires, and fully structured interviews.ResultsA list of 37 haptic cues used for outdoor wayfinding emerged from the focus-group interview. The following haptic cues were calculated to be the most significant for outdoor wayfinding by individuals with visual impairments: changes in the texture of walking surface, sidewalks, bus stops, slopes, curb ramps, walls, parking posts, traffic lights, flower beds, and potholes. Participants use these haptic cues as points of reference, to determine their position in an environment, and to collect information about other objects in the environment. Haptic cues also function as warnings of possible hazards and help individuals with visual impairments to avoid colliding with objects while walking.DiscussionThe participants reported that they use haptic cues that can be perceived through different means and that they use their sense of touch to gain different types of spatial information. A discrepancy was revealed for several cues between their scores in frequency and usefulness.Implications for practitionersFindings from this study could be used to inform training in orientation and mobility, to enhance route descriptions, and to inform the design of commercial tactile maps. }
}

@InProceedings{013ohnbar2018personalize,
title = 	 {Personalized Dynamics Models for Adaptive Assistive Navigation Systems},
author =       {OhnBar, Eshed and Kitani, Kris and Asakawa, Chieko},
booktitle = 	 {Proceedings of The 2nd Conference on Robot Learning},
pages = 	 {16--39},
year = 	 {2018},
editor = 	 {Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun},
volume = 	 {87},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {29--31 Oct},
publisher =    {PMLR},
address = {Zurich},
pdf = 	 {http://proceedings.mlr.press/v87/ohnbar18a/ohnbar18a.pdf},
url = 	 {https://proceedings.mlr.press/v87/ohnbar18a.html},
abstract = 	 {Consider an assistive system that guides visually impaired users through speech and haptic feedback to their destination. Existing robotic and ubiquitous navigation technologies (e.g., portable, ground, or wearable systems) often operate in a generic, user-agnostic manner. However, to minimize confusion and navigation errors, our real-world analysis reveals a crucial need to adapt theinstructional guidance across different end-users with diverse mobility skills. To address this practical issue in scalable system design, we propose a novel model based reinforcement learning framework for personalizing the system-user interaction experience. When incrementally adapting the system to new users, we propose to use a weighted experts model for addressing data-efficiency limitations in transfer learning with deep models. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for (1) more accurate long-term human behavior prediction (up to 20 seconds into the future) through improved reasoning over personal mobility characteristics, interaction with surrounding obstacles, and the current navigation goal, and (2) quick adaptation at the onset of learning, when data is limited.}
}

@inproceedings{014kulyukin2004robotic,
author = {Kulyukin, Vladimir and Gharpure, Chaitanya and Sute, Pradnya and De Graw, Nathan and Nicholson, John},
title = {A robotic wayfinding system for the visually impaired},
year = {2004},
isbn = {0262511835},
publisher = {AAAI Press},
abstract = {We present an emerging indoor assisted navigation system for the visually impaired. The core of the system is a mobile robotic base with a sensor suite mounted on it. The sensor suite consists of an RFID reader and a laser range finder. Small passive RFID sensors are manually inserted in the environment. We describe how the system was deployed in two indoor environments and evaluated by visually impaired participants in a series of pilot experiments.},
booktitle = {Proceedings of the 16th Conference on Innovative Applications of Artifical Intelligence},
pages = {864–869},
numpages = {6},
location = {San Jose, California, United States},
address={San Jose},
series = {IAAI'04},
url = {https://cdn.aaai.org/IAAI/2004/IAAI04-014.pdf}
}

@inproceedings{015guerreiro2019cabot,
author = {Guerreiro, Jo\~{a}o and Sato, Daisuke and Asakawa, Saki and Dong, Huixu and Kitani, Kris M. and Asakawa, Chieko},
title = {CaBot: Designing and Evaluating an Autonomous Navigation Robot for Blind People},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353771},
doi = {10.1145/3308561.3353771},
abstract = {Navigation robots have the potential to overcome some of the limitations of traditional navigation aids for blind people, specially in unfamiliar environments. In this paper, we present the design of CaBot (Carry-on roBot), an autonomous suitcase-shaped navigation robot that is able to guide blind users to a destination while avoiding obstacles on their path. We conducted a user study where ten blind users evaluated specific functionalities of CaBot, such as a vibro-tactile handle to convey directional feedback; experimented to find their comfortable walking speed; and performed navigation tasks to provide feedback about their overall experience. We found that CaBot's performance highly exceeded users' expectations, who often compared it to navigating with a guide dog or sighted guide. Users' high confidence, sense of safety, and trust on CaBot poses autonomous navigation robots as a promising solution to increase the mobility and independence of blind people, in particular in unfamiliar environments.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {68–82},
numpages = {15},
keywords = {obstacle avoidance, mobility, human-robot interaction, guide robot, blind navigation},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@INPROCEEDINGS{016chuang2018trail,
author={Chuang, Tzu-Kuan and Lin, Ni-Ching and Chen, Jih-Shi and Hung, Chen-Hao and Huang, Yi-Wei and Teng, Chunchih and Huang, Haikun and Yu, Lap-Fai and Giarré, Laura and Wang, Hsueh-Cheng},
booktitle={IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Deep Trail-Following Robotic Guide Dog in Pedestrian Environments for People who are Blind and Visually Impaired - Learning from Virtual and Real Worlds}, 
year={2018},
volume={},
number={},
address={Brisbane},
publisher={IEEE},
pages={5849-5855},
abstract={Navigation in pedestrian environments is critical to enabling independent mobility for the blind and visually impaired (BVI) in their daily lives. White canes have been commonly used to obtain contact feedback for following walls, curbs, or man-made trails, whereas guide dogs can assist in avoiding physical contact with obstacles or other pedestrians. However, the infrastructures of tactile trails or guide dogs are expensive to maintain. Inspired by the autonomous lane following of self-driving cars, we wished to combine the capabilities of existing navigation solutions for BVI users. We proposed an autonomous, trail-following robotic guide dog that would be robust to variances of background textures, illuminations, and interclass trail variations. A deep convolutional neural network (CNN) is trained from both the virtual and realworld environments. Our work included major contributions: 1) conducting experiments to verify that the performance of our models trained in virtual worlds was comparable to that of models trained in the real world; 2) conducting user studies with 10 blind users to verify that the proposed robotic guide dog could effectively assist them in reliably following man-made trails.},
keywords={Dogs;Cameras;Robot vision systems;Navigation;Mobile robots},
doi={10.1109/ICRA.2018.8460994},
ISSN={2577-087X},
month={21--25 May},}

@inproceedings{017kulyukin2006ergonomic,
author = {Kulyukin, Vladimir A. and Gharpure, Chaitanya},
title = {Ergonomics-for-one in a robotic shopping cart for the blind},
year = {2006},
isbn = {1595932941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1121241.1121267},
doi = {10.1145/1121241.1121267},
abstract = {Assessment and design frameworks for human-robot teams attempt to maximize generality by covering a broad range of potential applications. In this paper, we argue that, in assistive robotics, the other side of generality is limited applicability: it is oftentimes more feasible to custom-design and evolve an application that alleviates a specific disability than to spend resources on figuring out how to customize an existing generic framework. We present a case study that shows how we used a pure bottom-up learn-through-deployment approach inspired by the principles of ergonomics-for-one to design, deploy and iteratively re-design a proof-of-concept robotic shopping cart for the blind.},
booktitle = {Proceedings of the 1st ACM SIGCHI/SIGART Conference on Human-Robot Interaction},
pages = {142–149},
numpages = {8},
keywords = {assistive robotics, assistive technology, ergonomics-for-one, navigation and wayfinding for the blind},
location = {Salt Lake City, Utah, USA},
series = {HRI '06}
}

@INPROCEEDINGS{018nanavati2019dyad,
author={Nanavati, Amal and Tan, Xiang Zhi and Connolly, Joe and Steinfeld, Aaron},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Follow The Robot: Modeling Coupled Human-Robot Dyads During Navigation}, 
year={2019},
volume={},
number={},
publisher={IEEE},
address={Macau},
pages={3836-3843},
abstract={Many robot applications being explored involve robots leading humans during navigation. Developing effective robots for this task requires a way for robots to understand and model a human's following behavior. In this paper, we present results from a user study of how humans follow a guide robot in the halls of an office building. We then present a data-driven Markovian model of this following behavior, and demonstrate its generalizability across time interval and trajectory length. Finally, we integrate the model into a global planner and run a simulation experiment to investigate the benefits of coupled human-robot planning. Our results suggest that the proposed model effectively predicts how humans follow a robot, and that the coupled planner, while taking longer, leads the human significantly closer to the target position.},
keywords={},
doi={10.1109/IROS40897.2019.8967656},
ISSN={2153-0866},
month={Nov},}

@inproceedings{019zhang2023boss,
author = {Zhang, Yan and Li, Ziang and Guo, Haole and Wang, Luyao and Chen, Qihe and Jiang, Wenjie and Fan, Mingming and Zhou, Guyue and Gong, Jiangtao},
title = {"I am the follower, also the boss": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580884},
doi = {10.1145/3544548.3580884},
abstract = {Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy-switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and subjective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and autonomy, which can inform the design of assistive robots.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {542},
numpages = {22},
keywords = {control, guiding robot, level of autonomy, machine form, navigation, safety, trust, visual impairment},
location = {Hamburg, Germany},
series = {CHI '23}
}

@INPROCEEDINGS{020kulyukin2004rfid,
author={Kulyukin, V. and Gharpure, C. and Nicholson, J. and Pavithran, S.},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)}, 
title={RFID in robot-assisted indoor navigation for the visually impaired}, 
year={2004},
volume={2},
number={},
publisher={IEEE},
address={Sendai},
pages={1979-1984 vol.2},
keywords={Radiofrequency identification;Cognitive robotics;Dogs;Robot sensing systems;Computer science;Radio navigation;Orbital robotics;Testing;Legged locomotion;Passive RFID tags},
doi={10.1109/IROS.2004.1389688},}

@Article{021kulyukin2006structured,
author={Kulyukin, Vladimir
and Gharpure, Chaitanya
and Nicholson, John
and Osborne, Grayson},
title={Robot-assisted wayfinding for the visually impaired in structured indoor environments},
journal={Autonomous Robots},
year={2006},
month={Aug},
day={01},
volume={21},
number={1},
pages={29-41},
abstract={We present a robot-assisted wayfinding system for the visually impaired in structured indoor environments. The system consists of a mobile robotic guide and small passive RFID sensors embedded in the environment. The system is intended for use in indoor environments, such as office buildings, supermarkets and airports. We describe how the system was deployed in two indoor environments and evaluated by visually impaired participants in a series of pilot experiments. We analyze the system's successes and failures and outline our plans for future research and development.},
issn={1573-7527},
doi={10.1007/s10514-006-7223-8},
url={https://doi.org/10.1007/s10514-006-7223-8}
}

@inproceedings{022xiao2021hybrid,
author = {Xiao, Anxing and Tong, Wenzhe and Yang, Lizhi and Zeng, Jun and Li, Zhongyu and Sreenath, Koushil},
title = {Robotic Guide Dog: Leading a Human with Leash-Guided Hybrid Physical Interaction},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICRA48506.2021.9561786},
doi = {10.1109/ICRA48506.2021.9561786},
abstract = {An autonomous robot that is able to physically guide humans through narrow and cluttered spaces could be a big boon to the visually-impaired. Most prior robotic guiding systems are based on wheeled platforms with large bases with actuated rigid guiding canes. The large bases and the actuated arms limit these prior approaches from operating in narrow and cluttered environments. We propose a method that introduces a quadrupedal robot with a leash to enable the robot-guidinghuman system to change its intrinsic dimension (by letting the leash go slack) in order to fit into narrow spaces. We propose a hybrid physical Human Robot Interaction model that involves leash tension to describe the dynamical relationship in the robot-guiding-human system. This hybrid model is utilized in a mixed-integer programming problem to develop a reactive planner that is able to utilize slack-taut switching to guide a blind-folded person to safely travel in a confined space. The proposed leash-guided robot framework is deployed on a Mini Cheetah quadrupedal robot and validated in experiments (Video <sup>1</sup>)},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {11470–11476},
numpages = {7},
address = {Xi'an},
location = {Xi'an, China},
year = {2021},
}

@article{023albogamy2021sravip,
title = {SRAVIP: Smart Robot Assistant for Visually Impaired Persons},
journal = {International Journal of Advanced Computer Science and Applications},
doi = {10.14569/IJACSA.2021.0120739},
url = {http://dx.doi.org/10.14569/IJACSA.2021.0120739},
year = {2021},
publisher = {The Science and Information Organization},
pages={345-352},
volume = {12},
number = {7},
author = {Fahad Albogamy and Turk Alotaibi and Ghalib Alhawdan and Mohammed Faisal},
}

@ARTICLE{024ulrich2001guidecane,
author={Ulrich, I. and Borenstein, J.},
journal={IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans}, 
title={The GuideCane-applying mobile robot technologies to assist the visually impaired}, 
year={2001},
volume={31},
number={2},
pages={131-136},
abstract={The GuideCane is a device designed to help blind or visually impaired users navigate safely and quickly among obstacles and other hazards. During operation, the user pushes the lightweight GuideCane forward. When the GuideCane's ultrasonic sensors detect an obstacle, the embedded computer determines a suitable direction of motion that steers the GuideCane and the user around it. The steering action results in a very noticeable force felt in the handle, which easily guides the user without any conscious effort on his/her part.},
keywords={Mobile robots;Dogs;Hazards;Costs;Time of arrival estimation;Mechanical engineering;Shape measurement;Navigation;Motion detection;Embedded computing},
doi={10.1109/3468.911370},
ISSN={1558-2426},
month={March},}

@INPROCEEDINGS{025li2019ballbot,
author={Li, Zhongyu and Hollis, Ralph},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Toward A Ballbot for Physically Leading People: A Human-Centered Approach}, 
year={2019},
volume={},
number={},
pages={4827-4833},
address={Macua},
publisher={IEEE},
abstract={This work presents a new human-centered method for indoor service robots to provide people with physical assistance and active guidance while traveling through congested and narrow spaces. As most previous work is robotcentered, this paper develops an end-to-end framework which includes a feedback path of the measured human positions. The framework combines a planning algorithm and a humanrobot interaction module to guide the led person to a specified planned position. The approach is deployed on a person-size dynamically stable mobile robot, the CMU ballbot. Trials were conducted where the ballbot physically led a blindfolded person to safely navigate in a cluttered environment.},
keywords={},
doi={10.1109/IROS40897.2019.8968546},
ISSN={2153-0866},
month={Nov},}

@article{short2019designing,
author = {Eden J. Short, Stephen D. Reay and Reid A. Douglas},
title = {Designing wayfinding systems in healthcare: from exploratory prototyping to scalable solutions},
journal = {Design for Health},
volume = {3},
number = {1},
pages = {180-193},
year = {2019},
publisher = {Routledge},
doi = {10.1080/24735132.2019.1575659},
}

@article{farr2012wayfinding,
author = {Anna Charisse Farr, Tristan Kleinschmidt, Prasad Yarlagadda and Kerrie Mengersen},
title = {Wayfinding: A simple concept, a complex process},
journal = {Transport Reviews},
volume = {32},
number = {6},
pages = {715-743},
year = {2012},
publisher = {Routledge},
doi = {10.1080/01441647.2012.712555},
}

@ARTICLE{wakita2013humanwalkingintentionbasedmotioncontrolofanomnidirectionaltypecanerobot,
author={Wakita, Kohei and Huang, Jian and Di, Pei and Sekiyama, Kosuke and Fukuda, Toshio},
journal={IEEE/ASME Transactions on Mechatronics}, 
title={Human-Walking-Intention-Based Motion Control of an Omnidirectional-Type Cane Robot}, 
year={2013},
volume={18},
number={1},
pages={285-296},
doi={10.1109/TMECH.2011.2169980},}

@Article{026bradley2005directions,
author={Bradley, Nicholas A.
and Dunlop, Mark D.},
title={An Experimental Investigation into Wayfinding Directions for Visually Impaired People},
journal={Personal and Ubiquitous Computing},
year={2005},
month={Nov},
day={01},
volume={9},
number={6},
pages={395-403},
abstract={In recent years, there has been an escalation of orientation and wayfinding technologies and systems for visually impaired people. These technological advancements, however, have not been matched by a suitable investigation of human-computer interaction (e.g. designing navigation aids for people who form different cognitive maps for navigation). The aim of this study is to investigate whether a group of sighted participants and a group of visually impaired participants experience a difference in mental and physical demands when given two different sets of verbal instructions directing them to four landmarks. The content of the first set of instructions was proportioned to route descriptions derived from sighted people, and the second set proportioned to descriptions derived from visually impaired people. The objective assessment involved measuring the time taken by participants to reach landmarks and the number of deviations that occurred. A NASA--Task Load Index questionnaire provided an indication of participants subjective perception of workload. The results revealed that instructions formed from visually impaired people resulted in a lower weighted workload score, less minor deviations, and quicker times for visually impaired participants. In contrast, these instructions were found to cause a higher weighted workload score for sighted participants. The results are discussed in relation to the issue of personalisation of mobile context-aware systems for visually impaired people.},
issn={1617-4917},
doi={10.1007/s00779-005-0350-y},
url={https://doi.org/10.1007/s00779-005-0350-y}
}

@InProceedings{027kammoun2012virtual,
author="Kammoun, S.
and Mac{\'e}, M. J-M.
and Oriola, B.
and Jouffrais, C.",
editor="Langdon, Patrick
and Clarkson, John
and Robinson, Peter
and Lazar, Jonathan
and Heylighen, Ann",
title="Designing a Virtual Environment Framework for Improving Guidance for the Visually Impaired",
booktitle="Designing Inclusive Systems",
year="2012",
publisher="Springer London",
address="London",
pages="217--226",
abstract="Electronic Orientation Aids are dedicated to orientation assistance for the visually impaired. They are made of at least 3 essential components: 1) A positioning system (e.g. GPS); 2) A Geographical Information System (GIS) that includes both a digitised map and a software designed to select routes, track the traveller's path, and provide him with navigation information; 3) A User Interface (UI) that relies on nonvisual(usually auditory or tactile) interaction",
isbn="978-1-4471-2867-0"
}

@Article{028fernandes2019spatial/survey,
author={Fernandes, Hugo
and Costa, Paulo
and Filipe, Vitor
and Paredes, Hugo
and Barroso, Jo{\~a}o},
title={A review of assistive spatial orientation and navigation technologies for the visually impaired},
journal={Universal Access in the Information Society},
year={2019},
month={Mar},
day={01},
volume={18},
number={1},
pages={155-168},
abstract={The overall objective of this work is to review the assistive technologies that have been proposed by researchers in recent years to address the limitations in user mobility posed by visual impairment. This work presents an ``umbrella review.'' Visually impaired people often want more than just information about their location and often need to relate their current location to the features existing in the surrounding environment. Extensive research has been dedicated into building assistive systems. Assistive systems for human navigation, in general, aim to allow their users to safely and efficiently navigate in unfamiliar environments by dynamically planning the path based on the user's location, respecting the constraints posed by their special needs. Modern mobile assistive technologies are becoming more discrete and include a wide range of mobile computerized devices, including ubiquitous technologies such as mobile phones. Technology can be used to determine the user's location, his relation to the surroundings (context), generate navigation instructions and deliver all this information to the blind user.},
issn={1615-5297},
doi={10.1007/s10209-017-0570-8},
url={https://doi.org/10.1007/s10209-017-0570-8}
}

@InProceedings{029riviere2018tactibelt,
author="Riviere, Marc-Aur{\`e}le
and Gay, Simon
and Pissaloux, Edwige",
editor="Miesenberger, Klaus
and Kouroupetroglou, Georgios",
title="TactiBelt: Integrating Spatial Cognition and Mobility Theories into the Design of a Novel Orientation and Mobility Assistive Device for the Blind",
booktitle="Computers Helping People with Special Needs",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="110--113",
abstract="The aim of this paper is to introduce a novel functional design for an indoor and outdoor mobility assistive device for the visually impaired, based on the theoretical frameworks of mobility and spatial cognition. The originality of the proposed approach comes from the integration of two main aspects of navigation, locomotion and wayfinding. The cognitive theories which underpin the design of the proposed sensory substitution device, called TactiBelt, are identified and discussed in the framework of spatial knowledge acquisition.",
isbn="978-3-319-94274-2"
}

@InProceedings{030koustriava2016verbalortactile,
author="Koustriava, Eleni
and Papadopoulos, Konstantinos
and Koukourikos, Panagiotis
and Barouti, Marialena",
editor="Antona, Margherita
and Stephanidis, Constantine",
title="The Impact of Orientation and Mobility Aids on Wayfinding of Individuals with Blindness: Verbal Description vs. Audio-Tactile Map",
booktitle="Universal Access in Human-Computer Interaction. Users and Context Diversity",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="577--585",
abstract="The aim of the present study was to examine if a verbal description of an urban area or an audio-tactile map would support the development of an effective cognitive route that could be used consequently for detecting specific points of interest in the actual area. Twenty adults with blindness (total blindness or only light perception) took part in the research. Two O{\&}M aids were used: verbal descriptions and audio-tactile maps readable with the use of a touchpad device. Participants were asked to use each aid separately to encode the location of 6 points of interest, and next to walk within the area with the scope of detecting these points. The findings proved that an individual with visual impairments can acquire and use an effective cognitive route through the use of an audio-tactile map, while relying on a verbal description entails greater difficulties when he/she comes into the physical environment.",
isbn="978-3-319-40238-3"
}

@Inbook{031paiva2020improve/survey,
author="Paiva, Sara
and Gupta, Nishu",
title="Technologies and Systems to Improve Mobility of Visually Impaired People: A State of the Art",
bookTitle="Technological Trends in Improved Mobility of the Visually Impaired",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="105--123",
abstract="The concept of mobility is an important aspect for citizens and, given the development of cities, is gaining more and more importance each day. Although it is possible to speak of mobility solutions for the general citizen, there are very specific characteristics such as visually impaired people, which require specific and different solutions. In this chapter, we review the literature regarding solutions and technologies that are being used to assist in the mobility of visually impaired people, both in indoor and outdoor environments, as well as in the detection of obstacles, which may occur in both types of environment. We present some recent research in these three main categories and also present a discussion and summary of main technologies, approaches, and equipment used.",
isbn="978-3-030-16450-8",
doi="10.1007/978-3-030-16450-8_5",
url="https://doi.org/10.1007/978-3-030-16450-8_5"
}

@Inbook{032faria2019urban,
author="Faria, Paulo
and Curralo, Ana Filomena
and Paiva, Sara",
title="A Case Study for the Promotion of Urban Mobility for Visually Impaired People",
bookTitle="Mobile Solutions and Their Usefulness in Everyday Life",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="65--82",
abstract="This chapter presents a case study to promote and enhance transport sustainability by providing a mobile app that allows visually impaired people (VIP) to independently use public transportation. This work is a follow-up of a previous case study, and in this chapter, we present new functionalities available in the mobile app, including a careful and detailed layout definition to improve usability. We also present the new tests made in the field with some associates and that allowed us to make important conclusions regarding the usefulness of such an application as well as future directions.",
isbn="978-3-319-93491-4",
doi="10.1007/978-3-319-93491-4_4",
url="https://doi.org/10.1007/978-3-319-93491-4_4"
}

@InProceedings{033long2016mobileoutdoors,
author="Long, Shelby K.
and Karpinsky, Nicole D.
and D{\"o}ner, Hilal
and Still, Jeremiah D.",
editor="Di Bucchianico, Giuseppe
and Kercher, Pete",
title="Using a Mobile Application to Help Visually Impaired Individuals Explore the Outdoors",
booktitle="Advances in Design for Inclusion",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="213--223",
abstract="Visually impaired individuals face a variety of challenges when navigating outdoors, including uneven terrain, unexpected obstacles, safety concerns, and reliance on others for information. The goal of this study was to understand further the navigational needs of visually impaired individuals and to develop a mid-fidelity prototype to address these needs. Through interviews with visually impaired users and accessibility professionals, researchers found that present technology leads to an incomplete understanding of the trail and harmful situations. Currently, there is no known technology available that integrates real-time updates with static trail information for individuals navigating outdoors. In response, a mobile prototype was proposed, integrating user-provided updates with static trail information in a format that caters to all users. Our usability testing showed visually impaired users made few errors using the prototype and were satisfied with their experience.",
isbn="978-3-319-41962-6"
}

@InProceedings{034rocha2017platforms,
author="Rocha, T{\^a}nia
and Fernandes, Hugo
and Reis, Ars{\'e}nio
and Paredes, Hugo
and Barroso, Jo{\~a}o",
editor="Rocha, {\'A}lvaro
and Correia, Ana Maria
and Adeli, Hojjat
and Reis, Lu{\'i}s Paulo
and Costanzo, Sandra",
title="Assistive Platforms for the Visual Impaired: Bridging the Gap with the General Public",
booktitle="Recent Advances in Information Systems and Technologies",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="602--608",
abstract="The visual impaired are a specific minority group that can benefit from specific assistive systems in order to mitigate their mobility and accessibility constrains. In the last decade, our research group has been integrating and developing assistive technologies, focused in human-computer interaction, artificial vision, assisted navigation, pervasive computing, among others. Several projects and prototypes have been developed with the main objective of improving the blind's autonomy, mobility, and quality of life. Currently the technology has reached a maturation point that allows the development of systems based on video capturing, image recognition and location referencing, which are key for providing features of artificial vision, assisted navigation and spatial perception. The miniaturization of electronics can be used to create devices such as electronic canes that equipped with sensors can provide so much more contextual information to a blind user. The adoption of these systems is dependent of an information catalogue regarding points of interest and their physical location reference. In this paper we describe the current work on assistive systems for the blind and propose a new perspective on using the base information of those systems to provide new services to the general public. By bridging the gap between the two groups, we expect to further advance the development of the current systems and contribute to their economic sustainability.",
isbn="978-3-319-56538-5"
}

@InProceedings{035elzabadani2017walkingpal,
author="Elzabadani, Hicham
and Eid, Sara
and Hussein, Lilan Haj
and Hussain, Bassel
and El Nasan, Adnan",
editor="Mokhtari, Mounir
and Abdulrazak, Bessam
and Aloulou, Hamdi",
title="Walking Pal: A Spatial and Context Aware System  for the Visual Impaired",
booktitle="Enhanced Quality of Life and Smart Living",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="227--232",
abstract="People with visual impairment struggle to stay independent in unfamiliar places as they only have a sense of their immediate surroundings using the common available aids. The Walking Pal system aims to help the visually impaired become more independent in outdoor environments by being aware of both her immediate and extended surroundings. This would be achieved by describing points of interest, reading printed text, recognizing certain street features, and finally detecting obstacles. The Walking Pal system utilizes the power of smartphone technologies along with external hardware components to inform the user about the surrounding and guide her safely to her destination using audio messages.",
isbn="978-3-319-66188-9"
}

@InProceedings{036sangale2023navigation/survey,
author="Sangale, Samruddhi
and Morwadkar, Suchitra
and Chaugule, Esha
and Agarwal, Shruti
and Agarwal, Priyanshu",
editor="Goswami, Saptarsi
and Barara, Inderjit Singh
and Goje, Amol
and Mohan, C.
and Bruckstein, Alfred M.",
title="Literature Survey: Navigation System for Visually Impaired People",
booktitle="Data Management, Analytics and Innovation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="667--676",
abstract="Mass transportation connects people with education, employment, and community resources. However, navigating public transportation can be difficult for the visually impaired and physically challenged members of society. There seems to be a rapid proliferation of technology. There has been an urgent need to develop improved methodologies to assist people with disabilities in gaining access to public transportation. While there are numerous navigation systems, some of them rely on Global Positioning System technology, which is useful in outdoor environments but ineffective in indoor environments. Beacons are emerging sensors that are increasingly being used for indoor positioning in shopping malls and airports. They make use of Bluetooth Low Energy technology, which is widely supported by today's smartphones. In this paper, we propose a proof of concept for a mobile application that uses Bluetooth Low Energy beacons to assist people with special needs. This app would provide a simple voice interface for navigating within stations, buses, and trains. As part of our research, we intend to use existing beacons and deploy new beacons in some regional traffic centers. We intend to analyze `ridership data' in order to extract contextual data about the commuter's current environment in order to provide commuters with a cognitive solution and assistance during travel. Our research's ultimate goal is to improve public transportation for visually impaired people.",
isbn="978-981-19-2600-6"
}

@InProceedings{037ueda2014virtualstick,
author="Ueda, Thomas Akira
and de Ara{\'u}jo, Luciano Vieira",
editor="Stephanidis, Constantine
and Antona, Margherita",
title="Virtual Walking Stick: Mobile Application to Assist Visually Impaired People to Walking Safely",
booktitle="Universal Access in Human-Computer Interaction. Aging and Assistive Environments",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="803--813",
abstract="People affected by temporary visual limitations or early permanent limitations have the challenge of adapting the way to perform their daily tasks. In particular, the activity of walking without support of others not only requires extensive adaptation but also can expose individuals to the risk. For instance, if an object is not identified during the walk, serious accidents may happen. Therefore, assist blind people to walk independently and safely is an important challenge for computational area. With the popularity of smartphones, cameras and new sensors are available at affordable prices and can be used to develop software to help visually impaired people to walk more independently and safely. This paper presents the development of a mobile application to help visually impaired people to walk independently, using the smartphone's camera to alert them about obstacles on the way.",
isbn="978-3-319-07446-7"
}

@inproceedings{038liu2023designing, 
title={Designing a Wayfinding Robot for People with Visual Impairments}, 
author={Shuijing Liu and Aamir Hasan and Kaiwen Hong and Chun-Kai Yao and Justin Lin and Weihang Liang and Megan A. Bayles and Wendy A. Rogers and Katherine Driggs-Campbell},
year={2023},
eprint={2302.09144},
archivePrefix={arXiv},
primaryClass={cs.RO},
url={https://arxiv.org/abs/2302.09144}, 
booktitle = {International Conference on Robotics and Automation (ICRA) Workshop on Intelligent Control Methods and Machine Learning Algorithms for Human-Robot Interaction and Assistive Robotics},
address={London},
location={London, UK},
}

@article{remillard2024everyday,
title={Everyday challenges for individuals aging with vision impairment: Technology implications},
author={Remillard, Elena T and Koon, Lyndsie M and Mitzner, Tracy L and Rogers, Wendy A},
journal={The Gerontologist},
volume={64},
number={6},
pages={gnad169},
year={2024},
publisher={Oxford University Press US}
}

@InProceedings{039kim2023formalizing,
title = 	 {Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism},
author =       {Kim, J. Taery and Yu, Wenhao and Kothari, Yash and Walker, Bruce and Tan, Jie and Turk, Greg and Ha, Sehoon},
booktitle = 	 {Proceedings of The 7th Conference on Robot Learning},
pages = 	 {2288--2303},
year = 	 {2023},
editor = 	 {Tan, Jie and Toussaint, Marc and Darvish, Kourosh},
volume = 	 {229},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {06--09 Nov},
publisher =    {PMLR},
address = {Atlanta},
pdf = 	 {https://proceedings.mlr.press/v229/kim23c/kim23c.pdf},
url = 	 {https://proceedings.mlr.press/v229/kim23c.html},
abstract = 	 {This paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. A guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (BVI) users. To build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. First, we formalize the wayfinding task of the human-guide robot team using Markov Decision Processes based on the literature and interviews. Then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the "Delayed Harness" to effectively simulate the navigation behaviors of the team. Additionally, we introduce an action shielding mechanism to enhance user safety by predicting and filtering out dangerous actions. We evaluate the developed interaction model and the safety mechanism in simulation, which greatly reduce the prediction errors and the number of collisions, respectively. We also demonstrate the integrated system on an AlienGo robot with a rigid harness, by guiding users over 100+ meter trajectories.},}

@inproceedings{040kim2023train,
author = {Kim, J. Taery and Yu, Wenhao and Tan, Jie and Turk, Greg and Ha, Sehoon},
title = {How to Train Your Guide Dog: Wayfinding and Safe Navigation with Human-Robot Modeling},
year = {2023},
isbn = {9781450399708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568294.3580076},
doi = {10.1145/3568294.3580076},
abstract = {A robot guide dog has the potential to enhance the independence and quality of life of individuals who are blind or visually impaired by providing accessible, automated, and intelligent guidance. However, developing effective robot guide dogs requires researchers not only to solve robotic perception and planning problems but also to understand complicated two-way interactions of the human-robot team. This work presents the formal definition of the wayfinding task of the robotic guide dog that is grounded by common practices in the real world. Given such a task, we train an effective policy for the robot guide dog while investigating two different human models, a rotating rod model and a rigid harness model. We show that our robot can safely guide a human user to avoid several obstacles in the real world. We also demonstrate that a proper human model is necessary to achieve collision-free navigation for both the human and the robot.},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {221–225},
numpages = {5},
keywords = {accessibility, deep reinforcement learning, guide dog robot, human-robot modeling, navigation, quadrupedal robot},
location = {Stockholm, Sweden},
series = {HRI '23}
}                                 

@inproceedings{041agrawal2023indepedent,
author = {Agrawal, Shivendra},
title = {Assistive Robotics for Empowering Humans with Visual Impairments to Independently Perform Day-to-day Tasks},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The ability to perform common day-to-day tasks is essential for an independent lifestyle. However, many crucial tasks are unaddressed for blind or visually impaired (BVI) people with the current solutions. Our research goal aims to provide technical solutions to such problems to help support more autonomy for BVI people. Through this work, we present a proof-of-concept socially assistive robotic cane that can assist with 1) a navigation task which is finding a socially preferred seat in unknown public places and guiding the users toward it, 2) a manipulation task which is locating and retrieving the desired product from a grocery store shelf. We evaluated our system in an initial pilot study with sighted blindfolded testers, with encouraging results that show the system's potential to provide purposeful and effective navigation guidance optimizing for users' convenience, privacy, and intimacy while increasing their confidence in independent navigation. Another study we ran showed the system's success in locating and providing effective fine-grain manipulation guidance to retrieve desired products with novice users while eliciting a positive user experience.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {3023–3025},
numpages = {3},
keywords = {assistive robotics, computer vision, human-robot interaction, manipulation guidance, markov decision process, planner},
location = {London, United Kingdom},
series = {AAMAS '23},
}

@article{042thiyagarajan2022robot/survey,
author = {Karthick Thiyagarajan and Sarath Kodagoda and Mark Luu and Taliah Duggan-Harper and Doug Ritchie and Kelly Prentice and Jodi Martin},
title = {Intelligent Guide Robots for People who are Blind or have Low Vision: A Review},
journal = {Vision Rehabilitation International},
number = {1},
volume = {13},
year = {2022},
pages = {1--15}
}

@ARTICLE{043zhang2023motioncontrol,
author={Zhang, Bin and Okutsu, Mikiya and Ochiai, Rin and Tayama, Megumi and Lim, Hun-Ok},
journal={IEEE Access}, 
title={Research on Design and Motion Control of a Considerate Guide Mobile Robot for Visually Impaired People}, 
year={2023},
volume={11},
number={},
pages={62820-62828},
abstract={Guide mobile robot have been researched and developed for decades. However, current robot cannot guide the user to the destination considerately, since the status of the user and the properties of the obstacles are not considered, especially for the guiding work of servicing visually impaired people. In this paper, a guide mobile robot with an easy-to-hold handle is designed, a generation method of spatial risk map is proposed to evaluate the influences of potential spaces of objects and a motion control method based on spatial risk map considering potential occupied spaces of objects is proposed. The users are successfully guided to the destination naturally without influencing other pedestrians by avoiding entering the potential spaces of objects automatically, and they can adjust their moving status in their own will since the considerate robot can adaptively adjust its guide status to adapt to the user. Through comparing guiding experiments of different tasks for 10 blindfolded users, the proposed guide mobile robot is proven considerate by guiding the users adaptively and make other pedestrians feel comfortable by avoiding entering the potential spaces of objects automatically.},
keywords={Mobile robots;Robot sensing systems;Aerospace electronics;Autonomous robots;TV;Quadrupedal robots;Robot control;Assistive robots;Intelligent robots;autonomous systems;robot control;assistive robots;autonomous robots},
doi={10.1109/ACCESS.2023.3288152},
ISSN={2169-3536},
month={},}

@INPROCEEDINGS{044zhang2019viocane,
author={Zhang, He and Ye, Cang},
booktitle={12th International Conference on Human System Interaction (HSI)}, 
title={Human-Robot Interaction for Assisted Wayfinding of a Robotic Navigation Aid for the Blind}, 
year={2019},
volume={},
number={},
pages={137-142},
publisher={IEEE},
address={Richmond},
abstract={This paper introduces a new robotic navigation aid (RNA) for the visually impaired (VI). Two fundamental functions - wayfinding and human-robot interaction (HRI) - are presented for assisted wayfinding. The problem of wayfinding involves planning a path from the RNA's current location to the destination and following the path to get to the destination. To address the problem, we developed a new visual inertial odometry to estimate the RNA's pose by using the image and depth data from an RGB-D camera and the inertial data of an IMU. The estimated pose is used for path planning. To guide the user to follow the planned path, we designed an HRI interface with two guiding modes - the robocane mode and white-came mode. In the robocane mode, the RNA uses a motorized rolling tip to steer itself into the desired direction of travel (DDT) for the user to follow and track the planned path. In the white-cane mode, the RNA uses its speech interface to indicate the DDT to the user by audio messages. In this mode, the user swings the RNA just like using a conventional white cane. To make mode selection effortless, we developed a human intent detection (HID) method based on the decision tree mode. The method can detect the user intent and automatically select the appropriate mode according to the detected intent. Experimental results demonstrate the efficacies of the VIO, HRI, and HID methods for assisted wayfinding.},
keywords={Navigation;RNA;Pose estimation;Human-robot interaction;Cameras;Path planning;Decision trees;Robotic Navigation Aid;Assisted Wayfinding;Human-Robot Interaction;Human Intent Detection;Visually Impaired},
doi={10.1109/HSI47298.2019.8942612},
ISSN={2158-2254},
month={June},}

@Article{045mai2023cane/survey,
AUTHOR = {Mai, Chunming and Xie, Dongliang and Zeng, Lina and Li, Zaijin and Li, Zhibo and Qiao, Zhongliang and Qu, Yi and Liu, Guojun and Li, Lin},
TITLE = {Laser Sensing and Vision Sensing Smart Blind Cane: A Review},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {2},
ARTICLE-NUMBER = {869},
NUMPAGES = {22},
URL = {https://www.mdpi.com/1424-8220/23/2/869},
PubMedID = {36679665},
ISSN = {1424-8220},
ABSTRACT = {Laser sensing and vision sensing smart canes can improve the convenience of travel for the visually impaired, but for the present, most of the system functions of laser sensing and vision sensing smart canes are still defective. Guide equipment and smart blind canes are introduced and classified first, and the smart blind canes based on vision sensing, laser sensing and laser vision sensing are investigated, respectively, and the research status of laser vision sensing smart blind canes is sorted out. The advantages and disadvantages of various laser vision sensing smart blind canes are summarized, especially the research development of laser vision fusion as the core of new smart canes. The future development prospects of laser vision sensing smart blind cane are overviewed, to boost the development of laser vision sensing smart blind cane, to provide safe and efficient travel guarantee for the visually impaired.},
DOI = {10.3390/s23020869}
}

@article{046slade2021cane,
author = {Patrick Slade  and Arjun Tambe  and Mykel J. Kochenderfer},
title = {Multimodal sensing and intuitive steering assistance improve navigation and mobility for people with impaired vision},
journal = {Science Robotics},
volume = {6},
number = {59},
pages = {eabg6594},
year = {2021},
doi = {10.1126/scirobotics.abg6594},
URL = {https://www.science.org/doi/abs/10.1126/scirobotics.abg6594},
eprint = {https://www.science.org/doi/pdf/10.1126/scirobotics.abg6594},
abstract = {A robotic white cane enables people with impaired vision to improve their mobility and overcome major navigation challenges. Globally, more than 250 million people have impaired vision and face challenges navigating outside their homes, affecting their independence, mental health, and physical health. Navigating unfamiliar routes is challenging for people with impaired vision because it may require avoiding obstacles, recognizing objects, and wayfinding indoors and outdoors. Existing approaches such as white canes, guide dogs, and electronic travel aids only tackle some of these challenges. Here, we present the Augmented Cane, a white cane with a comprehensive set of sensors and an intuitive feedback method to steer the user, which addresses navigation challenges and improves mobility for people with impaired vision. We compared the Augmented Cane with a white cane by having sighted and visually impaired participants complete navigation challenges while blindfolded: walking along hallways, avoiding obstacles, and following outdoor waypoints. Across all experiments, the Augmented Cane increased the walking speed for participants with impaired vision by 18 ± 7\% and sighted participants by 35 ± 12\% compared with a white cane. The increase in walking speed may be due to accurate steering assistance, reduced cognitive load, fewer contacts with the environment, and higher participant confidence. We also demonstrate advanced navigation capabilities of the Augmented Cane: indoor wayfinding, recognizing and steering the participant to a key object, and navigating a sequence of indoor and outdoor challenges. The open-source and low-cost design of the Augmented Cane provides a platform that may improve the mobility and quality of life of people with impaired vision.}}

@ARTICLE{047lu2021navassit,
AUTHOR={Lu, Chen-Lung and Liu, Zi-Yan and Huang, Jui-Te and Huang, Ching-I and Wang, Bo-Hui and Chen, Yi and Wu, Nien-Hsin and Wang, Hsueh-Cheng and Giarré, Laura and Kuo, Pei-Yi},   
TITLE={Assistive Navigation Using Deep Reinforcement Learning Guiding Robot With UWB/Voice Beacons and Semantic Feedbacks for Blind and Visually Impaired People},
JOURNAL={Frontiers in Robotics and AI},      
VOLUME={8},           
YEAR={2021},
NUMPAGES={15},
ARTICLE-NUMBER={654132},
URL={https://www.frontiersin.org/articles/10.3389/frobt.2021.654132},       
DOI={10.3389/frobt.2021.654132},      
ISSN={2296-9144},   
ABSTRACT={Facilitating navigation in pedestrian environments is critical for enabling people who are blind and visually impaired (BVI) to achieve independent mobility. A deep reinforcement learning (DRL)–based assistive guiding robot with ultrawide-bandwidth (UWB) beacons that can navigate through routes with designated waypoints was designed in this study. Typically, a simultaneous localization and mapping (SLAM) framework is used to estimate the robot pose and navigational goal; however, SLAM frameworks are vulnerable in certain dynamic environments. The proposed navigation method is a learning approach based on state-of-the-art DRL and can effectively avoid obstacles. When used with UWB beacons, the proposed strategy is suitable for environments with dynamic pedestrians. We also designed a handle device with an audio interface that enables BVI users to interact with the guiding robot through intuitive feedback. The UWB beacons were installed with an audio interface to obtain environmental information. The on-handle and on-beacon verbal feedback provides points of interests and turn-by-turn information to BVI users. BVI users were recruited in this study to conduct navigation tasks in different scenarios. A route was designed in a simulated ward to represent daily activities. In real-world situations, SLAM-based state estimation might be affected by dynamic obstacles, and the visual-based trail may suffer from occlusions from pedestrians or other obstacles. The proposed system successfully navigated through environments with dynamic pedestrians, in which systems based on existing SLAM algorithms have failed.}
}

@inproceedings{048wang2021navdog,
author = {Wang, Liyang and Zhao, Jinxin and Zhang, Liangjun},
title = {NavDog: robotic navigation guide dog via model predictive control and human-robot modeling},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442098},
doi = {10.1145/3412841.3442098},
abstract = {Guide dogs can vastly improve vision-impaired people's daily-life quality by guiding them to destinations while avoiding obstacles. Animal guide dogs are costly for training. This paper presents a robot guide dog system to take a vision-impaired user to a destination while avoiding obstacles in the environment for both the user and the robot dog. A novel human-robot kinematic model and an MPC-based motion planning and control algorithm are proposed. We implement the method on a wheeled ground robot. All the sensors are mounted on the robot, and the human user does not have to take additional sensor devices. Simulation and real-world experiment results show that the proposed method can tackle challenging navigation tasks in narrow corridors for vision-impaired people.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {815–818},
numpages = {4},
keywords = {mobile robot, multi-agent system modeling, navigation, optimal control, service robot},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{049jimenez2020locomotion,
title = {Assistive locomotion device with haptic feedback for guiding visually impaired people},
journal = {Medical Engineering \& Physics},
volume = {80},
pages = {18-25},
year = {2020},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2020.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1350453320300515},
author = {Mario F. Jiménez and Ricardo C. Mello and Teodiano Bastos and Anselmo Frizera},
keywords = {Admittance control, Visual impairment, Haptic feedback, Smart walker},
abstract = {Robotic assistive devices are able to enhance physical stability and balance. Smart walkers, in particular, are also capable of offering cognitive support for individuals whom conventional walkers are unsuitable. However, visually impaired individuals often need additional sensorial assistance from those devices. This work proposes a smart walker with an admittance controller for guiding visually impaired individuals along a desired path. The controller uses as inputs the physical interaction between the user and the walker to provide haptic feedback hinting the path to be followed. Such controller is validated in a set of experiments with healthy individuals. At first, users were blindfolded during navigation to assess the capacity of the smart walker in providing guidance without visual input. Then, the blindfold is removed and the focus is on evaluating the human-robot interaction when the user had visual information during navigation. The results indicate that the admittance controller design and the design of the guidance path were factors impacting on the level of comfort reported by users. In addition, when the user was blindfolded, the linear velocity assumed lower values than when did not wear it, from a mean value of 0.19 m/s to 0.21 m/s.}
}

@INPROCEEDINGS{050chen2023comfort,
author={Chen, Yanbo and Xu, Zhengzhe and Jian, Zhuozhu and Tang, Gengpan and Yang, Liyunong and Xiao, Anxing and Wang, Xueqian and Liang, Bin},
booktitle={IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Quadruped Guidance Robot for the Visually Impaired: A Comfort-Based Approach}, 
year={2023},
volume={},
number={},
publisher={IEEE},
address={London},
pages={12078-12084},
abstract={Guidance robots that can guide people and avoid various obstacles, could potentially be owned by more visually impaired people at a fairly low cost. Most of the previous guidance robots for the visually impaired ignored the human response behavior and comfort, treating the human as an appendage dragged by the robot, which can lead to imprecise guidance of the human and sudden changes in the traction force experienced by the human. In this paper, we propose a novel quadruped guidance robot system with a comfort-based concept. We design a controllable traction device that can adjust the length and force between human and robot to ensure comfort. To allow the human to be guided safely and comfortably to the target position in complex environments, our proposed human motion planner can plan the traction force with the force-based human motion model. To track the planned force, we also propose a robot motion planner that can generate the specific robot motion command and design the force control device. Our system has been deployed on Unitree Laikago quadrupedal platform and validated in real-world scenarios. (Video11Video demonstration: https://youtu.be/gd-RcYOqGuo.)},
keywords={Robot motion;Target tracking;Costs;Automation;Force;Behavioral sciences;Quadrupedal robots},
doi={10.1109/ICRA48891.2023.10160854},
ISSN={},
month={May},}

@article{051diaztoro2021wander,
author    = {Andr\'{e}s A. D\'{i}az-Toro and Sixto E. Campa\~{n}a-Bastidas and Eduardo F. Caicedo-Bravo},
title     = {Vision-Based System for Assisting Blind People to Wander Unknown Environments in a Safe Way},
journal   = {Journal of Sensors},
volume    = {2021},
pages     = {18 pages},
year      = {2021},
articleID = {6685686},
doi       = {10.1155/2021/6685686},
url       = {https://doi.org/10.1155/2021/6685686}
}

@ARTICLE{052ye2016corobocane,
author={Ye, Cang and Hong, Soonhac and Qian, Xiangfei and Wu, Wei},
journal={IEEE Systems, Man, and Cybernetics Magazine}, 
title={Co-Robotic Cane: A New Robotic Navigation Aid for the Visually Impaired}, 
year={2016},
volume={2},
number={2},
pages={33-42},
abstract={This article presents a new robotic navigation aid (RNA) called a co-robotic cane (CRC). The CRC uses a three-dimensional (3-D) camera for both pose estimation and object recognition in an unknown indoor environment. The six-degrees-of-freedom (6-DOF) pose estimation method determines the CRC's pose change by an egomotion estimation method and the iterative closest point algorithm and reduces the pose integration error by a pose graph optimization algorithm. The pose estimation method does not require any prior knowledge of the environment. The object recognition method detects indoor structures, such as stairways and doorways, and objects, such as tables and computer monitors, by a Gaussian mixture model (GMM)-based pattern-recognition method. Some structures/objects (e.g., stairways) can be used as navigational waypoints and others for obstacle avoidance. The CRC can be used in either robot cane (active) mode or white cane (passive) mode. In the active mode, it guides the user by steering itself into the desired direction of travel, while in the passive mode it functions as a computer-visionenhanced white cane. The CRC is a co-robot. It can detect human intent and use the intent to select a suitable mode automatically.},
keywords={Cameras;Pose estimation;Mobile robots;Navigation;Assistive technologies;Blindness;Visualization},
doi={10.1109/MSMC.2015.2501167},
ISSN={2333-942X},
month={April},}

@inproceedings{053bonani2018eyesstudy,
author = {Bonani, Mayara and Oliveira, Raquel and Correia, Filipa and Rodrigues, Andr\'{e} and Guerreiro, Tiago and Paiva, Ana},
title = {What My Eyes Can't See, A Robot Can Show Me: Exploring the Collaboration Between Blind People and Robots},
year = {2018},
isbn = {9781450356503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234695.3239330},
doi = {10.1145/3234695.3239330},
abstract = {Blind people rely on sighted peers and different assistive technologies to accomplish everyday tasks. In this paper, we explore how assistive robots can go beyond information-giving assistive technologies (e.g., screen readers) by physically collaborating with blind people. We first conducted a set of focus groups to assess how blind people perceive and envision robots. Results showed that, albeit having stereotypical concerns, participants conceive the integration of assistive robots in a broad range of everyday life scenarios and are welcoming of this type of technology. In a second study, we asked blind participants to collaborate with two versions of a robot in a Tangram assembly task: one robot would only provide static verbal instructions whereas the other would physically collaborate with participants and adjust the feedback to their performance. Results showed that active collaboration had a major influence on the successful performance of the task. Participants also reported higher perceived warmth, competence and usefulness when interacting with the physically assistive robot. Overall, we provide preliminary results on the usefulness of assistive robots and the possible role these can hold in fostering a higher degree of autonomy for blind people.},
booktitle = {Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {15–27},
numpages = {13},
keywords = {human-robot interaction, collaboration, blind people},
location = {Galway, Ireland},
series = {ASSETS '18}
}

@ARTICLE{054shoval2003belt-cane-mag,
author={Shoval, S. and Ulrich, I. and Borenstein, J.},
journal={IEEE Robotics \& Automation Magazine}, 
title={NavBelt and the Guide-Cane [obstacle-avoidance systems for the blind and visually impaired]}, 
year={2003},
volume={10},
number={1},
pages={9-20},
abstract={NavBelt and GuideCane are computerized devices based on advanced mobile robotics obstacle-avoidance technologies. NavBelt is worn by the user like a belt and is equipped with an array of ultrasonic sensors. It provides acoustic signals via a set of stereo earphones that guides the user around obstacles or "displays" a virtual acoustic panoramic image of the traveler's surroundings. One limitation of the NavBelt is that it is exceedingly difficult for the user to comprehend the guidance signals in time to allow fast walking. A newer device, called GuideCane, effectively overcomes this problem. The GuideCane uses the same mobile robotics technology as the NavBelt but is a wheeled device pushed ahead of the user via an attached cane. When the GuideCane detects an obstacle, it steers around it. The user immediately feels this steering action and can follow the GuideCane's new path easily without any conscious effort. This article describes the two devices, including the mechanical, electronic, and software components, user-machine interface, and some experimental results.},
keywords={Sensor arrays;Mobile robots;Acoustic devices;Mobile computing;Robot sensing systems;Belts;Wearable sensors;Acoustic sensors;Headphones;Computer displays},
doi={10.1109/MRA.2003.1191706},
ISSN={1558-223X},
month={March},}

@ARTICLE{055shoval1998navbelt,
author={Shoval, S. and Borenstein, J. and Koren, Y.},
journal={IEEE Transactions on Biomedical Engineering}, 
title={The NavBelt-a computerized travel aid for the blind based on mobile robotics technology}, 
year={1998},
volume={45},
number={11},
pages={1376-1386},
abstract={This paper presents a new concept for a travel aid for the blind. A prototype device, called the NavBelt, was developed to test this concept. The device can be used as a primary or secondary aid, and consists of a portable computer, ultrasonic sensors, and stereophonic headphone. The computer applies navigation and obstacle avoidance technologies that were developed originally for mobile robots. The computer then uses a stereophonic imaging technique to process the signals from the ultrasonic sensors and relays their information to the user via stereophonic headphones. The user can interpret the information as an acoustic "picture" of the surroundings, or, depending on the operational mode, as the recommended travel direction. The acoustic signals are transmitted as discrete beeps or continuous sounds. Experimental results with the NavBelt simulator and a portable prototype show that users can travel safely in an unfamiliar and cluttered environment at speeds of up to 0.8 m/s.},
keywords={Mobile computing;Mobile robots;Headphones;Robot sensing systems;Prototypes;Testing;Portable computers;Navigation;Acoustic imaging;Ultrasonic imaging},
doi={10.1109/10.725334},
ISSN={1558-2531},
month={Nov},}

@ARTICLE{056zhang2017slam,
author={Zhang, He and Ye, Cang},
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={An Indoor Wayfinding System Based on Geometric Features Aided Graph SLAM for the Visually Impaired}, 
year={2017},
volume={25},
number={9},
pages={1592-1604},
abstract={This paper presents a 6-degree of freedom (DOF) pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired. The PE method involves two-graph simultaneous localization and mapping (SLAM) processes to reduce the accumulative pose error of the device. In the first step, the floor plane is extracted from the 3-D camera’s point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch, and  ${Z}$  errors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce  ${X}$ ,  ${Y}$ , and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a visually impaired person in an indoor environment. The system uses the estimated pose and floor plan to locate the device user in a building and guides the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.},
keywords={Simultaneous localization and mapping;RNA;Cameras;Three-dimensional displays;Navigation;Floors;Indoor environments;Blind navigation;wayfinding;robotic navigation aid;pose estimation;graph SLAM;3-D camera},
doi={10.1109/TNSRE.2017.2682265},
ISSN={1558-0210},
month={Sep.},}

@article{057sunitha2021assistive,
doi = {10.1088/1742-6596/2089/1/012056},
url = {https://dx.doi.org/10.1088/1742-6596/2089/1/012056},
year = {2021},
month = {nov},
publisher = {IOP Publishing},
volume = {2089},
number = {1},
pages = {012056},
author = {K.A. Sunitha and Ganti Sri Giri Sai Suraj and G Atchyut Sriram and N Savitha Sai},
title = {Assistive Robot For Visually Impaired People},
journal = {Journal of Physics: Conference Series},
abstract = {The proposed robot aims to serve as a personal assistant for visually impaired people in obstacle avoidance, in identifying the person (known or unknown) with whom they are interacting with and in navigating. The robot has a special feature in truly locating the subject’s location using GPS. The novel feature of this robot is to identify people with whom the subject interacts. Facial detection and identification in real-time has been a challenge and achieved with accurate image processing using viola jones and SURF algorithms. An obstacle avoidance design has been implanted in the system with many sensors to guide in the correct path. Hence, the robot is a fusion of providing the best of the comfort and safety with minimal cost.}
}

@INPROCEEDINGS{058cheraghi2017guidebeacon,
author={Cheraghi, Seyed Ali and Namboodiri, Vinod and Walker, Laura},
booktitle={IEEE International Conference on Pervasive Computing and Communications (PerCom)}, 
title={GuideBeacon: Beacon-based indoor wayfinding for the blind, visually impaired, and disoriented}, 
year={2017},
volume={},
number={},
pages={121-130},
publisher={IEEE},
address={Kona},
abstract={There are currently few options for navigational aids for the blind and visually impaired (BVI) in large indoor spaces. Such indoor spaces can be difficult to navigate even for the general sighted population if they are disoriented due to unfamiliarity or other reasons. This paper presents an indoor wayfinding system called GuideBeacon for the blind, visually impaired, and disoriented (BVID) that assists people in navigating between any two points within indoor environments. The GuideBeacon system allows users equipped with smartphones to interact with low cost Bluetooth-based beacons deployed strategically within the indoor space of interest to navigate their surroundings. This paper describes the technical challenges faced in designing such a system, the design decisions made in building the current version of the GuideBeacon system, the solutions developed to meet the technical challenges, and results from the evaluation of the system. Results presented in this paper obtained from field testing GuideBeacon with BVI and sighted participants suggests that it can be used by the BVID for navigation in large indoor spaces independently and effectively.},
keywords={Navigation;User interfaces},
doi={10.1109/PERCOM.2017.7917858},
ISSN={2474-249X},
month={13--17 March},}

@article{059bayles2022wayfindingreview,
author = {Megan A. Bayles and Travis Kadylak and Shuijing Liu and Aamir Hasan and Weihang Liang and Kaiwen Hong and Kathrine Driggs-Campbell and Wendy A. Rogers},
title ={An Interdisciplinary Approach: Potential for Robotic Support to Address Wayfinding Barriers Among Persons with Visual Impairments},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {66},
number = {1},
pages = {1164-1168},
year = {2022},
doi = {10.1177/1071181322661384},
URL = { https://doi.org/10.1177/1071181322661384 },
eprint = { https://doi.org/10.1177/ },
abstract = { Persons with Vision Impairments (PwVI) often have difficulties navigating indoor environments. The challenges and solutions can change based on their level of familiarity with the location. A collaborative effort was made to design a user needs assessment to understand the collaborative nature of human-robot interaction for wayfinding. The user study was an interview study to discuss with PwVI their navigation experience in familiar, somewhat familiar, and unfamiliar locations. Following this, we discussed their current solution strategies for wayfinding in those locations to discuss how they could imagine a robot to support wayfinding. We report on four case studies to illustrate specific user needs, such as vocal direction and orientation to learn a new environment and navigate, and highlight common strategies, such as supplemental lighting, different types of human assistance, and technologies used (i.e. white canes). }
}

@article{jeamwatthanachai2019challenges/survey,
author = {Watthanasak Jeamwatthanachai and Mike Wald and Gary Wills},
title ={Indoor navigation by blind people: Behaviors and challenges in unfamiliar spaces and buildings},
journal = {British Journal of Visual Impairment},
volume = {37},
number = {2},
pages = {140-153},
year = {2019},
doi = {10.1177/0264619619833723},
URL = {https://doi.org/10.1177/0264619619833723},
eprint = {https://doi.org/10.1177/0264619619833723},
abstract = { A number of visually impaired people suffer from navigation-related activities due to mishaps that discourage them from going out for social activities and interactions. In contrast to outdoors, traveling inside public spaces is a different story, as many environmental cues cannot be used and have their own set of difficulties. Some technologies have come into play in helping these people to have freedom in navigation (e.g., accessible map, indoor navigation systems, and wearable computing devices). However, technologies like accessible maps or indoor navigation systems are insufficient to fulfill the independent navigation gap as additional information is required (obstacles, barriers, and accessibility). To promote indoor navigation and create better use of technologies for visually impaired people, it is essential to understand the facts and actual problems that they experience, and what behaviors and strategies they use to overcome any problems; these are the concerns that led to this study. In all, 30 visually impaired people and 15 experts were recruited to give an interview about the behavior and strategies used to navigate indoor spaces, especially public spaces, for example, universities, hospitals, malls, museums, and airports. The findings from this study reveal that navigating inside buildings and public spaces full of unfamiliar features is too difficult to attempt the first time for a number of reasons, reducing their confidence in independent navigation. }
}

@ARTICLE{060zhong2016regionalized,
author={Zhong, Chaoliang and Liu, Shirong and Lu, Qiang and Zhang, Botao and Yang, Simon X.},
journal={IEEE Transactions on Cybernetics}, 
title={An Efficient Fine-to-Coarse Wayfinding Strategy for Robot Navigation in Regionalized Environments}, 
year={2016},
volume={46},
number={12},
pages={3157-3170},
abstract={This paper proposes an efficient wayfinding strategy for robot navigation in regionalized environments by designing a regionalized spatial knowledge model (RSK model) and a region-based wayfinding algorithm, i.e., a fine-to-coarse A* (FTC-A*) search algorithm. First, the RSK model, which imitates the representation of environments in the human brain, is presented to describe the search environments. The environments that are divided into regions are represented by a hierarchical nested structure where small regions are grouped together to form superordinate regions. Second, on the basis of the RSK model, an FTC-A* search algorithm is developed to plan the fine-to-coarse route. By making a fine planning to robot surroundings in vicinity, but a coarse planning to that at the distance, the FTC-A* algorithm can effectively reduce computational complexity, so as to enhance the efficiency of route search, and meanwhile makes robots to react quickly to user's commands, especially in large-scale environments. Finally, four exhaustive simulations and a physical experiment have been carried out to illustrate the feasibility and effectiveness of the proposed wayfinding strategy.},
keywords={Navigation;Planning;Computational modeling;Mobile robots;Path planning;Computational complexity;Path planning;region-based wayfinding strategy;robot navigation;spatial knowledge model},
doi={10.1109/TCYB.2015.2498760},
ISSN={2168-2275},
month={Dec},}

@article{061baldwin2003future/survey,
author = {Douglas Baldwin},
title ={Wayfinding Technology: A Road Map to the Future},
journal = {Journal of Visual Impairment \& Blindness},
volume = {97},
number = {10},
pages = {612-620},
year = {2003},
doi = {10.1177/0145482X0309701006},
URL = { https://doi.org/10.1177/0145482X0309701006 },
eprint = { https://doi.org/10.1177/0145482X0309701006 },
abstract = { From a broad visionary perspective, this article examines three promising areas in which technology has the potential to revolutionize wayfinding for travelers who are blind: smart environments, smart consumers, and smart helpers. Its perspective is personal, based on the author's experience as the director of the Institute for Innovative Blind Navigation, and it is strongly influenced by the research and writings of popular futurists. }
}

@article{062bhat2022confused,
author = {Bhat, Prajna and Zhao, Yuhang},
title = {"I was Confused by It; It was Confused by Me:" Exploring the Experiences of People with Visual Impairments around Mobile Service Robots},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555582},
doi = {10.1145/3555582},
abstract = {Mobile service robots have become increasingly ubiquitous. However, these robots can pose potential accessibility issues and safety concerns to people with visual impairments (PVI). We sought to explore the challenges faced by PVI around mainstream mobile service robots and identify their needs. Seventeen PVI were interviewed about their experiences with three emerging robots: vacuum robots, delivery robots, and drones. We comprehensively investigated PVI's robot experiences by considering their different roles around robots---direct users and bystanders. Our study highlighted participants' challenges and concerns about the accessibility, safety, and privacy issues around mobile service robots. We found that the lack of accessible feedback made it difficult for PVI to precisely control, locate, and track the status of the robots. Moreover, encountering mobile robots as bystanders confused and even scared the participants, presenting safety and privacy barriers. We further distilled design considerations for more accessible and safe robots for PVI.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {481},
numpages = {26},
keywords = {visual impairments, mobile service robots, human-robot interaction, accessibility}
}

@INPROCEEDINGS{063yang2022seeway,
author={Yang, Zongming and Yang, Liang and Kong, Liren and Wei, Ailin and Leaman, Jesse and Brooks, Johnell and Li, Bing},
booktitle={IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={SeeWay: Vision-Language Assistive Navigation for the Visually Impaired}, 
year={2022},
volume={},
number={},
pages={52-58},
publisher={IEEE},
address={Prague},
abstract={Assistive navigation for blind or visually impaired (BVI) individuals is of significance to extend their mobility and safety in traveling, enhancing their employment opportunities and fostering personal fulfillment. Conventional research is mainly based on robotic navigation approaches through localization, mapping, and path planning frameworks. They require heavy manual annotation of semantic information in maps and its alignment with sensor mapping. Inspired by the fact that we human beings naturally rely on language instruction inquiry and visual scene understanding to navigate in an unfamiliar environment, this paper proposes a novel vision-language model-based approach for BVI navigation. It does not need heavy-labeled indoor maps and provides a Safe and Efficient E-Wayfinding (SeeWay) assistive solution for BVI individuals. The system consists of a scene-graph map construction module, a navigation path generation module for global path inference by vision-language navigation (VLN), and a navigation with obstacle avoidance module for real-time local navigation. The SeeWay system was deployed on portable iPhone devices with cloud computing assistance for the VLN model inference. The field tests show the effectiveness of the VLN global path finding and local path re-planning. Experiments and quantitative results reveal that heuristic-style instruction outperforms direction/detailed-style instructions for VLN success rate (SR), and the SR decreases as the navigation length increases.},
keywords={Visualization;Navigation;Semantics;Manuals;Robot sensing systems;Real-time systems;Path planning;The Blind or Visually Impaired;Assistive Devices;Scene-Graph Map;Vision-Language Navigation},
doi={10.1109/SMC53654.2022.9945087},
ISSN={2577-1655},
month={Oct},}

@Article{064tian2013glasses-unfamiliar,
author={Tian, YingLi and Yang, Xiaodong and Yi, Chucai and Arditi, Aries},
title={Toward a computer vision-based wayfinding aid for blind persons to access unfamiliar indoor environments},
journal={Machine Vision and Applications},
year={2013},
month={Apr},
day={01},
volume={24},
number={3},
pages={521-535},
abstract={Independent travel is a well-known challenge for blind and visually impaired persons. In this paper, we propose a proof-of-concept computer vision-based wayfinding aid for blind people to independently access unfamiliar indoor environments. In order to find different rooms (e.g. an office, a laboratory, or a bathroom) and other building amenities (e.g. an exit or an elevator), we incorporate object detection with text recognition. First, we develop a robust and efficient algorithm to detect doors, elevators, and cabinets based on their general geometric shape, by combining edges and corners. The algorithm is general enough to handle large intra-class variations of objects with different appearances among different indoor environments, as well as small inter-class differences between different objects such as doors and door-like cabinets. Next, to distinguish intra-class objects (e.g. an office door from a bathroom door), we extract and recognize text information associated with the detected objects. For text recognition, we first extract text regions from signs with multiple colors and possibly complex backgrounds, and then apply character localization and topological analysis to filter out background interference. The extracted text is recognized using off-the-shelf optical character recognition software products. The object type, orientation, location, and text information are presented to the blind traveler as speech.},
issn={1432-1769},
doi={10.1007/s00138-012-0431-7},
url={https://doi.org/10.1007/s00138-012-0431-7}
}

@Article{065nguyen2017phonerobot,
author={Nguyen, Quoc-Hung
and Vu, Hai
and Tran, Thanh-Hai
and Nguyen, Quang-Hoan},
title={Developing a way-finding system on mobile robot assisting visually impaired people in an indoor environment},
journal={Multimedia Tools and Applications},
year={2017},
month={Jan},
day={01},
volume={76},
number={2},
pages={2645-2669},
abstract={A way-finding system in an indoor environment consists of several components: localization, representation, path planning, and interaction. For each component, numerous relevant techniques have been proposed. However, deploying feasible techniques, particularly in real scenarios, remains challenging. In this paper, we describe a functional way-finding system deployed on a mobile robot to assist visual impairments (VI). The proposed system deploys state-of-the-art techniques that are adapted to the practical issues at hand. First, we adapt an outdoor visual odometry technique to indoor use by covering manual markers or stickers on ground-planes. The main purpose is to build reliable travel routes in the environment. Second, we propose a procedure to define and optimize the landmark/representative scenes of the environment. This technique handles the repetitive and ambiguous structures of the environment. In order to interact with VI people, we deploy a convenient interface on a smart phone. Three different indoor scenarios and thirteen subjects are conducted in our evaluations. Our experimental results show that VI people, particularly VI pupils, can find the right way to requested targets.},
issn={1573-7721},
doi={10.1007/s11042-015-3204-2},
url={https://doi.org/10.1007/s11042-015-3204-2}
}

@inproceedings{066kayukawa2020blindpilot,
author = {Kayukawa, Seita and Ishihara, Tatsuya and Takagi, Hironobu and Morishima, Shigeo and Asakawa, Chieko},
title = {BlindPilot: A Robotic Local Navigation System that Leads Blind People to a Landmark Object},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382925},
doi = {10.1145/3334480.3382925},
abstract = {Blind people face various local navigation challenges in their daily lives such as identifying empty seats in crowded stations, navigating toward a seat, and stopping and sitting at the correct spot. Although voice navigation is a commonly used solution, it requires users to carefully follow frequent navigational sounds over short distances. Therefore, we presented an assistive robot, BlindPilot, which guides blind users to landmark objects using an intuitive handle. BlindPilot employs an RGB-D camera to detect the positions of target objects and uses LiDAR to build a 2D map of the surrounding area. On the basis of the sensing results, BlindPilot then generates a path to the object and guides the user safely. To evaluate our system, we also implemented a sound-based navigation system as a baseline system, and asked six blind participants to approach an empty chair using the two systems. We observed that BlindPilot enabled users to approach a chair faster with a greater feeling of security and less effort compared to the baseline system.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {local navigation, robotic system, visual impairments},
location = {Honolulu, Hawaii, United States of Amerca},
series = {CHI EA '20}
}

@inproceedings{067zhao2020visualaudioglasses,
author = {Zhao, Yuhang and Kupferstein, Elizabeth and Rojnirun, Hathaitorn and Findlater, Leah and Azenkot, Shiri},
title = {The Effectiveness of Visual and Audio Wayfinding Guidance on Smartglasses for People with Low Vision},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376516},
doi = {10.1145/3313831.3376516},
abstract = {Wayfinding is a critical but challenging task for people who have low vision, a visual impairment that falls short of blindness. Prior wayfinding systems for people with visual impairments focused on blind people, providing only audio and tactile feedback. Since people with low vision use their remaining vision, we sought to determine how audio feedback compares to visual feedback in a wayfinding task. We developed visual and audio wayfinding guidance on smartglasses based on de facto standard approaches for blind and sighted people and conducted a study with 16 low vision participants. We found that participants made fewer mistakes and experienced lower cognitive load with visual feedback. Moreover, participants with a full field of view completed the wayfinding tasks faster when using visual feedback. However, many participants preferred audio feedback because of its shorter learning curve. We propose design guidelines for wayfinding systems for low vision.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {accessibility, audio feedback, augmented reality, low vision, visual feedback, wayfinding},
location = {Honolulu, Hawaii, United States of American},
series = {CHI '20}
}

@inproceedings{068fallah2012tactilelandmark,
author = {Fallah, Navid and Apostolopoulos, Ilias and Bekris, Kostas and Folmer, Eelke},
title = {The user as a sensor: navigating users with visual impairments in indoor spaces using tactile landmarks},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2207735},
doi = {10.1145/2207676.2207735},
abstract = {Indoor navigation systems for users who are visually impaired typically rely upon expensive physical augmentation of the environment or expensive sensing equipment; consequently few systems have been implemented. We present an indoor navigation system called Navatar that allows for localization and navigation by exploiting the physical characteristics of indoor environments, taking advantage of the unique sensing abilities of users with visual impairments, and minimalistic sensing achievable with low cost accelerometers available in smartphones. Particle filters are used to estimate the user's location based on the accelerometer data as well as the user confirming the presence of anticipated tactile landmarks along the provided path. Navatar has a high possibility of large-scale deployment, as it only requires an annotated virtual representation of an indoor environment. A user study with six blind users determines the accuracy of the approach, collects qualitative experiences and identifies areas for improvement.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {425–432},
numpages = {8},
keywords = {visual impairment, mobility, indoor navigation},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@misc{069zhang2019depthvio,
author = {Zhang, He and Jin, Lingqiu and Ye, Cang},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Workshop on Visual-Inertial Navigation: Challenges and Applications}, 
title = {A Depth-Enhanced Visual Intertial Odometry for a Robotic Navigation Aid for Blind People},
year={2019},
volume={},
number={},
address={Macau},
publisher={IEEE},
abstract={This paper presents a new method, called depth-enhanced visual-inertial odometry (DVIO), for real-time pose estimation of a robotic navigation aid (RNA) for assistive wayfinding. The method estimates the device pose by using an RGB-D camera and an inertial measurement unit (IMU). It extracts the floor plane from the camera’s depth data and tightly couple the floor plane, the visual features (with depth data from the RGB-D camera or unknown depth), and the IMU’s inertial data in a graph optimization framework for 6-DOF pose estimation. Due to use of the floor plane and the depth data from the RGB-D camera, the DVIO method has a better pose estimation accuracy than its VIO counterpart. To enable real-time computing on the RNA, the size of the sliding window for the graph optimization is reduced to trade some accuracy for computational efficiency. Experimental results demonstrate that the method achieved a pose estimation accuracy similar to that of the state of the art VIO but ran at a much faster speed (with a pose update rate of 18 Hz).},
keywords={},
month={Nov},}

@INPROCEEDINGS{070bhatlawande2013bracelet,
author={Bhatlawande, Shripad and Mahadevappa, Manjunatha and Mukhopadhyay, Jayanta},
booktitle={IEEE Point-of-Care Healthcare Technologies (PHT)}, 
title={Way-finding Electronic Bracelet for visually impaired people}, 
year={2013},
volume={},
number={},
publisher={IEEE},
address={Bangalore},
pages={260-263},
abstract={Way-finding Electronic Bracelet (WEB) for visually impaired (subject) is a portable embedded system for obstacle detection and way-finding. This MSP430G2553 processor based real-time system employs single Maxbotix MB1340 ultrasonic transceiver mounted on customized circular bracelet for detecting obstacle in the range 20 to 600 centimetres. Half duplex wireless communication is used for invoking vibrotactile and audio cues at receiver side. Maintaining a safety margin distance in front, left and right direction, WEB dynamically calculates obstacle distance (if any). Using on-demand hand movements, subject can understand surrounding situations and can perform successful way-finding. WEB system is available with choice of optimum hardware as per wearable comfort and requirement of subject. Preliminary trials on blindfolded subjects, WEB demonstrated substantial potential for cost-effective wearable real-time system with minimum physical interface for mobility of visually impaired people.},
keywords={Acoustics;Sensors;Receivers;Transmitters;Batteries;Legged locomotion;Transceivers},
doi={10.1109/PHT.2013.6461334},
ISSN={2377-5270},
month={16--18 Jan},}

@ARTICLE{071flores2015vibrotactile,
author={Flores, German and Kurniawan, Sri and Manduchi, Roberto and Martinson, Eric and Morales, Lourdes M. and Sisbot, Emrah Akin},
journal={IEEE Transactions on Haptics}, 
title={Vibrotactile Guidance for Wayfinding of Blind Walkers}, 
year={2015},
volume={8},
number={3},
pages={306-317},
abstract={We propose a vibrotactile interface in the form of a belt for guiding blind walkers. This interface enables blind walkers to receive haptic directional instructions along complex paths without negatively impacting users' ability to listen and/or perceive the environment the way some auditory directional instructions do. The belt interface was evaluated in a controlled study with 10 blind individuals and compared to the audio guidance. The experiments were videotaped and the participants' behaviors and comments were content analyzed. Completion times and deviations from ideal paths were also collected and statistically analyzed. By triangulating the quantitative and qualitative data, we found that the belt resulted in closer path following to the expense of speed. In general, the participants were positive about the use of vibrotactile belt to provide directional guidance.},
keywords={Assistive technology;Vibrations;DC motors;Haptic interfaces;Global Positioning System;Legged locomotion;Assistive technology;haptic belt;wayfinding.;Assistive technology;haptic belt;wayfinding},
doi={10.1109/TOH.2015.2409980},
ISSN={2329-4051},
month={July},}

@INPROCEEDINGS{072kulyukin2008iwalker,
author={Kulyukin, Vladimir and Kutiyanawala, Aliasgar and LoPresti, Edmund and Matthews, Judith and Simpson, Richard},
booktitle={IEEE International Conference on RFID}, 
title={iWalker: Toward a Rollator-Mounted Wayfinding System for the Elderly}, 
year={2008},
volume={},
number={},
pages={303-311},
publisher={IEEE},
address={Las Vegas},
location={Las Vegas, Nevada, United States},
abstract={Research on intelligent walkers aims at helping elderly individuals to maintain their independence in familiar and unfamiliar environments. Several walkers have been developed by researchers at Carnegie Mellon University and the University of Pittsburgh. This article contributes to this research venue by describing the design and initial evaluations of iWalker, a multi-sensor rollator-mounted wayfinding system for the elderly. The primary difference of the proposed navigation aid from other intelligent walkers is that iWalker is assumed to operate in a smart world (SW), a physical space equipped with embedded sensors. By integrating inexpensive sensors into the environment, the cost and complexity of the walker can be reduced.},
keywords={Senior citizens;Legged locomotion;Intelligent sensors;Sonar navigation;Computer science;Cognitive robotics;Laser feedback;Sonar detection;Radiofrequency identification;USA Councils},
doi={10.1109/RFID.2008.4519363},
ISSN={2374-0221},
month={April},}

@ARTICLE{073liu2024dragon,
author={Liu, Shuijing and Hasan, Aamir and Hong, Kaiwen and Wang, Runxuan and Chang, Peixin and Mizrachi, Zachary and Lin, Justin and McPherson, D. Livingston and Rogers, Wendy A. and Driggs-Campbell, Katherine},
journal={IEEE Robotics and Automation Letters}, 
title={DRAGON: A Dialogue-Based Robot for Assistive Navigation With Visual Language Grounding}, 
year={2024},
volume={9},
number={4},
pages={3712-3719},
abstract={Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form language to the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, provide a good guiding experience, and connect users with their surrounding environment in an intuitive manner.},
keywords={Robots;Navigation;Semantics;Grounding;Visualization;Robot vision systems;Cameras;Human-centered robotics;natural dialog for HRI;AI-enabled robotics},
doi={10.1109/LRA.2024.3362591},
ISSN={2377-3766},
month={April},}

@InProceedings{074oh2017indoornav,
author="Oh, Yeonju
and Kao, Wei-Liang
and Min, Byung-Cheol",
editor="Stephanidis, Constantine",
title="Indoor Navigation Aid System Using No Positioning Technique for Visually Impaired People",
booktitle="HCI International 2017 -- Posters' Extended Abstracts",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="390--397",
abstract="We propose a novel navigation aid system that assists the visually impaired to travel in unfamiliar indoor environments independently. The main idea of the proposed system is that it does not employ any of indoor positioning techniques. Instead, the system generates shoreline based optimal paths including a series of recognizable landmarks and detailed instructions, which enables the visually impaired to navigate to their destinations by listening the instructions on their smartphones. The paths and instructions are generated on a computer installed on an indoor kiosk where the visually impaired enters his or her destination, and then the generated instructions are wirelessly transfered to the user's smartphone. To validate the proposed system, we developed an Android-based smartphone App and conducted the user study with three visually impaired participants. The study shows that the proposed idea is feasible, useful and potential.",
isbn="978-3-319-58753-0"
}

@InProceedings{075carrasco2014argus,
author="Carrasco, Eduardo
and Loyo, Est{\'i}baliz
and Otaegui, Oihana
and F{\"o}sleitner, Claudia
and Dubielzig, Markus
and Olmedo, Rafael
and Wasserburger, Wolfgang
and Spiller, John",
editor="Miesenberger, Klaus
and Fels, Deborah
and Archambault, Dominique
and Pe{\v{n}}{\'a}z, Petr
and Zagler, Wolfgang",
title="ARGUS Autonomous Navigation System for People with Visual Impairments",
booktitle="Computers Helping People with Special Needs",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="100--107",
abstract="This work addresses the challenge of designing an effective, reliable and affordable autonomous navigation system for blind and visually impaired people which also covers journey planning and post journey activities (such as recommendations and experiences sharing) . The main contribution focuses on the integration of accurate real-time user positioning data with binaural 3D audio based guiding techniques on mobile devices and a web services delivering platform. The aim is to produce an autonomous navigation system that can be used to guide targeted users along pre-defined tracks and that can be used also before and after the journey to carry out several related tasks such as journey planning, training and sharing of experiences. A preliminary prototype of this concept has been built and tested with 4 end users in both rural and urban environments, obtaining encouraging results.",
isbn="978-3-319-08599-9"
}

@InProceedings{076padmanaban2017densitystudy,
author="Padmanaban, Rajchandar
and Krukar, Jakub",
editor="Gartner, Georg
and Huang, Haosheng",
title="Increasing the Density of Local Landmarks in Wayfinding Instructions for the Visually Impaired",
booktitle="Progress in Location-Based Services 2016",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="131--150",
abstract="Multiple approaches to support non-visual navigation have been proposed, of which traditional auditory turn-by-turn navigational systems achieved high popularity. Despite being modified according to the needs of visually impaired users, the underlying dataset communicated to the wayfinder is sourced primarily from traditional POI databases which are of limited use to blind navigators. This work proposes the use of environmental features spontaneously detected by blind navigators during their everyday locomotion as `local landmarks' for enriching auditory navigational instructions. We report results of a survey which served to identify such environmental features. Consequently, we propose a list of potential local landmarks for the blind. Next, in a usability study, we demonstrate that enriching traditional turn-by-turn auditory instructions with local landmarks can improve the subjective satisfaction and confidence in navigation. Results indicate that the improvements seem to be achieved even without increasing the subjective complexity of the instructions. Finally we discuss how using local landmarks to enrich auditory navigational instructions can benefit visually impaired users.",
isbn="978-3-319-47289-8"
}

@InProceedings{077lee2016smartactilemap,
author="Lee, MyungJoong
and Hwang, Jie-Eun",
editor="Stephanidis, Constantine",
title="smarTactile Map: An Interactive and Smart Map to Help the Blind to Navigate by Touch",
booktitle="HCI International 2016 -- Posters' Extended Abstracts",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="372--378",
abstract="Tactile map is a popular method for the visually impaired to help their independent walking. It is made of protruding dot, line, surface and braille. Visually impaired people can recognize the structure of space by touching a tactile map. However, it is not smart enough to use in their real life. We carried on user interviews with thirty subjects of visually impaired people to define problems from the real situation. In addition, we conducted a field observation how they utilized the tactile map. We analyzed their touching behaviors and reclaimed the problems of existing tactile maps. Based on these findings, we designed prototype of new tactile map with 3D volumetric symbol system, which is potentially deployable to an interactive braille device. We proposed a new way of representing space for navigation with the 3D symbol system. The landmarks and routes are customized by the end users through dynamically scaled symbols with consistency of reading. Adapted by the user's level of vision, familiarity of the space, and the smart tactile map can customize representation of space so then reinforce perception of the place. We conducted an initial usability evaluation of the new system and discussed about fundamental benefits of such map system.",
isbn="978-3-319-40542-1"
}

@InProceedings{078nair2019assist,
author="Nair, Vishnu
and Budhai, Manjekar
and Olmschenk, Greg
and Seiple, William H.
and Zhu, Zhigang",
editor="Leal-Taix{\'e}, Laura
and Roth, Stefan",
title="ASSIST: Personalized Indoor Navigation via Multimodal Sensors and High-Level Semantic Information",
booktitle="Computer Vision -- ECCV 2018 Workshops",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="128--143",
abstract="Blind {\&} visually impaired (BVI) individuals and those with Autism Spectrum Disorder (ASD) each face unique challenges in navigating unfamiliar indoor environments. In this paper, we propose an indoor positioning and navigation system that guides a user from point A to point B indoors with high accuracy while augmenting their situational awareness. This system has three major components: location recognition (a hybrid indoor localization app that uses Bluetooth Low Energy beacons and Google Tango to provide high accuracy), object recognition (a body-mounted camera to provide the user momentary situational awareness of objects and people), and semantic recognition (map-based annotations to alert the user of static environmental characteristics). This system also features personalized interfaces built upon the unique experiences that both BVI and ASD individuals have in indoor wayfinding and tailors its multimodal feedback to their needs. Here, the technical approach and implementation of this system are discussed, and the results of human subject tests with both BVI and ASD individuals are presented. In addition, we discuss and show the system's user-centric interface and present points for future work and expansion.",
isbn="978-3-030-11024-6"
}

@InProceedings{079li2016isana,
author="Li, Bing
and Mu{\~{n}}oz, J. Pablo
and Rong, Xuejian
and Xiao, Jizhong
and Tian, Yingli
and Arditi, Aries",
editor="Hua, Gang
and J{\'e}gou, Herv{\'e}",
title="ISANA: Wearable Context-Aware Indoor Assistive Navigation with Obstacle Avoidance for the Blind",
booktitle="Computer Vision -- ECCV 2016 Workshops",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="448--462",
abstract="This paper presents a novel mobile wearable context-aware indoor maps and navigation system with obstacle avoidance for the blind. The system includes an indoor map editor and an App on Tango devices with multiple modules. The indoor map editor parses spatial semantic information from a building architectural model, and represents it as a high-level semantic map to support context awareness. An obstacle avoidance module detects objects in front using a depth sensor. Based on the ego-motion tracking within the Tango, localization alignment on the semantic map, and obstacle detection, the system automatically generates a safe path to a desired destination. A speech-audio interface delivers user input, guidance and alert cues in real-time using a priority-based mechanism to reduce the user's cognitive load. Field tests involving blindfolded and blind subjects demonstrate that the proposed prototype performs context-aware and safety indoor assistive navigation effectively.",
isbn="978-3-319-48881-3"
}

@InProceedings{080fauzul2021spatialultrasonic,
author="Fauzul, Muhd Amin Hj
and Salleh, Noor Deenina Hj Mohd",
editor="Suhaili, Wida Susanty Haji
and Siau, Nor Zainah
and Omar, Saiful
and Phon-Amuaisuk, Somnuk",
title="Navigation for the Vision Impaired with Spatial Audio and Ultrasonic Obstacle Sensors",
booktitle="Computational Intelligence in Information Systems",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="43--53",
abstract="We develop a visual assistive system to aid the visually impaired users for safe and convenient navigation in indoor and outdoor environments. The system has two main components: a mobile app and an obstacle sensor. The mobile app makes use of the microelectromechanical sensors inside the smartphone, location services, and Google Maps to provide audio cues directions for the user. The obstacle sensor employs ultrasonic sensors to detect obstacles and provide haptic feedback. The obstacle avoidance device can be attached to a traditional probing cane, and the device vibrates the probing cane handle at different intensity according to the nature of obstacles. Spatial sound cues are generated based on the spatial distance and direction of the current location to the desired destination. We discuss the system design and report the testing to evaluate the device effectiveness for obstacle distance feedback as well as the effectiveness of the spatial sound cues for navigation feedback.",
isbn="978-3-030-68133-3"
}

@InProceedings{081als2018blukane,
author="Als, Adrian
and King, Adrian
and Johnson, Kevin
and Sargeant, Ramon",
editor="Miesenberger, Klaus
and Kouroupetroglou, Georgios",
title="BluKane: An Obstacle Avoidance Navigation App to Assist the Visually Impaired",
booktitle="Computers Helping People with Special Needs",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="36--43",
abstract="In the Caribbean, the cost of technological aids to assist visually impaired persons with navigation is extravagant. Moreover, a recent effort by a student at the University of the West Indies (Cave Hill Campus) to develop a low-cost SmartCane solution for this vulnerable group has been met with limited success. The SmartCane system which uses an ultra-sonic device mounted on a traditional white cane communicates the proximity of objects to the user via vibrations on a Bluetooth connected mobile device. However, the lack of a user-friendly interface, for the mobile application hinders the overall user experience. This research is aimed at developing a new mobile application that will improve the user experience. Application requirements will be gathered through the administration of surveys to visually impaired members of the Barbados Association for the Deaf and Blind. The mobile application is expected to provide visually impaired individuals with assistance in navigating to their desired destinations. Moreover when the mobile device is paired with the SmartCane system users will also have the benefit of obstacle avoidance. This student-led effort to integrate this vulnerable group into society is poised to move beyond the classroom and bring greater awareness of the need for low-cost assistive technological solutions in the Caribbean region.",
isbn="978-3-319-94274-2"
}

@InProceedings{082kumar2019iot,
author="Kumar, Aashis
and Jain, Monika
and Saxena, Rahul
and Jain, Vidyanshu
and Jaidka, Siddharth
and Sadana, Tushar",
editor="Bera, Rabindranath
and Sarkar, Subir Kumar
and Singh, Om Prakash
and Saikia, Hemanta",
title="IOT-Based Navigation for Visually Impaired Using Arduino and Android",
booktitle="Advances in Communication, Devices and Networking",
year="2019",
publisher="Springer Singapore",
address="Singapore",
pages="499--508",
abstract="Like many other projects, ultrasonic sensors have been used in devices that enable a visually impaired person to tackle obstacles. However, little has been done to expand the navigation in a broader context, and to enable the visually impaired person to go about hands-free and connect with the world like normal people do. This paper is derived from the in-depth study of the real-life scenarios and problem faced and proposes an advancement of the existing smart stick technology. The system is designed to act like a navigation guide through voice outputs consisting of ultrasonic sensors, Arduino Uno R3, HC05 Bluetooth module, and an Android application. The Android application is used as a navigation tool to calculate the shortest distance from the source and the location, and the ultrasonic sensors attached to the person's knees look out for obstacles while the blind person walks as per the directions narrated to him on an earphone, and upon detection of an obstacle, the HC05 triggers a warning and the Android application describes the environment to the person. With this project, we seek to take a step closer in narrowing the gap between blind people and us while keeping it within the reach of even the poorest.",
isbn="978-981-13-3450-4"
}

@InProceedings{083hussain2021designnavigation,
author="Hussain, Adedoyin A.
and Al-Turjman, Fadi
and Gemikonakli, Eser
and Ever, Yoney Kirsal",
editor="Ever, Enver
and Al-Turjman, Fadi",
title="Design of a Navigation System for the Blind/Visually Impaired",
booktitle="Forthcoming Networks and Sustainability in the IoT Era",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="25--45",
abstract="Since individuals with needs in the general public increased, the work introduced is a navigation system that will give a solid and durable obstacle detection and environmental imager and navigation for the user. It provides minimal cost system to permit navigation. The obstacle detection is to distinguish the deterrent and guide the visually impaired (VIP) about a suitable pathway. The framework utilises sensor based obstacle detection, and sends back buzzer or audio sound as a reaction that warns the VIP about position. The primary technique utilised by each blind or visually impaired is the strolling stick for identifying deterrent in which its functionality is restricted, it doesn't secure territories close to the head let alone all obstacle. This framework acquires data about impediments close to the head and provides the right pathway for the VIP. When utilised with a mobile stick, the VIP is completely ensured against a snag, and the route is made simple. The environmental imager and navigation mode is the sound and visual guide for the VIP which permits users to just touch a button and proposed destination to the caregiver. This includes GPS and live video feed direction. The general system is versatile and can be conveyed by a VIP. The accuracy achieved for the system differs from 94.15{\$}{\$}{\backslash}{\%}{\$}{\$}{\%}to 99.72{\$}{\$}{\backslash}{\%}{\$}{\$}{\%}. The percentage rate of the snag discovery for either indoor or outside varies from 95.40{\$}{\$}{\backslash}{\%}{\$}{\$}{\%}to 99.67{\$}{\$}{\backslash}{\%}{\$}{\$}{\%}. This examination will Increase the VIP mobility significantly.",
isbn="978-3-030-69431-9"
}

@InProceedings{084seervi2023snavi,
author="Seervi, Madhu R.
and Mukhopadhyay, Adwitiya",
editor="Sharma, Sanjay
and Subudhi, Bidyadhar
and Sahu, Umesh Kumar",
title="SNAVI: A Smart Navigation Assistant for Visually Impaired",
booktitle="Intelligent Control, Robotics, and Industrial Automation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="893--905",
abstract="Our lives are made easier by automated solutions based on the Internet of Things (IoT). Navigating from one place to another can be challenging for blind people. IoT can increase navigational confidence while simultaneously decreasing reliance on others. The goal is to make the device less bulky and to aid them with two ultrasonic sensors and a flame sensor, which will be cost-effective for blind people who can comfortably travel both indoors and outdoors with minimum sensor use. SNAVI: A Smart Navigation Assistant for the Visually Impaired might be molded into a stick for visually impaired persons so that they receive a notification through their headphones when a barrier is identified in front of them via two ultrasonic sensors, as well as the height of the obstacle. The obstruction might be stationary or moving, and the system can detect fire through a flame IR sensor and send voice alerts. This method helps vision-impaired people travel about with less stress.",
isbn="978-981-99-4634-1",
doi="10.1007/978-981-99-4634-1_70"
}

@Inbook{085kandil2020amie,
author="Kandil, Marwa
and AlAttar, Fatemah
and Al-Baghdadi, Reem
and Damaj, Issam",
title="AmIE: An Ambient Intelligent Environment for Blind and Visually Impaired People",
bookTitle="Technological Trends in Improved Mobility of the Visually Impaired",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="207--236",
abstract="The Internet of things (IoT) is a revolutionary technology that is rapidly changing our world. IoT systems are becoming essential in modern life; traditional devices are becoming ubiquitous, connected, wireless, and smart. The aim of this investigation is to develop an IoT context-aware system that creates an Ambient Intelligence environment; in an apartment, house, or a building; to assist blind, visually-impaired, and elderly people. The main challenges that the system addresses include supporting assisted navigation, communication, security, and safe-living at an affordable cost. The proposed solution aims at providing effective indoor navigation, an easy-to-deploy system, a multilingual user-friendly interface and supports speech instructions. The proposed system is supported by sensors, communication nodes, control processors, wireless Internet access, a smart watch, to name but a few. The chapter includes studying the effectiveness of adding Bluetooth low energy devices at doors, stairs, walls, exit signs, service rooms, and more to enable accurate navigation. In addition, an investigation is included on the training needed by the users as related to the adoption of such a modern tool. System testing includes multiple navigation scenarios, accuracies, reliability of interactions, and the effectiveness of the multilingual features. System testing, analysis, and evaluation confirm the effectiveness of the developed system in application.",
isbn="978-3-030-16450-8",
doi="10.1007/978-3-030-16450-8_9",
url="https://doi.org/10.1007/978-3-030-16450-8_9"
}

@Inbook{086basori2020hapar,
author="Basori, Ahmad Hoirul",
title="HapAR: Handy Intelligent Multimodal Haptic and Audio-Based Mobile AR Navigation for the Visually Impaired",
bookTitle="Technological Trends in Improved Mobility of the Visually Impaired",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="319--334",
abstract="Visually impaired people have suffered greatly on finding the right direction toward their destination. This paper initiates an innovative and low-cost solution by providing mobile Augmented Reality (HapAR) that is capable to stimulate haptic and audio sensation for guiding them inside the campus. The direction is generated using the geo-location of the building and current position of the user. The initial testing was conducted inside the campus and successfully gives a promising result. They found the system was easy to use by pointing out the mobile devices and they can feel the vibration when the user is out of track and hear the voice assistant to correct their track.",
isbn="978-3-030-16450-8",
doi="10.1007/978-3-030-16450-8_13",
url="https://doi.org/10.1007/978-3-030-16450-8_13",
}

@InProceedings{087fagernes2018hapticnav,
author="Fagernes, Siri
and Gr{\o}nli, Tor-Morten",
editor="Kurosu, Masaaki",
title="Navigation for Visually Impaired Using Haptic Feedback",
booktitle="Human-Computer Interaction. Interaction in Context",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="347--356",
abstract="Smartphones have become commodity tools and exist in large multiple in all parts of the population. The typical use of navigation applications focus on aiding users with no impairments. Guidance applications with enhancing features, and facilitating for i.e. people with visual impairments, focus on voice-based feedback. In this paper we focus on the use of haptic feedback as a tool and guidance for navigation, through utilising the vibration mechanism available in mobile phones. Through the development of a prototype application we illustrate how haptic feedback can be used to guide users in cases of visual impairment or hindrance. The preliminary results display a novel contribution to multi-modal navigation and exemplifies active use of receptors for smartphone feedback interpretation.",
isbn="978-3-319-91244-8"
}

@InProceedings{088sharma2021anavi,
author="Sharma, Arjun
and Vasan, Vivek Ram
and Prasanna Bharathi, S.",
editor="Dash, Subhransu Sekhar
and Das, Swagatam
and Panigrahi, Bijaya Ketan",
title="ANAVI: Advanced Navigation Assistance for Visually Impaired",
booktitle="Intelligent Computing and Applications",
year="2021",
publisher="Springer Singapore",
address="Singapore",
pages="285--296",
abstract="The advancement in technology over the decades has provided for exceptional solutions and aid for the visually impaired. However, they are standalone solutions that target specific problems encountered by visually impaired individuals. This paper addresses some of the most common problems faced by the visually impaired and to provide an innovative and inexpensive solution that collectively addresses the problems. The solution proposed in this paper makes use of RF communication, AWS (Amazon Web Services) IoT (Internet of Things) core, and Computer Vision to aid visually impaired people to detect an object and use public transportation independently. Computer vision labels identify the obstacles or objects in front to aid the visually impaired individual to navigate through rough terrain. The problem faced by the visually impaired is access to public transport, which a majority of them use to commute daily as it is the only viable mobility option in our country. The visually impaired are forced to always rely on other individuals to help them travel making them feel dependent. The other problem is when navigating the streets or in their home, it is hard to recognize the type of object and the distance at which it is in front of them. Hence, there is a system that is required to allow them to travel independently and help them feel empowered. The handheld unit will be connected to the bus stop modules using RF module and Py-camera. The bus stop modules will have an ESP 8266 which provides the Internet connection. The modules and the buses will be connected using the services of AWS IoT core. Even though there are many ways for the visually impaired individuals to navigate the streets, as the environment and the objects around us change rapidly, it is close to impossible for them to spot using conventional methods and that is where this system comes in handy. For one such instance, an open drainage, which is 1 by 1 foot wide, cannot be detected using the tap and go method or sound sensing methods while the system has a pretrained dataset which can send a voice alert saying there is an open drain at 20 m in front and a vibration-based alert stimulation is given at the stick handle to help circumnavigate the open drain.",
isbn="978-981-15-5566-4"
}

@InProceedings{089megalingam2015soundandtouchcane,
author="Megalingam, Rajesh Kannan
and Nambissan, Aparna
and Thambi, Anu
and Gopinath, Anjali
and Megha, K.",
editor="Jain, Lakhmi C.
and Patnaik, Srikanta
and Ichalkaranje, Nikhil",
title="Sound- and Touch-Based Smart Cane: Better Walking Experience for Visually Challenged",
booktitle="Intelligent Computing, Communication and Devices",
year="2015",
publisher="Springer India",
address="New Delhi",
pages="589--595",
abstract="Moving with the help of a white cane is an elusive task for the visually challenged unless they create a mental route map with recognizable reference elements. The Smart Cane is intended to provide the visually challenged a better walking experience. The design is incorporated with Bluetooth-enabled obstacle detection module, supported with heat detection and haptic modules. The ultrasonic range finders help in detecting obstacles. The calculated distance is send to an android device via Bluetooth. The user gets voice alerts about the distance through Bluetooth headset. Haptics module is designed to warn the user of moving obstacles with the help of vibratory motors. The goal of this project is to arm its wielder with the functional support of a walking cane without having to possess one physically.",
isbn="978-81-322-2012-1"
}

@InProceedings{090vashisth2022environment,
author="Vashisth, Tushar
and Khareta, Ritika
and Bhati, Nishi
and Samsani, Venkata Chanakya
and Sharma, Shanu",
editor="Bianchini, Monica
and Piuri, Vincenzo
and Das, Sanjoy
and Shaw, Rabindra Nath",
title="A Low Cost and Enhanced Assistive Environment for People with Vision Loss",
booktitle="Advanced Computing and Intelligent Technologies",
year="2022",
publisher="Springer Singapore",
address="Singapore",
pages="245--255",
abstract="Usually, people with vision disabilities can be seen using white canes, which are static devices and are used for navigation around. The limitation of commonly used static mobility devices is their inability to perceive the surroundings around them. To provide a real-time visual experience to people with vision loss, a system with dynamic abilities is required. With this aim, in this paper, an advanced vision-enhancing architecture is proposed for visually impaired people to provide them a real-time experience of the surroundings along with navigation assistance through voice rendering. The framework of the proposed system is based on advanced technologies like IoT (Internet of Things), Machine learning, and Computer Vision. The proposed architecture is aimed to build a low-cost wearable device that will act as a guiding agent for a visually impaired person so that the individual can get to know about the things around with precision and ease both in an indoor as well as outdoor environment.",
isbn="978-981-16-2164-2",
doi="10.1007/978-981-16-2164-2_20"
}

@InProceedings{091elgendy2018phone/survey,
author="Elgendy, Mostafa
and Sik Lanyi, Cecilia",
editor="Miesenberger, Klaus
and Kouroupetroglou, Georgios",
title="Review on Smart Solutions for People with Visual Impairment",
booktitle="Computers Helping People with Special Needs",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="81--84",
abstract="Nowadays, over a billion people are estimated to be living with disabilities. The lack of support services make them overly dependent on their families and prevent them from being socially included. A good solution is to use Mobile Assistive Technologies (MAT) to perform tasks in everyday lives, but one of the most important and challenging tasks is to create a solution which offers the assistance and support they need to achieve a good quality of life and allows them to participate in social life. This paper reviews researches within the field of MATs to help people with visual impairment in their daily activities like navigation and shopping.",
isbn="978-3-319-94277-3"
}

@ARTICLE{092islam2019walkers/survey,
author={Islam, Md. Milon and Sheikh Sadi, Muhammad and Zamli, Kamal Z. and Ahmed, Md. Manjur},
journal={IEEE Sensors Journal}, 
title={Developing Walking Assistants for Visually Impaired People: A Review}, 
year={2019},
volume={19},
number={8},
pages={2814-2828},
abstract={The development of walking assistants for visually impaired people has become a prominent research area due to the rapid growth of these individuals in recent decades. Although numerous frameworks have been developed to aid visually impaired people, a considerable portion of these is limited in their scopes. In this review, we exhibit a similar review of walking assistants for visually impaired people to demonstrate the advancement of such technologies. This review discusses the recent innovative technologies developed for the visually impaired to aid them in walking with their merits and demerits. With the help of this review, a schema is drawn for upcoming development in the field of sensors, computer vision, and smartphone-based walking assistants. This review aims to present the majority of the issues of such frameworks to serve as a basis for different researchers to develop walking assistants that ensure movability and safety of visually impaired people.},
keywords={Legged locomotion;Sensors;Navigation;Computer vision;Visualization;Blindness;Tools;Visually impaired people;walking assistants;computer vision;sensors;smartphone;electronic travel aid;navigation;review},
doi={10.1109/JSEN.2018.2890423},
ISSN={1558-1748},
month={April},}

@inproceedings{093ranganeni2023levelsofcontrol,
author = {Ranganeni, Vinitha and Sinclair, Mike and Ofek, Eyal and Miller, Amos and Campbell, Jonathan and Kolobov, Andrey and Cutrell, Edward},
title = {Exploring Levels of Control for a Navigation Assistant for Blind Travelers},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3578630},
doi = {10.1145/3568162.3578630},
abstract = {Only a small percentage of blind and low-vision people use traditional mobility aids such as a cane or a guide dog. Various assistive technologies have been proposed to address the limitations of traditional mobility aids. These devices often give either the user or the device majority of the control. In this work, we explore how varying levels of control affect the users' sense of agency, trust in the device, confidence, and successful navigation. We present Glide, a novel mobility aid with two modes for control: Glide-directed and User-directed. We employ Glide in a study (N=9) in which blind or low-vision participants used both modes to navigate through an indoor environment. Overall, participants found that Glide was easy to use and learn. Most participants trusted Glide despite its current limitations, and their confidence and performance increased as they continued to use Glide. Users' control mode preferences varied in different situations; no single mode "won" in all situations.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {4–12},
numpages = {9},
keywords = {assistive navigation, robotics, user study},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@INPROCEEDINGS{094wang2023quadruped,
author={Wang, Luyao and Chen, Qihe and Zhang, Yan and Li, Ziang and Yan, Tingmin and Wang, Fan and Zhou, Guyue and Gong, Jiangtao},
booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Can Quadruped Guide Robots be Used as Guide Dogs?}, 
year={2023},
volume={},
number={},
publisher={IEEE},
address={Detroit, MI, USA},
pages={4094-4100},
keywords={Legged locomotion;Navigation;Dogs;Market research;Biology;Quadrupedal robots;Usability},
doi={10.1109/IROS55552.2023.10341792}
}

@Article{095yang2022unav,
AUTHOR = {Yang, Anbang and Beheshti, Mahya and Hudson, Todd E. and Vedanthan, Rajesh and Riewpaiboon, Wachara and Mongkolwat, Pattanasak and Feng, Chen and Rizzo, John-Ross},
TITLE = {UNav: An Infrastructure-Independent Vision-Based Navigation System for People with Blindness and Low Vision},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {22},
ARTICLE-NUMBER = {8894},
NUMPAGES = {20},
URL = {https://www.mdpi.com/1424-8220/22/22/8894},
PubMedID = {36433501},
ISSN = {1424-8220},
ABSTRACT = {Vision-based localization approaches now underpin newly emerging navigation pipelines for myriad use cases, from robotics to assistive technologies. Compared to sensor-based solutions, vision-based localization does not require pre-installed sensor infrastructure, which is costly, time-consuming, and/or often infeasible at scale. Herein, we propose a novel vision-based localization pipeline for a specific use case: navigation support for end users with blindness and low vision. Given a query image taken by an end user on a mobile application, the pipeline leverages a visual place recognition (VPR) algorithm to find similar images in a reference image database of the target space. The geolocations of these similar images are utilized in a downstream task that employs a weighted-average method to estimate the end user’s location. Another downstream task utilizes the perspective-n-point (PnP) algorithm to estimate the end user’s direction by exploiting the 2D–3D point correspondences between the query image and the 3D environment, as extracted from matched images in the database. Additionally, this system implements Dijkstra’s algorithm to calculate a shortest path based on a navigable map that includes the trip origin and destination. The topometric map used for localization and navigation is built using a customized graphical user interface that projects a 3D reconstructed sparse map, built from a sequence of images, to the corresponding a priori 2D floor plan. Sequential images used for map construction can be collected in a pre-mapping step or scavenged through public databases/citizen science. The end-to-end system can be installed on any internet-accessible device with a camera that hosts a custom mobile application. For evaluation purposes, mapping and localization were tested in a complex hospital environment. The evaluation results demonstrate that our system can achieve localization with an average error of less than 1 m without knowledge of the camera’s intrinsic parameters, such as focal length.},
DOI = {10.3390/s22228894}
}

@InProceedings{096hao2023detectandapproach,
author="Hao, Yu
and Feng, Junchi
and Rizzo, John-Ross
and Wang, Yao
and Fang, Yi",
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="Detect and Approach: Close-Range Navigation Support for People with Blindness and Low Vision",
booktitle="Computer Vision -- ECCV 2022 Workshops",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="607--622",
abstract="People with blindness and low vision (pBLV) experience significant challenges when locating final destinations or targeting specific objects in unfamiliar environments. Furthermore, besides initially locating and orienting oneself to a target object, approaching the final target from one's present position is often frustrating and challenging, especially when one drifts away from the initial planned path to avoid obstacles. In this paper, we develop a novel wearable navigation solution to provide real-time guidance for a user to approach a target object of interest efficiently and effectively in unfamiliar environments. Our system contains two key visual computing functions: initial target object localization in 3D and continuous estimation of the user's trajectory, both based on the 2D video captured by a low-cost monocular camera mounted on in front of the chest of the user. These functions enable the system to suggest an initial navigation path, continuously update the path as the user moves, and offer timely recommendation about the correction of the user's path. Our experiments demonstrate that our system is able to operate with an error of less than 0.5 m both outdoor and indoor. The system is entirely vision-based and does not need other sensors for navigation, and the computation can be run with the Jetson processor in the wearable system to facilitate real-time navigation assistance.",
isbn="978-3-031-25075-0"
}

@INPROCEEDINGS{097mohammadi2018pathplanning,
author={Mohammadi, Mehdi and Al-Fuqaha, Ala and Oh, Jun-Seok},
booktitle={IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)}, 
title={Path Planning in Support of Smart Mobility Applications Using Generative Adversarial Networks}, 
year={2018},
volume={},
number={},
pages={878-885},
publisher={IEEE},
address={Halifax, Canada},
abstract={This paper describes and evaluates the use of Generative Adversarial Networks (GANs) for path planning in support of smart mobility applications such as indoor and outdoor navigation applications, individualized wayfinding for people with disabilities (e.g., vision impairments, physical disabilities, etc.), path planning for evacuations, robotic navigations, and path planning for autonomous vehicles. We propose an architecture based on GANs to recommend accurate and reliable paths for navigation applications. The proposed system can use crowd-sourced data to learn the trajectories and infer new ones. The system provides users with generated paths that help them navigate from their local environment to reach a desired location. As a use case, we experimented with the proposed method in support of a wayfinding application in an indoor environment. Our experiments assert that the generated paths are correct and reliable. The accuracy of the classification task for the generated paths is up to 99% and the quality of the generated paths has a mean opinion score of 89%.},
keywords={Generative adversarial networks;Gallium nitride;Navigation;Generators;Trajectory;Data models;Path planning;smart mobility;wayfinding;intelligent transportation systems;generative adversarial network;GAN;Internet of Things;smart cities},
doi={10.1109/Cybermatics_2018.2018.00168},
ISSN={},
month={July},}


@inproceedings{098nanavati2018coupled,
author = {Nanavati, Amal and Tan, Xiang Zhi and Steinfeld, Aaron},
title = {Coupled Indoor Navigation for People Who Are Blind},
year = {2018},
isbn = {9781450356152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173386.3176976},
doi = {10.1145/3173386.3176976},
abstract = {This paper presents our design of an autonomous navigation system for a mobile robot that guides people who are blind and low vision in indoor settings. It begins by presenting user studies that shaped our design of the system, moves on to describing our model of human-robot coupled motion, and concludes by describing our autonomous navigation system.},
booktitle = {Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {201–202},
numpages = {2},
keywords = {human-robot interaction, autonomous navigation, accessibility},
location = {Chicago, IL, USA},
series = {HRI '18}
}

@inproceedings{099tan2017manipulator,
author = {Tan, Xiang Zhi and Steinfeld, Aaron},
title = {Using Robot Manipulation to Assist Navigation by People Who Are Blind or Low Vision},
year = {2017},
isbn = {9781450348850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3029798.3034808},
doi = {10.1145/3029798.3034808},
abstract = {This work explores the capability of bi-directional manipulation during assistance to blind or low vision users. We describe a haptic approach that utilizes the manipulator arm to support interaction and provide navigational information supplemented with landmarks and spatial cues to users.},
booktitle = {Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {379–380},
numpages = {2},
keywords = {indoor navigation, haptic interface, assistive robots},
location = {Vienna, Austria},
series = {HRI '17}
}

@article{100xmalmeida2015tactilemaps,
title = {Analysis of Wayfinding Strategies of Blind People Using Tactile Maps},
journal = {Procedia Manufacturing},
volume = {3},
pages = {6020-6027},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.716},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915007179},
author = {Maria de Fatima X.M. Almeida and Laura B. Martins and Francisco J. Lima},
keywords = {Tactile map, Wayfinding, Ergonomic system, Public buildings},
abstract = {The current work aims to analyse the wayfinding strategies for blind people when interacting with a tactile map so as to plan an unfamiliar route. It results from reflections carried out as from research on the support to navigation by blind pedestrians whose study objective was the guided decision-making of 4 congenital blind, 4 adventitious blind and 4 low-vision blind in the process of route finding. The field study took place at the Education Faculty of the Federal University of Pernambuco – Recife – Brazil. The proposed experiment has been divided in 2 phases: learning and experimentation. Each one of them consisted of 3 steps with the following tasks for each user: plan a route, perform and represent it. A map was made upon a wooden structure covered by cardboard, upon which acetate was laid on the circulation areas. Different textures indicating the route to be followed by volunteers as well as the architectonic elements of the space to be experienced were placed upon them. Two reading strategies were used by the blind people to understand the space and plan a route: one based, exclusively, on the tracking of the route to be followed; and another devised as from the tracking of the general map. Those who limited their knowledge of the environment through the route, tended to memorize a lot of the decision-making orientation, thus causing hesitations and route deviation. Those who tracked the whole map, developed a panoramic idea of the space, generally related to some of the elements in the environment connected to the route structure, thus facilitating their spatial orientation. Due to the complexity of the wayfinding task for blind people, it is suggested that the tactile map is not studied in isolation but as from an ergonomic systemic view which considers it as an integral part of the information system which favours the decision-making orientation.}
}

@INPROCEEDINGS{101kulkarni2016indoornav,
author={Kulkarni, Aditi and Wang, Allan and Urbina, Lynn and Steinfeld, Aaron and Dias, Bernardine},
booktitle={11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
title={Robotic assistance in indoor navigation for people who are blind}, 
year={2016},
volume={},
number={},
pages={461-462},
address={Christchurch},
publisher={ACM/IEEE},
abstract={In this paper, we describe the process of making a robot useful as a guide robot for people who are blind or visually impaired. For this group, the interactive audio feature of a robot assumes a very high level of importance. We have introduced some features that will help to make the robot sound natural and be more comfortable. We first addressed the question of the speaker placement to help the user determine the size and distance of the robot. After the initial meeting, user data will be retained by the robot so that their communication evolves with every interaction. The robot will also ask the users if they need to take a rest after a specified interval depending upon the user's age and the distance they need to cover. The next time they visit, all this information will be used to make the interaction more natural and customized for each individual user.},
keywords={Robot sensing systems;Software;Buildings;Presses;Mobile robots;Speech;mobile robot;navigational assistance;blind;low vision;audio interaction},
doi={10.1109/HRI.2016.7451806},
ISSN={2167-2148},
month={March},}

@INPROCEEDINGS{102chen2016navcue,
author={Chen, Kangwei and Plaza-Leiva, Victoria and Min, Byung-Cheol and Steinfeld, Aaron and Dias, Mary Bernardine},
booktitle={11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
title={NavCue: Context immersive navigation assistance for blind travelers}, 
year={2016},
volume={},
number={},
address={Christchurch},
publisher={ACM/IEEE},
pages={559-559},
abstract={Research in assistive systems for travelers who are blind/low vision (B/LV) has been largely focused on basic map information. We present NavCue, an intelligent system module for providing rich, multi-sensory, context-based information using speech guidance and robot physical gestures. This approach is motivated by our previous user studies with people who are blind or low vision. This rich information should enhance user location awareness and confidence when traveling through unfamiliar locations.},
keywords={Navigation;Robot sensing systems;Context;Floors;Feature extraction;assistive robots;context immersive navigation;human-robot interaction;blind and low vision},
doi={10.1109/HRI.2016.7451855},
ISSN={2167-2148},
month={7--10 March},}

@article{103sheinker2016beaconphone,
title = {A method for indoor navigation based on magnetic beacons using smartphones and tablets},
journal = {Measurement},
volume = {81},
pages = {197-209},
year = {2016},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2015.12.023},
url = {https://www.sciencedirect.com/science/article/pii/S026322411500682X},
author = {Arie Sheinker and Boris Ginzburg and Nizan Salomonski and Lev Frumkis and Ben-Zion Kaplan and Mark B. Moldwin},
keywords = {Magnetic sensing, Localization, Indoor navigation, Smartphones and tablets},
abstract = {Outdoor navigation using GPS receivers installed in various types of consumer electronics devices, especially smartphones and tablet computers has become very common. However, indoor navigation can be problematic as GPS signals are blocked by ceilings and building walls and accuracy is on the order of building dimensions. In present work we propose using an array of magnetic beacons for localizing a receiver equipped with a magnetic sensor. A smartphone or a tablet computer with an internal magnetometer can be employed as a receiver. Exploiting smartphones and tablets for indoor navigation is a great advantage when considering convenience, simplicity and low cost. The navigation area is covered by magnetic beacons deployed in known locations. Each beacon generates an AC magnetic field with a unique signature enabling the receiver to distinguish between beacons. The signature may feature a specific single frequency tone, a combination of frequencies, or any other modulated signal. A software application running on the receiver enables self-localization by means of detection and identification of the nearest beacon. A system prototype has been developed and used to test the proposed method in field conditions. Experimental results show successful localization, which paves the way for a full scale development of an effective indoor navigation system. The good results together with simple implementation make the proposed method attractive for a wide range of indoor localization applications, including: pedestrian and robot navigation, inbuilding rescue missions, vision impaired assistance, and location aware services, just to mention a few.}
}

@Article{104bousbiasalah2011navaid,
author={Bousbia-Salah, Mounir
and Bettayeb, Maamar
and Larbi, Allal},
title={A Navigation Aid for Blind People},
journal={Journal of Intelligent {\&} Robotic Systems},
year={2011},
month={Dec},
day={01},
volume={64},
number={3},
pages={387-400},
abstract={This paper presents a navigation aid for the blind based on a microcontroller with synthetic speech output. The system consists of two vibrators, two ultrasonic sensors mounted on the user's shoulders and another one integrated into the cane. It is able to give information to the blind about urban walking routes and to provide real-time information on the distance of over-hanging obstacles within 6 m along the travel path ahead of the user. The suggested system can then sense the surrounding environment via sonar sensors and sending vibro-tactile feedback to the user of the position of the closest obstacles in range. For the ultrasonic cane, it is used to detect any obstacle on the ground. Experimental results show the effectiveness of the proposed system for blind navigation.},
issn={1573-0409},
doi={10.1007/s10846-011-9555-7},
url={https://doi.org/10.1007/s10846-011-9555-7}
}

@article{105gaunet2005specification,
author = {Florence Gaunet and Xavier Briffault},
title = {Exploring the Functional Specifications of a Localized Wayfinding Verbal Aid for Blind Pedestrians: Simple and Structured Urban Areas},
journal = {Human–Computer Interaction},
volume = {20},
number = {3},
pages = {267--314},
year = {2005},
publisher = {Taylor \& Francis},
doi = {10.1207/s15327051hci2003\_2},
URL = { https://www.tandfonline.com/doi/abs/10.1207/s15327051hci2003_2 },
eprint = { https://www.tandfonline.com/doi/pdf/10.1207/ },
abstract = { We propose functional specifications for a localized verbal wayfinding aid for blind pedestrians, in simple and structured urban areas. A user-centered design approach, that is, the analyses of route descriptions produced by blind pedestrians, allowed first to evidence verbal guidance rules and then to elaborate route descriptions of unfamiliar paths; their efficiency was confirmed.
We found that specific database features are streets, sidewalks, crosswalks, and intersections and that guidance functions consist of a combination of orientation and localization, goal location, intersection, crosswalks, and warning information as well as of progression, crossing, orientation, and route-ending instructions; they have to be provided between 5 to 10 meters before an intersection, after crossing, at middle block, and after entering a street. Last, verbal guidance is possible in simple and structured urban areas, with no localization aid, and is optimal within 5 meters' precision. The outcomes and limits of the requirements of the navigational aid evidenced are discussed. }
}

@inproceedings{106ren2023routenavapp,
author = {Ren, Peng and Lam, Jonathan and Manduchi, Roberto and Mirzaei, Fatemeh},
title = {Experiments with RouteNav, A Wayfinding App for Blind Travelers in a Transit Hub},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608428},
doi = {10.1145/3597638.3608428},
abstract = {RouteNav is an iOS app designed to support wayfinding for blind travelers in an indoor/outdoor transit hub. It doesn’t rely on external infrastructure (such as BLE beacons); instead, localization is obtained by fusing spatial information from inertial dead reckoning and GPS (when available) via particle filtering. Routes are expressed as sequences of “tiles”, where each tile may contain relevant points of interest. Redundant modalities are used to guide users to switching goalposts within tiles. In this paper, we describe the different components of RouteNav, and report on a user study with seven blind participants, who traversed three challenging routes in a transit hub while receiving input from the app.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {2},
numpages = {15},
keywords = {mobility, orientation, wayfinding},
location = {New York, New York, United States of America},
series = {ASSETS '23},
}

@article{107serrao2012landmarkgis,
title = {Indoor Localization and Navigation for Blind Persons using Visual Landmarks and a GIS},
journal = {Procedia Computer Science},
volume = {14},
pages = {65-73},
year = {2012},
note = {Proceedings of the 4th International Conference on Software Development for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2012)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912007703},
author = {M. Serrão and J.M.F. Rodrigues and J.I. Rodrigues and J.M.H. {du Buf}},
keywords = {Navigation, Accessibility, Vision, Blind, Geographic information system},
abstract = {In an unfamiliar environment we spot and explore all available information which might guide us to a desired location. This largely unconscious processing is done by our trained sensory and cognitive systems. These recognize and memorize sets of landmarks which allow us to create a mental map of the environment, and this map enables us to navigate by exploiting very few but the most important landmarks stored in our memory. We present a system which integrates a geographic information system of a building with visual landmarks for localizing the user in the building and for tracing and validating a route for the user's navigation. Hence, the developed system complements the white cane for improving the user's autonomy during indoor navigation. Although designed for visually impaired persons, the system can be used by any person for wayfinding in a complex building.}
}

@InProceedings{108tian2010context,
author="Tian, YingLi
and Yi, Chucai
and Arditi, Aries",
editor="Miesenberger, Klaus
and Klaus, Joachim
and Zagler, Wolfgang
and Karshmer, Arthur",
title="Improving Computer Vision-Based Indoor Wayfinding for Blind Persons with Context Information",
booktitle="Computers Helping People with Special Needs",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="255--262",
abstract="There are more than 161 million visually impaired people in the world today, of which 37 million are blind. Camera-based computer vision systems have the potential to assist blind persons to independently access unfamiliar buildings. Signs with text play a very important role in identification of bathrooms, exits, office doors, and elevators. In this paper, we present an effective and robust method of text extraction and recognition to improve computer vision-based indoor wayfinding. First, we extract regions containing text information from indoor signage with multiple colors and complex background and then identify text characters in the extracted regions by using the features of size, aspect ratio and nested edge boundaries. Based on the consistence of distances between two neighboring characters in a text string, the identified text characters have been normalized before they are recognized by using off-the-shelf optical character recognition (OCR) software products and output as speech for blind users.",
isbn={978-3-642-14100-3},
doi={10.1007/978-3-642-14100-3_38},
url={https://doi.org/10.1007/978-3-642-14100-3_38}
}

@INPROCEEDINGS{109murata2018phonebeacon,
author={Murata, Masayuki and Ahmetovic, Dragan and Sato, Daisuke and Takagi, Hironobu and Kitani, Kris M. and Asakawa, Chieko},
booktitle={IEEE International Conference on Pervasive Computing and Communications (PerCom)}, 
title={Smartphone-based Indoor Localization for Blind Navigation across Building Complexes}, 
year={2018},
volume={},
number={},
pages={1-10},
address={Athens},
location={Athens, Greece},
publisher={IEEE},
abstract={Continuous and accurate smartphone-based localization is a promising technology for supporting independent mobility of people with visual impairments. However, despite extensive research on indoor localization techniques, they are still not ready for deployment in large and complex environments, like shopping malls and hospitals, where navigation assistance is needed. To achieve accurate, continuous, and real-time localization with smartphones in such environments, we present a series of key techniques enhancing a probabilistic localization algorithm. The algorithm is designed for smartphones and employs inertial sensors on a mobile device and Received Signal Strength (RSS) from Bluetooth Low Energy (BLE) beacons. We evaluate the proposed system in a 21,000 m2 shopping mall which includes three multi-story buildings and a large open underground passageway. Experiments in this space validate the effect of the proposed technologies to improve localization accuracy. Field experiments with visually impaired participants confirm the practical performance of the proposed system in realistic use cases.},
keywords={Navigation;Smart phones;Buildings;Visualization;Wireless fidelity;Sensors;Computational modeling},
doi={10.1109/PERCOM.2018.8444593},
ISSN={2474-249X},
month={March},}

@article{110treuillet2010chestcamera,
author = {Treuillet, Sylvie and Royer, Eric},
title = {Outdoor/Indoor Vision-Based Localization for Blind Pedestrian Navigation Assistance},
journal = {International Journal of Image and Graphics},
volume = {10},
number = {04},
pages = {481-496},
year = {2010},
doi = {10.1142/S0219467810003937},
URL = { https://doi.org/10.1142/S0219467810003937 },
eprint = { https://doi.org/10.1142/S0219467810003937 },
abstract = { The most challenging issue facing the navigation assistive systems for the visually impaired is the instantaneous and accurate spatial localization of the user. Most of the previously proposed systems are based on global positioning system (GPS) sensors. However, the accuracy of low-cost versions is insufficient for pedestrian use. Furthermore, GPS-based systems are confined to outdoor navigation and experience severe signal losts in urban areas. This paper presents a new approach for localizing a person by using a single-body-mounted camera and computer vision techniques. Instantaneous accurate localization and heading estimates of the person are computed from images as the user progresses along a memorized path. A portable prototype has been tested for outdoor as well as indoor pedestrian use. Experimental results demonstrate the effectiveness of the vision-based localization: the accuracy is sufficient for making it possible to guide and maintain the blind person within a navigation corridor less than 1 m wide along the intended path. In combination with a suitable guiding interface, such a localization system will be convenient to assist the visually impaired in their everyday movements outdoors as well as indoors. }
}

@article{111sharma2012wheelchairassist,
author = {Sharma, Vinod and Simpson, Richard and LoPresti, Edmund and Schmeler, Mark},
title = {Driving backwards using a semi-autonomous smart wheelchair system DSS: A clinical evaluation},
year = {2012},
issue_date = {October 2012},
publisher = {IOS Press},
address = {NLD},
volume = {9},
number = {4},
issn = {1176-2322},
abstract = {Some wheelchair users have difficulty looking backward when backing up in confined spaces due to limited neck range of motion or low vision, which can lead to collisions which may result in personal injury or property damage. The Drive Safe System DSS was evaluated in a controlled laboratory setting with blindfolded able-bodied individuals on various backward driving tasks. Performance with the DSS was compared with a standard white cane used for navigation assistance by people with visual impairment. Results indicate that the DSS significantly reduced the number of collisions compared to using a cane p = 0.0001 alone. There was no difference in task completion time when participants were using the cane or the DSS p = 0.915. Users rated the DSS favourably as they experience less total workload p = 0.026, less physical demand p = 0.006, felt less frustrated p = 0.002 and put less effort p = 0.007 to achieve better performance when using the DSS, compared to using a cane. These findings suggest that the DSS can be a viable powered mobility solution for wheelchair users with visual impairments.},
journal = {Appl. Bionics Biomechanics},
month = {oct},
pages = {347–365},
numpages = {19},
keywords = {Smart Wheelchair, Semi-Autonomous System, Robotics, Intelligent Mobility Aids Imas, Embedded Distributed System, Collision Avoidance}
}

@article{112barati2015mobilesensor,
author    = {F. Barati and M. R. Delavar},
title     = {Design and Development of a Mobile Sensor Based the Blind Assistance Wayfinding System},
journal   = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
volume    = {XL-1/W5},
pages     = {91--96},
year      = {2015},
doi       = {10.5194/isprsarchives-XL-1-W5-91-2015},
url       = {https://doi.org/10.5194/isprsarchives-XL-1-W5-91-2015}
}

@article{113spagnol2018soundicon,
title = {Blind wayfinding with physically-based liquid sounds},
journal = {International Journal of Human-Computer Studies},
volume = {115},
pages = {9-19},
year = {2018},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918300600},
author = {Simone Spagnol and Rebekka Hoffmann and Marcelo {Herrera Martínez} and Runar Unnthorsson},
keywords = {Sensory substitution, Sonification, Electronic travel aid, Physical sound model},
abstract = {Translating visual representations of real environments into auditory feedback is one of the key challenges in the design of an electronic travel aid for visually impaired persons. Although the solutions currently available in the literature can lead to effective sensory substitution, high commitment to an extensive training program involving repetitive sonic patterns is typically required, undermining their use in everyday life. The current study explores a novel sensory substitution algorithm that extracts information from raw depth maps and continuously converts it into parameters of a naturally sounding, physically based liquid sound model describing a population of bubbles. This approach is tested in a simplified wayfinding experiment with 14 blindfolded sighted participants and compared against the most popular sensory substitution algorithm available in the literature – the vOICe (Meijer, 1992) – following a short-time training program. The results indicate a superior performance of the proposed sensory substitution algorithm in terms of navigation accuracy, intuitiveness and pleasantness of the delivered sounds compared to the vOICe algorithm. These results should be applied to the visually impaired population with caution.}
}

@inproceedings{114ahmetovic2016navcog,
author = {Ahmetovic, Dragan and Gleason, Cole and Ruan, Chengxiong and Kitani, Kris and Takagi, Hironobu and Asakawa, Chieko},
title = {NavCog: a navigational cognitive assistant for the blind},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935361},
doi = {10.1145/2935334.2935361},
abstract = {Turn-by-turn navigation is a useful paradigm for assisting people with visual impairments during mobility as it reduces the cognitive load of having to simultaneously sense, localize and plan. To realize such a system, it is necessary to be able to automatically localize the user with sufficient accuracy, provide timely and efficient instructions and have the ability to easily deploy the system to new spaces.We propose a smartphone-based system that provides turn-by-turn navigation assistance based on accurate real-time localization over large spaces. In addition to basic navigation capabilities, our system also informs the user about nearby points-of-interest (POI) and accessibility issues (e.g., stairs ahead). After deploying the system on a university campus across several indoor and outdoor areas, we evaluated it with six blind subjects and showed that our system is capable of guiding visually impaired users in complex and unfamiliar environments.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {90–99},
numpages = {10},
keywords = {visual impairments, turn-by-turn navigation, real world accessibility, navigation assistance, localization, bluetooth low-energy beacons, assistive technologies},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@Article{115crabb2023visionalgo,
AUTHOR = {Crabb, Ryan and Cheraghi, Seyed Ali and Coughlan, James M.},
TITLE = {A Lightweight Approach to Localization for Blind and Visually Impaired Travelers},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {5},
ARTICLE-NUMBER = {2701},
NUMPAGES = {30},
URL = {https://www.mdpi.com/1424-8220/23/5/2701},
PubMedID = {36904904},
ISSN = {1424-8220},
ABSTRACT = {Independent wayfinding is a major challenge for blind and visually impaired (BVI) travelers. Although GPS-based localization approaches enable the use of navigation smartphone apps that provide accessible turn-by-turn directions in outdoor settings, such approaches are ineffective in indoor and other GPS-deprived settings. We build on our previous work on a localization algorithm based on computer vision and inertial sensing; the algorithm is lightweight in that it requires only a 2D floor plan of the environment, annotated with the locations of visual landmarks and points of interest, instead of a detailed 3D model (used in many computer vision localization algorithms), and requires no new physical infrastructure (such as Bluetooth beacons). The algorithm can serve as the foundation for a wayfinding app that runs on a smartphone; crucially, the approach is fully accessible because it does not require the user to aim the camera at specific visual targets, which would be problematic for BVI users who may not be able to see these targets. In this work, we improve upon the existing algorithm so as to incorporate recognition of multiple classes of visual landmarks to facilitate effective localization, and demonstrate empirically how localization performance improves as the number of these classes increases, showing the time to correct localization can be decreased by 51–59%. The source code for our algorithm and associated data used for our analyses have been made available in a free repository.},
DOI = {10.3390/s23052701},}

@Article{116real2019app-wearable4nav/survey,
AUTHOR = {Real, Santiago and Araujo, Alvaro},
TITLE = {Navigation Systems for the Blind and Visually Impaired: Past Work, Challenges, and Open Problems},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3404},
NUMPAGES = {20},
URL = {https://www.mdpi.com/1424-8220/19/15/3404},
PubMedID = {31382536},
ISSN = {1424-8220},
ABSTRACT = {Over the last decades, the development of navigation devices capable of guiding the blind through indoor and/or outdoor scenarios has remained a challenge. In this context, this paper’s objective is to provide an updated, holistic view of this research, in order to enable developers to exploit the different aspects of its multidisciplinary nature. To that end, previous solutions will be briefly described and analyzed from a historical perspective, from the first “Electronic Travel Aids” and early research on sensory substitution or indoor/outdoor positioning, to recent systems based on artificial vision. Thereafter, user-centered design fundamentals are addressed, including the main points of criticism of previous approaches. Finally, several technological achievements are highlighted as they could underpin future feasible designs. In line with this, smartphones and wearables with built-in cameras will then be indicated as potentially feasible options with which to support state-of-art computer vision solutions, thus allowing for both the positioning and monitoring of the user’s surrounding area. These functionalities could then be further boosted by means of remote resources, leading to cloud computing schemas or even remote sensing via urban infrastructure.},
DOI = {10.3390/s19153404},}

@inproceedings{117heuten2008tactilewayfinderbelt,
author = {Heuten, Wilko and Henze, Niels and Boll, Susanne and Pielot, Martin},
title = {Tactile wayfinder: a non-visual support system for wayfinding},
year = {2008},
isbn = {9781595937049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1463160.1463179},
doi = {10.1145/1463160.1463179},
abstract = {Digital maps and route descriptions on a PDA have become very popular for navigation, not the least with the advent of the iPhone and its Google Maps application. A visual support for wayfinding, however, is not reasonable or even possible all the time. A pedestrian must pay attention to traffic on the street, a hiker should concentrate on the narrow trail, and a blind person relies on other modalities to find her way. To overcome these limitations, we developed a non-visual support for wayfinding that guides and keeps a mobile user en route by a tactile display. We designed a belt with vibrators that indicates directions and deviations from the path in an accurate and unobtrusive way. Our first user evaluation showed that on an open field without any landmarks the participants stayed well to given test routes and that wayfinding support is possible with our Tactile Wayfinder.},
booktitle = {Proceedings of the 5th Nordic Conference on Human-Computer Interaction: Building Bridges},
pages = {172–181},
numpages = {10},
keywords = {wayfinding, tactile display, pedestrian navigation},
location = {Lund, Sweden},
series = {NordiCHI '08}
}

@article{118nair2022assist,
author = {Vishnu Nair, Greg Olmschenk, William H. Seiple and Zhigang Zhu},
title = {ASSIST: Evaluating the usability and performance of an indoor navigation assistant for blind and visually impaired people},
journal = {Assistive Technology},
volume = {34},
number = {3},
pages = {289--299},
year = {2022},
publisher = {Taylor \& Francis},
doi = {10.1080/10400435.2020.1809553},
note ={PMID: 32790580},
URL = { https://doi.org/10.1080/10400435.2020.1809553 },
eprint = { https://doi.org/10.1080/10400435.2020.1809553 },
abstract = { This paper describes the interface and testing of an indoor navigation app – ASSIST – that guides blind \&amp; visually impaired (BVI) individuals through an indoor environment with high accuracy while augmenting their understanding of the surrounding environment. ASSIST features personalized interfaces by considering the unique experiences that BVI individuals have in indoor wayfinding and offers multiple levels of multimodal feedback. After an overview of the technical approach and implementation of the first prototype of the ASSIST system, the results of two pilot studies performed with BVI individuals are presented – a performance study to collect data on mobility (walking speed, collisions, and navigation errors) while using the app, and a usability study to collect user evaluation data on the perceived helpfulness, safety, ease-of-use, and overall experience while using the app. Our studies show that ASSIST is useful in providing users with navigational guidance, improving their efficiency and (more significantly) their safety and accuracy in wayfinding indoors. Findings and user feedback from the studies confirm some of the previous results, while also providing some new insights into the creation of such an app, including the use of customized user interfaces and expanding the types of information provided. },
}

@techreport{119mau2008blindaid,
author = {Sandra Mau and Nicholas Melchior and Maxim Makatchev and Aaron Steinfeld},
title = {BlindAid: An Electronic Travel Aid for the Blind},
year = {2008},
month = {May},
institute = {Carnegie Mellon University},
institution = {Carnegie Mellon University},
address = {Pittsburgh, PA},
number = {CMU-RI-TR-07-39},
keywords = {blind navigation, assistive technology, electronic travel aid, quality of life},}

@INPROCEEDINGS{120chumkamon2008rfidindoornav, 
author={Chumkamon, Sakmongkon and Tuvaphanthaphiphat, Peranitti and Keeratiwintakorn, Phongsak},
booktitle={5th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology}, 
title={A Blind Navigation System Using RFID for Indoor Environments}, 
year={2008},
volume={2},
number={},
pages={765-768},
address={Krabi, Thailand},
publisher={IEEE},
abstract={A location and tracking system becomes very important to our future world of pervasive computing, where information is all around us. Location is one of the most needed information for emerging and future applications. Since the public use of GPS satellite is allowed, several state-of-the-art devices become part of our life, e.g. a car navigator and a mobile phone with a built-in GPS receiver. However, location information for indoor environments is still very limited. Several techniques are proposed to get location information in buildings such as using a radio signal triangulation, a radio signal (beacon) emitter, or signal fingerprinting. Using radio frequency identification (RFID) tags is a new way of giving location information to users. Due to its passive communication circuit, RFID tags can be embedded almost anywhere without an energy source. The tags stores location information and gives it to any reader that is within a proximity range which can be up to 10-15 meters for UHF RFID systems. We propose an RFID-based system for navigation in a building for blind people or visually impaired. The system relies on the location information on the tag, a userpsilas destination, and a routing server where the shortest route from the userpsilas current location to the destination. The navigation device communicates with the routing server using GPRS networks. We build a prototype based on our design and show some results. We found that there are some delay problems in the devices which are the communication delay due to the cold start cycle of a GPRS modem and the voice delay due to the file transfer delay from a MMC module.},
keywords={Navigation;Radiofrequency identification;Servers;Radio navigation;Prototypes;Delay;Fires},
doi={10.1109/ECTICON.2008.4600543},
ISSN={},
month={May},}

@article{121loomis1998displays,
author = {Loomis, Jack M. and Golledge, Reginald G. and Klatzky, Roberta L.},
title = "{Navigation System for the Blind: Auditory Display Modes and Guidance}",
journal = {Presence: Teleoperators and Virtual Environments},
volume = {7},
number = {2},
pages = {193-203},
year = {1998},
month = {04},
abstract = "{The research we are reporting here is part of our effort to develop a navigation system for the blind. Our long-term goal is to create a portable, self-contained system that will allow visually impaired individuals to travel through familiar and unfamiliar environments without the assistance of guides. The system, as it exists now, consists of the following functional components: (1) a module for determining the traveler's position and orientation in space, (2) a Geographic Information System comprising a detailed database of our test site and software for route planning and for obtaining information from the database, and (3) the user interface. The experiment reported here is concerned with one function of the navigation system: guiding the traveler along a predefined route. We evaluate guidance performance as a function of four different display modes: one involving spatialized sound from a virtual acoustic display, and three involving verbal commands issued by a synthetic speech display. The virtual display mode fared best in terms of both guidance performance and user preferences.}",
doi = {10.1162/105474698565677},
url = {https://doi.org/10.1162/105474698565677},
eprint = {https://direct.mit.edu/pvar/article pdf/7/2/193/1622966/105474698565677.pdf},
}

@article{122cecilio2015blindedroid,
title = {BlindeDroid: An Information Tracking System for Real-time Guiding of Blind People},
journal = {Procedia Computer Science},
volume = {52},
pages = {113-120},
year = {2015},
note = {The 6th International Conference on Ambient Systems, Networks and Technologies (ANT-2015), the 5th International Conference on Sustainable Energy Information Technology (SEIT-2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S187705091500839X},
author = {José Cecílio and Karen Duarte and Pedro Furtado},
keywords = {Blind People, Human Navigation, Assistive Technology, Localization, Indoor Navigation.},
abstract = {Among the activities affected by visual impairment, navigation plays a fundamental role, since it enables the person to independently move in safety. The heterogeneous environment, easily perceived by visually enabled people, is hardly known by partially sighted people. A challenging task for these people is independent navigation in new spaces/buildings/environments. The environment is usually signaled and labeled with visual marks and signs which are not appropriate for blind persons. With the purpose of balancing the access to services and spaces among all persons, this work proposes an innovative navigation and information system to help the navigation of blind people within new environments (e.g. shopping center, public office building). Based on smartphones and wireless sensors deployed in the environment, we propose an information tracking system for realtime guide blind people (BlindeDroid). It offers guided navigation, answering questions, and providing objective information about places, products and services that are available surrounding the user.}
}

@inproceedings{123sato2017navcog3,
author = {Sato, Daisuke and Oh, Uran and Naito, Kakuya and Takagi, Hironobu and Kitani, Kris and Asakawa, Chieko},
title = {NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment},
year = {2017},
isbn = {9781450349260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132525.3132535},
doi = {10.1145/3132525.3132535},
abstract = {Navigating in unfamiliar environments is challenging for most people, especially for individuals with visual impairments. While many personal navigation tools have been proposed to enable in- dependent indoor navigation, they have insufficient accuracy (e.g., 5-10 m), do not provide semantic features about surroundings (e.g., doorways, shops, etc.), and may require specialized devices to function. Moreover, the deployment of many systems is often only evaluated in constrained scenarios, which may not precisely reflect the performance in the real world. Therefore, we have de- signed and implemented NavCog3, a smartphone-based indoor navigation assistant that has been evaluated in a 21,000 m2 shop- ping mall. In addition to turn-by-turn instructions, it provides in- formation on landmarks (e.g., tactile paving) and points of interests nearby. We first conducted a controlled study with 10 visually im- paired users to assess localization accuracy and the perceived use- fulness of semantic features. To understand the usability of the app in a real-world setting, we then conducted another study with 43 participants with visual impairments where they could freely nav- igate in the shopping mall using NavCog3. Our findings suggest that NavCog3 can open a new opportunity for users with visual im- pairments to independently find and visit large and complex places with confidence.},
booktitle = {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {270–279},
numpages = {10},
keywords = {indoor navigation, points of interest, user evaluation, visual impairments, voice- based interaction},
location = {Baltimore, Maryland, USA},
series = {ASSETS '17}
}

@INPROCEEDINGS{124wachaja2015walker,
author={Wachaja, Andreas and Agarwal, Pratik and Zink, Mathias and Adame, Miguel Reyes and Möller, Knut and Burgard, Wolfram},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Navigating blind people with a smart walker}, 
year={2015},
volume={},
number={},
publisher={IEEE},
address={Hamburg, Germany},
pages={6014-6019},
abstract={Navigation in complex and unknown environments is a major challenge for blind people. The most popular, conventional navigation aids such as white canes and guide dogs, however, provide limited assistance in such settings as they are constrained to interpret the local environment only. At the same time, they can hardly be combined with a walker required by elderly people with walking disabilities. Technologies developed in the field of robotics have the potential to assist blind people in complex navigation tasks as they can provide information about obstacles and reason on both global and local environment models. The contribution of this paper is a smart walker that enables blind users to safely navigate. It includes an innovative vibro-tactile user interface and a controller that takes into account human characteristics based on a user study. The walker has been designed to deal with the fact that humans can only sense and interpret a limited number of commands and have a delayed response. Our experiments validate our claim that the technique outlined in this paper guides a user to the desired goal in less time and with shorter traveled distance compared to a standard robotic controller.},
keywords={Navigation;Vibrations;Legged locomotion;Senior citizens;Robot sensing systems},
doi={10.1109/IROS.2015.7354233},
ISSN={},
month={Sep.},}

@CONFERENCE{125tachi1985guidedog,
author = {Tachi, Susumu and Komoriya, Kiyoshi},
title = {GUIDE DOG ROBOT.},
booktitle= {The Robotics Research 2 (The Second International Symposium 1984)},
year = {1985},
address = {Kyoto, Japan},
pages = {333 – 340},
url = {https://tachilab.org/content/files/publication/tp/tachi1985MIT.pdf},
affiliations = {MITI, Ibaraki, Jpn, MITI, Ibaraki, Jpn},
abstract = {The Guide Dog Robot Project started in the 1977 fiscal year at MEL. The project's goal is to enhance mobility aids for the blind by providing them with the functions of guide dogs, i. e. , obedience in navigating or guiding a blind master, intelligent disobedience in detecting and avoiding obstacles in his/her path, and well-organized man-machine communication which does not interfere with his/her remaining senses. In this paper the design concept of the Guide Dog Robot MELDOG is described first. Next, the navigation method using an organized map and landmarks, obstacle detection/avoidance system based on the ultrasonic environment measurement and man-machine communication via electrocutaneous stimulation system are presented. The results of the feasibility studies using MELDOG MARK I, II, III and IV test hardwares are discussed. Future problems are also elucidated.},
keywords = {BIOMEDICAL EQUIPMENT; GUIDE DOG ROBOT; LANDMARK TRACKING METHOD; MELDOG; NAVIGATION MAP; OBSTACLE DETECTION; ROBOTICS},
publisher = {MIT Press},
isbn = {0262081512},
language = {English},
type = {Conference paper},
publication_stage = {Final},
source = {Scopus},
note = {Cited by: 28}
}

@ARTICLE{204tachi1985meldog,
author={Tachi, Susumu and Tanie, Kazuo and Komoriya, Kiyoshi and Abe, Minoru},
journal={IEEE Transactions on Biomedical Engineering}, 
title={Electrocutaneous Communication in a Guide Dog Robot (MELDOG)}, 
year={1985},
volume={BME-32},
number={7},
pages={461-469},
abstract={Two main problems to be solved in designing truly effective mobility aids for the blind are: 1) to determine what kinds and how many pieces of information are necessary and/or sufficient to mobilize humans, and 2) to establish the optimal coding and display method of the acquired information.},
keywords={Robot sensing systems;Mobile robots;Humans;Navigation;Cities and towns;Dogs;Intelligent robots;Communication systems;Mechanical engineering;Displays},
doi={10.1109/TBME.1985.325561},
ISSN={1558-2531},
month={July},}

@INPROCEEDINGS{126lakde2015navwearable,
author={Lakde, Chaitali Kishor and Prasad, Prakash S.},
booktitle={International Conference on Computation of Power, Energy, Information and Communication (ICCPEIC)}, 
title={Navigation system for visually impaired people}, 
year={2015},
volume={},
number={},
pages={0093-0098},
publisher={IEEE},
address={Melmaruvathur, India},
abstract={Navigation assistance for visually impaired (NAVI) refers to systems that are able to assist or guide people with vision loss, ranging from partially sighted to totally blind, by means of sound commands. Many researchers are working to assist visually impaired people in different ways like voice based assistance, ultrasonic based assistance, camera based assistance and in some advance way researchers are trying to give transplantation of real eyes with robotic eyes which can capable enough to plot the real image over patient retina using some biomedical technologies. In other way creating a fusion of sensing technology and voice based guidance system some of the products were developed which could give better result than individual technology. There are some limitation in system like obstacle detection which could not see the object but detection the object and camera based system can't work properly in different light level so the proposed system is a fusion of color sensing sensor and the obstacle sensor along with the voice based assistance system. The main idea of the proposed system to make person aware of path he is walking and also the obstacle in the path.},
keywords={Navigation;Robot sensing systems;Headphones;Educational robots;Computers;Real-time systems;Navigation system;visually impaired;obstacle detection;mobility;pattern matching;IR sensor;RGB sensor},
doi={10.1109/ICCPEIC.2015.7259447},
ISSN={},
month={April},}

@Article{127gharpure2008shopping,
author={Gharpure, Chaitanya P.
and Kulyukin, Vladimir A.},
title={Robot-assisted shopping for the blind: issues in spatial cognition and product selection},
journal={Intelligent Service Robotics},
year={2008},
month={Jul},
day={01},
volume={1},
number={3},
pages={237-251},
abstract={Research on spatial cognition and blind navigation suggests that a device aimed at helping blind people to shop independently should provide the shopper with effective interfaces to the locomotor and haptic spaces of the supermarket. In this article, we argue that robots can act as effective interfaces to haptic and locomotor spaces in modern supermarkets. We also present the design and evaluation of three product selection modalities---browsing, typing and speech, which allow the blind shopper to select the desired product from a repository of thousands of products.},
issn={1861-2784},
doi={10.1007/s11370-008-0020-9},
url={https://doi.org/10.1007/s11370-008-0020-9}
}

@ARTICLE{128rentschler2008guidowalker,
title    = "Clinical evaluation of Guido robotic walker",
author   = "Rentschler, Andrew J and Simpson, Richard and Cooper, Rory A and
Boninger, Michael L",
abstract = "The Guido is a robotic walker that provides navigation and obstacle avoidance assistance. Engineering tests have found that the device performs adequately and presents no hazard to the user. The performance of the Guido was compared with a low-tech mobility aid, the Assistive Mobility Device (AMD) developed at the Atlanta Department of Veterans Affairs Medical Center, in trials involving older adults with visual impairments. The purpose of this study was to determine whether the Guido could increase the safety and mobility of elderly visually impaired individuals in supervised care facilities. Subjects traversed an obstacle course with the Guido and the AMD. Completion time, obstacle/wall contacts, and reorientations were compared for both devices. No significant differences were found between the devices for any of the tests. The Guido did not perform better than the AMD during the trials. Revisions to the device as well as a change in subject requirements and testing protocol may produce different results.",
journal  = "J. Rehabil. Res. Dev.",
volume   =  45,
number   =  9,
pages    = "1281--1293",
year     =  2008,
language = "en",
doi      = {10.1682/JRRD.2007.10.0160},
url      = {https://pubmed.ncbi.nlm.nih.gov/19319753/},
}

@INPROCEEDINGS{129wang2017wearablenav,
author={Wang, Hsueh-Cheng and Katzschmann, Robert K. and Teng, Santani and Araki, Brandon and Giarré, Laura and Rus, Daniela},
booktitle={IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Enabling independent navigation for visually impaired people through a wearable vision-based feedback system}, 
year={2017},
volume={},
number={},
pages={6533-6540},
publisher={IEEE},
address={Singapore},
abstract={This work introduces a wearable system to provide situational awareness for blind and visually impaired people. The system includes a camera, an embedded computer and a haptic device to provide feedback when an obstacle is detected. The system uses techniques from computer vision and motion planning to (1) identify walkable space; (2) plan step-by-step a safe motion trajectory in the space, and (3) recognize and locate certain types of objects, for example the location of an empty chair. These descriptions are communicated to the person wearing the device through vibrations. We present results from user studies with low- and high-level tasks, including walking through a maze without collisions, locating a chair, and walking through a crowded environment while avoiding people.},
keywords={Navigation;Three-dimensional displays;Vibrations;Cameras;Legged locomotion;Robot sensing systems;Haptic interfaces},
doi={10.1109/ICRA.2017.7989772},
ISSN={},
month={May},}

@ARTICLE{130katzschmann2018tofhapticbelt,
author={Katzschmann, Robert K. and Araki, Brandon and Rus, Daniela},
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={Safe Local Navigation for Visually Impaired Users With a Time-of-Flight and Haptic Feedback Device}, 
year={2018},
volume={26},
number={3},
pages={583-593},
abstract={This paper presents ALVU (Array of Lidars and Vibrotactile Units), a contactless, intuitive, hands-free, and discreet wearable device that allows visually impaired users to detect low- and high-hanging obstacles, as well as physical boundaries in their immediate environment. The solution allows for safe local navigation in both confined and open spaces by enabling the user to distinguish free space from obstacles. The device presented is composed of two parts: a sensor belt and a haptic strap. The sensor belt is an array of time-of-flight distance sensors worn around the front of a user's waist, and the pulses of infrared light provide reliable and accurate measurements of the distances between the user and surrounding obstacles or surfaces. The haptic strap communicates the measured distances through an array of vibratory motors worn around the user's upper abdomen, providing haptic feedback. The linear vibration motors are combined with a point-loaded pretensioned applicator to transmit isolated vibrations to the user. We validated the device's capability in an extensive user study entailing 162 trials with 12 blind users. Users wearing the device successfully walked through hallways, avoided obstacles, and detected staircases.},
keywords={Haptic interfaces;Navigation;Robot sensing systems;Vibrations;Belts;Sensor arrays;Cameras;Assistive device;sightless navigation;human-robot interaction;perception;haptic feedback array},
doi={10.1109/TNSRE.2018.2800665},
ISSN={1558-0210},
month={March},}

@inproceedings{131ullman2018trust,
author = {Ullman, Daniel and Malle, Bertram F.},
title = {What Does it Mean to Trust a Robot? Steps Toward a Multidimensional Measure of Trust},
year = {2018},
isbn = {9781450356152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173386.3176991},
doi = {10.1145/3173386.3176991},
abstract = {Research on trust in human-human interaction has typically focused on notions of vulnerability, integrity, and exploitation whereas research on trust in human-machine interaction has typically focused on competence and reliability. In this initial study, we explore whether these different aspects of trust can be considered parts of a multidimensional conception and measure of trust. We gathered 62 words from dictionaries and trust literatures and asked participants to evaluate the words as belonging to a "personal" meaning or a "capacity" meaning. Through an iterative process using Principal Components Analysis (PCA) and item analysis, we derived four components that capture the multidimensional space occupied by the concept of trust. The resulting four components yield four subscales of trust with five items each and alpha reliabilities as follows: Capable = .88, Ethical = .87, Sincere = .84, and Reliable = .72.},
booktitle = {Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {263–264},
numpages = {2},
keywords = {human-robot interaction, human-robot trust, social robotics, trust},
location = {Chicago, IL, USA},
series = {HRI '18}
}

@INPROCEEDINGS{132moon2019hapticfollowing,
author={Moon, Hee-Seung and Seo, Jiwon},
booktitle={IEEE World Haptics Conference (WHC)}, 
title={Prediction of Human Trajectory Following a Haptic Robotic Guide Using Recurrent Neural Networks}, 
year={2019},
volume={},
number={},
pages={157-162},
abstract={Social intelligence is an important requirement for enabling robots to collaborate with people. In particular, human path prediction is an essential capability for robots in that it prevents potential collision with a human and allows the robot to safely make larger movements. In this paper, we present a method for predicting the trajectory of a human who follows a haptic robotic guide without using sight, which is valuable for assistive robots that aid the visually impaired. We apply a deep learning method based on recurrent neural networks using multimodal data: (1) human trajectory, (2) movement of the robotic guide, (3) haptic input data measured from the physical interaction between the human and the robot, (4) human depth data. We collected actual human trajectory and multimodal response data through indoor experiments. Our model outperformed the baseline result while using only the robot data with the observed human trajectory, and it shows even better results when using additional haptic and depth data.},
keywords={Haptic interfaces;Trajectory;Data models;Robot sensing systems;End effectors;Recurrent neural networks},
doi={10.1109/WHC.2019.8816157},
publisher={IEEE},
address={Tokyo},
ISSN={},
month={July},}

@INPROCEEDINGS{133hwang2023robotvdog,
author={Hwang, Hochul and Xia, Tim and Keita, Ibrahima and Suzuki, Ken and Biswas, Joydeep and Lee, Sunghoon I. and Kim, Donghyun},
booktitle={IEEE International Conference on Robotics and Automation (ICRA)}, 
title={System Configuration and Navigation of a Guide Dog Robot: Toward Animal Guide Dog-Level Guiding Work}, 
year={2023},
volume={},
number={},
pages={9778-9784},
abstract={A robot guide dog has compelling advantages over animal guide dogs for its cost-effectiveness, the potential for mass production, and low maintenance burden. However, despite the long history of guide dog robot research, previous studies were conducted with little or no consideration of how the guide dog handler and the guide dog work as a team for navigation. To develop a robotic guiding system that genuinely benefits blind or visually impaired individuals, we performed qualitative research, including interviews with guide dog handlers, trainers, and first-hand blindfold walking experiences with various guide dogs. We build a collaborative indoor navigation scheme for a guide dog robot that includes preferred features such as speed and directional control. For collaborative navigation, we propose a semantic-aware local path planner that enables safe and efficient guiding work by utilizing semantic information about the environment and considering the handler's position and directional cues to determine the collision-free path. We evaluate our integrated robotic system by testing blindfolded walking in indoor settings and demonstrate guide dog-like navigation behavior by avoiding obstacles at typical gait speed (0.7m/s). The following demonstration video link includes an audio description: https://youtu.be/YxlcMeaL7GA},
keywords={Legged locomotion;Mass production;Semantics;Collaboration;Dogs;Maintenance engineering;Collision avoidance},
doi={10.1109/ICRA48891.2023.10160573},
publisher={IEEE},
address={London},
location={London, UK},
ISSN={},
month={May},}

@inproceedings{134soto2017dronenavigator,
author = {Avila Soto, Mauro and Funk, Markus and Hoppe, Matthias and Boldt, Robin and Wolf, Katrin and Henze, Niels},
title = {DroneNavigator: Using Leashed and Free-Floating Quadcopters to Navigate Visually Impaired Travelers},
year = {2017},
isbn = {9781450349260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132525.3132556},
doi = {10.1145/3132525.3132556},
abstract = {Although a large number of navigation support systems for visually impaired people have been proposed in the past, navigating through unknown environments is still a major challenge for visually impaired travelers. Existing systems provide navigation information through headphones, speakers or tactile actuators. In this paper, we propose to use small lightweight quadcopters instead to provide navigation information for people with visual impairments. Using a leashed or free-floating quadcopter, the user is navigated by the distinct sound that the quadcopter emits and a haptic stimulus provided by the leash. In a user with 14 visually impaired participants, we compared leashed quadcopter navigation, free-floating quadcopter navigation, and traditional audio navigation. The results show that compared to audio navigation, participants navigate significantly faster with a free-floating quadcopter and make fewer navigation errors using the quadcopter navigation methods.},
booktitle = {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {300–304},
numpages = {5},
keywords = {visual impairments, quadcopter, navigation aid, drones},
location = {Baltimore, Maryland, USA},
series = {ASSETS '17}
}

@INPROCEEDINGS{135morris2003guidewalker,
author={Morris, A. and Donamukkala, R. and Kapuria, A. and Steinfeld, A. and Matthews, J.T. and Dunbar-Jacob, J. and Thrun, S.},
booktitle={IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)}, 
title={A robotic walker that provides guidance}, 
year={2003},
publisher={IEEE},
address={Taipei, Taiwan},
location={Taipei, Taiwan},
volume={1},
number={},
pages={25-30 vol.1},
keywords={Legged locomotion;Cognitive robotics;Senior citizens;Navigation;Safety;Stability;Robot localization;Haptic interfaces;System testing;Retirement},
doi={10.1109/ROBOT.2003.1241568},}

@article{136langelaan2007pwviqol/motiv,
author = {Maaike Langelaan and Michiel R. de Boer and Ruth M. A. van Nispen and Bill Wouters and Annette C. Moll and Ger H. M. B. van Rens},
title = {Impact of Visual Impairment on Quality of Life: A Comparison With Quality of Life in the General Population and With Other Chronic Conditions},
journal = {Ophthalmic Epidemiology},
volume = {14},
number = {3},
pages = {119--126},
year = {2007},
publisher = {Taylor \& Francis},
doi = {10.1080/09286580601139212},
note ={PMID: 17613846},
URL = { https://doi.org/10.1080/09286580601139212 },
eprint = { https://doi.org/10.1080/09286580601139212 },
abstract = { Purpose: Subjective evaluation of health-related quality of life (HRQoL) and health status is recognized as an important tool in the assessment and treatment of visually impaired patients. The aims of this study are to describe the generic HRQoL and health status of visually impaired patients and to compare the HRQoL of visually impaired patients with that of both the general population of the Netherlands and patients with other chronic conditions. Methods: 128 persons attending a rehabilitation centre for visually impaired adults completed the EuroQol questionnaire (EQ-5D). These patients' EQ-5D scores were compared with EQ-5D norms of the Dutch population and of patients with other chronic conditions; both sets of data were taken from the literature. Results: The average EQ-5Dindex score of the total study population was 0.73 (SD 0.22). Visually impaired patients reported more problems on every dimension of the EQ-5D than the general Dutch population. Only stroke patients and patients with chronic fatigue syndrome and reported more problems on every dimension of the EQ-5D than visually impaired patients. Conclusions: Visual impairment has a substantial impact on the quality of life; compared with other chronic conditions, it seems to affect the HRQoL, spoiling the quality of life more than diabetes type II, coronary syndrome, and hearing impairments, but less than stroke, multiple sclerosis, chronic fatigue syndrome, major depressive disorder, and severe mental illness. }
}

@article{137hersh2010pwviwants-part1/motiv,
author = {Marion A. Hersh and Michael A. Johnson},
title = {A robotic guide for blind people. Part 1. A multi-national survey of the attitudes, requirements and preferences of potential end-users},
journal = {Applied Bionics and Biomechanics},
volume = {7},
number = {4},
pages = {277--288},
year = {2010},
publisher = {Taylor \& Francis},
doi = {10.1080/11762322.2010.523626},
URL = { https://doi.org/10.1080/11762322.2010.523626 },
eprint = { https://doi.org/10.1080/11762322.2010.523626},
abstract = { This paper reports the results of a multi-national survey in several different countries on the attitudes, requirements and preferences of blind and visually impaired people for a robotic guide. The survey is introduced by a brief overview of existing work on robotic travel aids and other mobile robotic devices. The questionnaire comprises three sections on personal information about respondents, existing use of mobility and navigation devices and the functions and other features of a robotic guide. The survey found that respondents were very interested in the robotic guide having a number of different functions and being useful in a wide range of circumstances. They considered the robot's appearance to be very important but did not like any of the proposed designs. From their comments, respondents wanted the robot to be discreet and inconspicuous, small, light weight and portable, easy to use, robust to damage, require minimal maintenance, have a long life and a long battery life. },}

@article{138hersh2012design-part2/motiv,
author = {Hersh, Marion A. and Johnson, Michael A.},
title = {A robotic guide for blind people Part 2: Gender and national analysis of a multi-national survey and the application of the survey results and the CAT model to framing robot design specifications},
year = {2012},
issue_date = {January 2012},
publisher = {IOS Press},
address = {NLD},
volume = {9},
number = {1},
issn = {1176-2322},
abstract = {This paper presents a gender and country-based analysis of the results of a multi-national survey questionnaire on the attitudes, requirements and preferences of blind and visually impaired people for a robotic guide. This is introduced by a brief summary of the findings of the survey introduced and reported in Part 1 and a brief overview of some of the technologies that are currently used in the construction of mobile robotic guides. An analysis of the gender dimension revealed very few differences in preferences or requirements between male and female respondents. There was also considerable commonality of preferences and requirements across the four countries, France, Italy, Spain and the UK, for which the comparative analysis was carried out. This implies that, at least initially, one robotic guide can be developed to be used in a number of different countries and by both women and men. The survey results were then applied to develop design specifications for a mobile robotic guide for blind and visually impaired people. The framework of the Comprehensive Assistive Technology CAT model developed by the authors was used to organise the information and structure the development of the design specifications. Further work will involve the construction of design mock-ups to implement the design specifications and their investigation with end-users to choose a design for further development.},
journal = {Appl. Bionics Biomechanics},
month = {jan},
pages = {29–43},
numpages = {15},
keywords = {Comprehensive Assistive Technology Model, Design, Design Specifications, End-User Involvement, Gender And National Analysis, Robotic Guide, Survey},}

@INPROCEEDINGS{139azenkot2016interactiondesign/motiv,
author={Azenkot, Shiri and Feng, Catherine and Cakmak, Maya},
booktitle={11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
title={Enabling building service robots to guide blind people a participatory design approach}, 
year={2016},
volume={},
number={},
pages={3-10},
publisher={IEEE},
address={Christchurch},
abstract={Building service robots - robots that perform various services in buildings - are becoming more common in large buildings such as hotels and stores. We aim to leverage such robots to serve as guides for blind people. In this paper, we sought to design specifications that detail how a building service robot could interact with and guide a blind person through a building in an effective and socially acceptable way. We conducted participatory design sessions with three designers and five non-designers. Two of the designers and all of the non-designers had a vision disability. Primary features of the design include allowing the user to (1) summon the robot after entering the building, (2) choose from three modes of assistance (Sighted Guide, Escort, and Information Kiosk), and (3) receive information about the building's layout from the robot. We conclude with a discussion of themes and a reflection about our design process that can benefit robot design for blind people in general.},
keywords={Indoor navigation;Legged locomotion;Building services;Indoor environments;Robots;accessibility;blind;participatory design},
doi={10.1109/HRI.2016.7451727},
ISSN={2167-2148},
month={March},}

@inproceedings{140kayukawa2023suitecase,
author = {Kayukawa, Seita and Sato, Daisuke and Murata, Masayuki and Ishihara, Tatsuya and Takagi, Hironobu and Morishima, Shigeo and Asakawa, Chieko},
title = {Enhancing Blind Visitor’s Autonomy in a Science Museum Using an Autonomous Navigation Robot},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581220},
doi = {10.1145/3544548.3581220},
abstract = {Enabling blind visitors to explore museum floors while feeling the facility’s atmosphere and increasing their autonomy and enjoyment are imperative for giving them a high-quality museum experience. We designed a science museum exploration system for blind visitors using an autonomous navigation robot. Blind users can control the robot to navigate them toward desired exhibits while playing short audio descriptions along the route. They can also browse detailed explanations on their smartphones and call museum staff if interactive support is needed. Our real-world user study at a science museum during its opening hour revealed that blind participants could explore the museum safely and independently at their own pace. The study also showed that the sighted visitors who saw the participants walking with the robot accepted the assistive robot well. We finally conducted focus group sessions with the blind participants and discussed further requirements toward a more independent museum experience.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {541},
numpages = {14},
keywords = {Visual impairment, autonomous navigation robot, blind navigation, museum},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{141feng2015indoor/motiv,
author = {Feng, Catherine and Azenkot, Shiri and Cakmak, Maya},
title = {Designing a Robot Guide for Blind People in Indoor Environments},
year = {2015},
isbn = {9781450333184},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701973.2702060},
doi = {10.1145/2701973.2702060},
abstract = {Navigating indoors is challenging for blind people and they often rely on assistance from sighted people. We propose a solution for indoor navigation involving multi-purpose robots that will likely reside in many buildings in the future. In this report, we present a design for how robots can guide blind people to an indoor destination in an effective and socially-acceptable way. We used participatory design, creating a design team with three designers and five non-designers. All but one member of the team had a visual impairment. Our resulting design specifies how the robot and the user initially meet, how the robot guides the user through hallways and around obstacles, and how the robot and user conclude their session.},
booktitle = {Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts},
pages = {107–108},
numpages = {2},
keywords = {blind, indoor navigation, participatory design, robots},
location = {Portland, Oregon, USA},
series = {HRI'15 Extended Abstracts}
}

@Article{142zhang2024aerialguidedog,
AUTHOR = {Zhang, Xiaochen and Pan, Ziyi and Song, Ziyang and Zhang, Yang and Li, Wujing and Ding, Shiyao},
TITLE = {The Aerial Guide Dog: A Low-Cognitive-Load Indoor Electronic Travel Aid for Visually Impaired Individuals},
JOURNAL = {Sensors},
VOLUME = {24},
YEAR = {2024},
NUMBER = {1},
ARTICLE-NUMBER = {297},
NUMPAGES = {21},
URL = {https://www.mdpi.com/1424-8220/24/1/297},
PubMedID = {38203159},
ISSN = {1424-8220},
ABSTRACT = {Most navigation aids for visually impaired individuals require users to pay close attention and actively understand the instructions or feedback of guidance, which impose considerable cognitive loads in long-term usage. To tackle the issue, this study proposes a cognitive burden-free electronic travel aid for individuals with visual impairments. Utilizing human instinctive compliance in response to external force, we introduce the “Aerial Guide Dog”, a helium balloon aerostat drone designed for indoor guidance, which leverages gentle tugs in real time for directional guidance, ensuring a seamless and intuitive guiding experience. The introduced Aerial Guide Dog has been evaluated in terms of directional guidance and path following in the pilot study, focusing on assessing its accuracy in orientation and the overall performance in navigation. Preliminary results show that the Aerial Guide Dog, utilizing Ultra-Wideband (UWB) spatial positioning and Measurement Unit (IMU) angle sensors, consistently maintained minimal deviation from the targeting direction and designated path, while imposing negligible cognitive burdens on users while completing the guidance tasks.},
DOI = {10.3390/s24010297},}

@inproceedings{143tan2019manipulatordirs,
author = {Tan, Xiang Zhi and Carter, Elizabeth J. and Reig, Samantha and Steinfeld, Aaron},
title = {Go That Way: Exploring Supplementary Physical Movements by a Stationary Robot When Providing Navigation Instructions},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353805},
doi = {10.1145/3308561.3353805},
abstract = {We describe an exploration of how kiosk-type stationary robots might provide navigation instructions for blind people. Inspired by a technique used by Orientation \& Mobility experts in which a route is traced out on a person's palm, we developed five methods that supplement verbal instructions with physical movements. We explored the usability, strengths, and limitations of each of our methods in two exploratory studies with blind participants. One method, in which the robot used its entire arm to create path gestures while participants held its gripper, was preferred by 5 out of 8 blind participants and performed comparably on a recall task as a verbal-only instruction method. A closer approximation of the original palm method failed. We analyzed interview data to understand the reasons behind the failures and successes. We discuss the lessons learned from our studies about instruction methods, how robots in public settings can be useful for blind people, and the challenges of deploying such systems in public.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {299–311},
numpages = {13},
keywords = {assistive robots, blindness, human-robot interaction, navigation instructions},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19},}

@INPROCEEDINGS{144agrawal2022cane2seat,
author={Agrawal, Shivendra and West, Mary Etta and Hayes, Bradley},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={A Novel Perceptive Robotic Cane with Haptic Navigation for Enabling Vision-Independent Participation in the Social Dynamics of Seat Choice}, 
year={2022},
volume={},
number={},
pages={9156-9163},
publisher={IEEE},
address={Kyoto, Japan},
location={Kyoto, Japan},
abstract={Goal-based navigation in public places is critical for independent mobility and for breaking barriers that exist for blind or visually impaired (BVI) people in a sight-centric society. Through this work we present a proof-of-concept system that autonomously leverages goal-based navigation assistance and perception to identify socially preferred seats and safely guide its user towards them in unknown indoor environments. The robotic system includes a camera, an IMU, vibrational motors, and a white cane, powered via a backpack-mounted laptop. The system combines techniques from computer vision, robotics, and motion planning with insights from psychology to perform 1) SLAM and object localization, 2) goal disambiguation and scoring, and 3) path planning and guidance. We introduce a novel 2-motor haptic feedback system on the cane's grip for navigation assistance. Through a pilot user study we show that the system is successful in classifying and providing haptic navigation guidance to socially preferred seats, while optimizing for users' convenience, privacy, and intimacy in addition to increasing their confidence in independent navigation. The implications are encouraging as this technology, with careful design guided by the BVI community, can be adopted and further developed to be used with medical devices enabling the BVI population to better independently engage in socially dynamic situations like seat choice.},
keywords={Computer vision;Privacy;Simultaneous localization and mapping;Navigation;Sociology;Robot vision systems;Psychology},
doi={10.1109/IROS47612.2022.9981219},
ISSN={2153-0866},
month={Oct},}

@INPROCEEDINGS{145ye2014cane,
author={Ye, Cang and Hong, Soonhac and Qian, Xiangfei},
booktitle={IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={A Co-Robotic Cane for blind navigation}, 
year={2014},
volume={},
number={},
pages={1082-1087},
publisher={IEEE},
address={San Diego, California, United States},
location={San Diego, California, United States},
abstract={This paper presents a new robotic navigation aid, called Co-Robotic Cane (CRC). The CRC uses a 3D camera for both Pose Estimation (PE) and Object Recognition (OR) in an unknown indoor environment. The 6-DOF PE method determines the CRC's pose change by an egomotion estimation method, called Visual Range Odometry (VRO), and the Iterative Closest Point (ICP) algorithm and reduces the pose integration error by a pose graph optimization algorithm. The PE method does not require any prior knowledge of the environment. The OR method detects indoor structures (stairways, doorways, etc.) and objects (tables, computer monitors, etc.) by the Gaussian Mixture Models. Some of structures/objects (e.g., stairways) may be used as navigational waypoints and the others for obstacle avoidance. The CRC is a co-robot. It may detect human intent and collaborate with its user in performing a navigation task. The proposed CRC is the first in its kind.},
keywords={Three-dimensional displays;Support vector machine classification;Vectors;Cameras;Navigation;Estimation;Object recognition;robotic navigation aid;robot pose estimation;egomotion estimation;pose graph optimization;object recognition},
doi={10.1109/SMC.2014.6974058},
ISSN={1062-922X},
month={Oct},}

@ARTICLE{146mancini2015sensor-phone,
author={Mancini, Adriano and Frontoni, Emanuele and Zingaretti, Primo},
journal={IEEE Transactions on Intelligent Transportation Systems}, 
title={Embedded Multisensor System for Safe Point-to-Point Navigation of Impaired Users}, 
year={2015},
volume={16},
number={6},
pages={3543-3555},
abstract={New smart objects to improve the quality of life in the ambient assisted living (AAL) scenario are capturing the interest of researchers and companies. In particular, novel assistive technologies are being developed to make accessible street navigation to impaired people. The solution that we propose in this new application domain of intelligent transportation systems is a framework for a safe point-to-point navigation, owing to high-detailed road graphs, including sidewalks, crosswalks, and generic “obstacles.” The system is based on a low-cost modular sensor box (embedded hardware) interfaced with a mobile/phone application that acts as an intelligent navigator. The main novelty is the capability to sense the surrounding area while being able to perform a fast path replanning, owing to a real-time link to a remote server, if an obstacle is detected. The sensing is performed using different sensors, such as ultrasound, lidar, and a 77-GHz mid-range automotive radar (absolutely novel in the AAL context), which are processed and fused in the well-established robot operating system (ROS). We tested the framework by analyzing its performance in two different configurations and environments by using, respectively, a sonar and a laser rangefinder in a building scenario and a radar in an urban environment. Even if in both cases results demonstrated a quite good robustness in the obstacle detection with a quasi-real-time route replanning, we were mainly interested and succeeded in demonstrating the high flexibility and extensibility of our framework.},
keywords={Robot sensing systems;Navigation;Embedded systems;Mobile communication;Radar;Ambient assisted living;Multisensor systems;AAL;impaired people;ITS;route planning;point to point navigation;embedded systems;sensor box;radar;ROS;AAL;impaired people;ITS;route planning;point to point navigation;embedded systems;sensor box;radar;ROS},
doi={10.1109/TITS.2015.2489261},
ISSN={1558-0016},
month={Dec},}

@Inbook{147montella2014wheelchair,
author="Montella, Corey
and Perkins, Timothy
and Spletzer, John
and Sands, Michael",
title="To the Bookstore! Autonomous Wheelchair Navigation in an Urban Environment",
bookTitle="Field and Service Robotics: Results of the 8th International Conference",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="249--263",
abstract="In this paper, we demonstrate reliable navigation of a smart wheelchair system (SWS) in an urban environment. Urban environments present unique challenges for service robots. They require localization accuracy at the sidewalk level, but compromise GPS position estimates through significant multi-path effects. However, they are also rich in landmarks that can be leveraged by feature-based localization approaches. To this end, our SWS employed a map-based localization approach. A map of the environment was acquired using a server vehicle, synthesized a priori, and made accessible to the SWS. The map embedded not only the locations of landmarks, but also semantic data delineating 7 different landmark classes to facilitate robust data association. Landmark segmentation and tracking by the SWS was then accomplished using both 2D and 3D LIDAR systems. The resulting localization method has demonstrated decimeter level positioning accuracy in a global coordinate frame. The localization package was integrated into a ROS framework with a sample based motion planner and control loop running at 5 Hz to enable autonomous navigation. For validation, the SWS repeatedly navigated autonomously between Lehigh University's Packard Laboratory and the University bookstore, a distance of approximately 1.0 km roundtrip.",
isbn="978-3-642-40686-7",
doi="10.1007/978-3-642-40686-7_17",
url="https://doi.org/10.1007/978-3-642-40686-7_17",}

@inproceedings{148ito2005cyarm,
author = {Ito, Kiyohide and Okamoto, Makoto and Akita, Junichi and Ono, Tetsuo and Gyobu, Ikuko and Takagi, Tomohito and Hoshi, Takahiro and Mishima, Yu},
title = {CyARM: an alternative aid device for blind persons},
year = {2005},
isbn = {1595930027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1056808.1056947},
doi = {10.1145/1056808.1056947},
abstract = {With the concept of 'human-machine interface', designed especially for visually impaired persons, we have developed an electric aid device for use in guiding orientation and locomotion. The device, which we call CyARM, measures the distance between a person and an object with an ultrasonic sensor and transmits the distance information to the user's haptic sense. In this report, we will: (1) outline the concept of CyARM, (2) describe its mechanism, and (3) demonstrate three preliminary experiments that verify the usability of CyARM. We conducted the experiments in terms of detection of objects, detection of space, and tracking object movement. As a result of these experiments, we have concluded that CyARM is potentially effective for visually impaired persons. Our study will encourage the related studies of user interfaces, particularly focusing on electric aid devices that guide visually impaired persons in detecting their environment.},
booktitle = {CHI '05 Extended Abstracts on Human Factors in Computing Systems},
pages = {1483–1488},
numpages = {6},
keywords = {visual impairment, user interfaces, haptic sense, electric aid device},
location = {Portland, OR, USA},
series = {CHI EA '05}
}

@inproceedings{149pariti2020cane,
author = {Pariti, Jagannadh and Tibdewal, Vinita and Oh, Tae},
title = {Intelligent Mobility Cane - Lessons Learned from Evaluation of Obstacle Notification System using a Haptic Approach},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3375217},
doi = {10.1145/3334480.3375217},
abstract = {Existing smart cane prototypes provide audio and/or haptic feedback to inform people who are blind or visually impaired about upcoming obstacles. However, limited user research is conducted to evaluate the usefulness of the haptic feedback provided by these devices. To better understand the users' perceptions of haptic feedback, we developed a smart cane prototype called Intelligent Mobility Cane (IMC) that consists of 2 haptic vibrators on the handle. They are used to inform different parts of the user's hand that an obstacle is detected. 8 people who are blind and 3 people who have low vision explored the IMC's handset by navigating an indoor obstacle path. The participants provided their feedback on the IMC's haptic notification system with regards to the intensity of the vibration and location of the vibrators and discussed various scenarios where the feedback will or will not be useful to them. In this case study, IMC handle design recommendations based on the participant's feedback and suggestions are presented.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {accessibility, feedback, handle, intelligent mobility cane, vibration},
location = {Honolulu, Hawaii, United States of Amerca},
series = {CHI EA '20}
}

@inproceedings{150kayukawa2019bbeep,
author = {Kayukawa, Seita and Higuchi, Keita and Guerreiro, Jo\~{a}o and Morishima, Shigeo and Sato, Yoichi and Kitani, Kris and Asakawa, Chieko},
title = {BBeep: A Sonic Collision Avoidance System for Blind Travellers and Nearby Pedestrians},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300282},
doi = {10.1145/3290605.3300282},
abstract = {We present an assistive suitcase system, BBeep, for supporting blind people when walking through crowded environments. BBeep uses pre-emptive sound notifications to help clear a path by alerting both the user and nearby pedestrians about the potential risk of collision. BBeep triggers notifications by tracking pedestrians, predicting their future position in real-time, and provides sound notifications only when it anticipates a future collision. We investigate how different types and timings of sound affect nearby pedestrian behavior. In our experiments, we found that sound emission timing has a significant impact on nearby pedestrian trajectories when compared to different sound types. Based on these findings, we performed a real-world user study at an international airport, where blind participants navigated with the suitcase in crowded areas. We observed that the proposed system significantly reduces the number of imminent collisions.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {blind navigation, collision prediction, obstacle avoidance, path clearing, pedestrian detection, visual impairments},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{151kacorri2018envfactors/motiv,
author = {Kacorri, Hernisa and Ohn-Bar, Eshed and Kitani, Kris M. and Asakawa, Chieko},
title = {Environmental Factors in Indoor Navigation Based on Real-World Trajectories of Blind Users},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173630},
doi = {10.1145/3173574.3173630},
abstract = {Indoor localization technologies can enhance quality of life for blind people by enabling them to independently explore and navigate indoor environments. Researchers typically evaluate their systems in terms of localization accuracy and user behavior along planned routes. We propose two measures of path-following behavior: deviation from optimal route and trajectory variability. Through regression analysis of real-world trajectories from blind users, we identify relationships between a) these measures and b) elements of the environment, route characteristics, localization error, and instructional cues that users receive. Our results provide insights into path-following behavior for turn-by-turn indoor navigation and have implications for the design of future interactions. Moreover, our findings highlight the importance of reporting these environmental factors and route properties in similar studies. We present automated and scalable methods for their calculation and to encourage their reporting for better interpretation and comparison of results across future studies.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {accessibility, blind, indoor navigation, trajectory, turn-by-turn navigation},
location = {Montreal, Quebec, Canada},
series = {CHI '18}
}

@ARTICLE{152lii2020imminent/wearable,
author={Li, Zhongen and Song, Fanghao and Clark, Brian C. and Grooms, Dustin R. and Liu, Chang},
journal={IEEE Access}, 
title={A Wearable Device for Indoor Imminent Danger Detection and Avoidance With Region-Based Ground Segmentation}, 
year={2020},
volume={8},
number={},
pages={184808-184821},
abstract={Avoiding objects independently in indoor environments for individuals with severe visual impairment is one of the significant challenges in daily life. This paper presents a wearable application to help visually impaired people quickly build situational awareness and traverse safely. The system utilizes Red, Green, Blue, and Depth (RGB-D) camera and an Inertial Measurement Unit (IMU) to detect objects and the collision-free path in real-time. A region proposal module is presented to decide where to identify the ground from 3D point clouds. The segmented ground area can act as the traversable path, and its corresponding region in the image is removed to prevent detecting painted objects. The system can provide information about the category, distance, and direction of the detected objects by fusing the depth image and the neural network results. A 3D acoustic feedback mechanism is designed to improve the situational awareness for visually impaired people, and guild them traverse safely. The advantage of this system is that our 3D region proposal module can robustly propose the potential ground region and greatly reduce the computation cost of the ground segmentation. Besides, a typical machine-learning-based approach may miss objects because they could not be recognized, though they still may pose a danger. Another advantage of our approach is that the imminent danger detector can detect such unrecognizable objects to help users avoid a collision. Finally, experimental results demonstrate that the proposed system can be a useful indoor assistant tool to help blind individuals with collision avoidance and wayfinding.},
keywords={Three-dimensional displays;Real-time systems;Cameras;Acoustics;Object detection;Navigation;Visualization;Convolutional neural network;ground segmentation;object detection;point cloud;region of interest;visual impairment;wearable assistive system},
doi={10.1109/ACCESS.2020.3028527},
ISSN={2169-3536},
month={},}

@INPROCEEDINGS{153jin2021navmanip/wearable,
author={Jin, Lingqiu and Zhang, He and Ye, Cang},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={A Wearable Robotic Device for Assistive Navigation and Object Manipulation}, 
year={2021},
volume={},
number={},
pages={765-770},
publisher={IEEE},
address={Prague},
abstract={This paper presents a hand-worn assistive device to assist a visually impaired person with object manipulation. The device uses a Google Pixel 3 as the computational platform, a Structure Core (SC) sensor for perception, a speech interface, and a haptic interface for human-device interaction. W-ROMA is intended to assist a visually impaired person to locate a target object (nearby or afar) and guide the user to move towards and eventually take a hold of the object. To achieve this objective, three functions, including object detection, wayfinding, and motion guidance, are developed. Object detection locates the target object’s position if it falls within the camera’s field of view. Wayfinding enables the user to approach the object. The haptic/speech interface guides the user to move close to the object and then guides the hand to reach the object. A new visual-inertial odometery (VIO), called RGBD-VIO, is devised to accurately estimate the device’s pose (position and orientation), which is then used to generate the motion command to guide the user and his/her hand to reach the object. Experimental results demonstrate that RGBD-VIO outperforms the state-of-the-art VIO methods in 6-DOF device pose estimation and the device is effective in assistive object manipulation.},
keywords={Target tracking;Navigation;Pose estimation;Object detection;Robot sensing systems;Internet;Haptic interfaces},
doi={10.1109/IROS51168.2021.9636126},
ISSN={2153-0866},
month={Sep.},}

@INPROCEEDINGS{154wilson2007swan,
author={Wilson, Jeff and Walker, Bruce N. and Lindsay, Jeffrey and Cambias, Craig and Dellaert, Frank},
booktitle={11th IEEE International Symposium on Wearable Computers}, 
title={SWAN: System for Wearable Audio Navigation}, 
year={2007},
volume={},
number={},
pages={91-98},
publisher={IEEE},
address={Boston},
location={Boston, MA, USA},
abstract={Wearable computers can certainly support audio-only presentation of information; a visual interface need not be present for effective user interaction. A system for wearable audio navigation (SWAN) is being developed to serve as a navigation and orientation aid for persons temporarily or permanently visually impaired. SWAN is a wearable computer consisting of audio-only output and tactile input via a handheld interface. SWAN aids a user in safe pedestrian navigation and includes the ability for the user to author new GIS data relevant to their needs of wayfinding, obstacle avoidance, and situational awareness support. Emphasis is placed on representing pertinent data with non-speech sounds through a process of sonification. SWAN relies on a geographic information system (GIS) infrastructure for supporting geocoding and spatialization of data. Furthermore, SWAN utilizes novel tracking technology.},
keywords={Navigation;Wearable computers;Geographic Information Systems;Auditory displays;Path planning;Speech synthesis;Psychology;Biomedical computing;Computer interfaces;Humans},
doi={10.1109/ISWC.2007.4373786},
ISSN={2376-8541},
month={Oct},}

@article{155hong2022guidedog/survey,
title = {Development and application of key technologies for Guide Dog Robot: A systematic literature review},
journal = {Robotics and Autonomous Systems},
volume = {154},
pages = {104104},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104104},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022000562},
author = {Bin Hong and Zhangxi Lin and Xin Chen and Jing Hou and Shunya Lv and Zhendong Gao},
keywords = {Guide Dog Robot, Visually impaired, Navigation, Obstacle avoidance, Human–robot​ interaction},
abstract = {In the current situation of many visually handicapped people worldwide, yet the corresponding number of guide dogs is quite rare. It activates the application of advanced technology to broaden their horizons and allow them to embrace the world. This paper will review the research state of the Guide Dog Robot (GDR) for people with visual impairment and present some views. According to the application scenes, we have divided the GDR into two categories: specific scene applicable type and universal scene applicable type, with the description of different performances under various scenes. Then the current research focuses are elaborated, including localization and navigation technology, recognition of traffic signs, human–robot interaction (HRI), speed coordination, and walking structure design. Subsequently, the studying directions and challenges of GDR are discussed, and collaborative human–robot mode is believed to become the research mainstream. Finally, we conclude this review and explain why few GDR has realized commercialization. The limitations of current studies and some recommendations for future research are presented.},
}

@Article{156hsieh2024indoormonocularnav,
author={Hsieh, Yi-Zeng
and Ku, Xiang-Long
and Lin, Shih-Syun},
title={The development of assisted- visually impaired people robot in the indoor environment based on deep learning},
journal={Multimedia Tools and Applications},
year={2024},
month={Jan},
day={01},
volume={83},
number={3},
pages={6555-6578},
abstract={The indoor positioning for visually impaired people has influence on their daily life in unknown indoor environment. This study designs the robot that can assist the blind walking safety and navigate in indoor environment by a single camera. The sense classification is proposed to position the blind in indoor environment by proposed convolutional neural network framework and integrate the semantic segmentation to find the road surface through a depth camera to guide the blind walking. The proposed vision-based sense classification method is compared with the traditional WiFi triangular-positioning method, and the average error of x-y coordinate position result as (9.25,3.65) is better. From the experiment, the designed robot can help the visually impaired people to indoor navigation in unknown indoor environment.},
issn={1573-7721},
doi={10.1007/s11042-023-15644-y},
url={https://doi.org/10.1007/s11042-023-15644-y}
}

@article{157due2023outdoornavoz/motiv,
author = {Brian L. Due},
title ={A Walk in the Park With Robodog: Navigating Around Pedestrians Using a Spot Robot as a “Guide Dog”},
journal = {Space and Culture},
volume = {0},
number = {0},
pages = {12063312231159215},
year = {2023},
doi = {10.1177/12063312231159215},
URL = { https://doi.org/10.1177/12063312231159215 },
eprint = { https://doi.org/10.1177/12063312231159215 },
abstract = { This article explores how visually impaired people (VIP) navigate around (a) stationary people and (b) moving people, when guided by the Boston Dynamics’ robotic “dog” and its human operator. By focusing on the micro-spatial dimensions of human mobility while being guided by a mobile robot, the paper argues that the VIP+robodog+operator is in situ emerging as a socio-material assemblage in which agency, perception, and trust gets distributed and that this distribution enables the accomplishment of navigation. The article is based on ethnomethodology and multimodal conversation analysis (EMCA) and a video ethnographic methodology. It contributes to studies in perception, agency, human–robot interaction, space and culture, and distributed co-operative action in socio-material settings. },
}

@inproceedings{158rahman2023handwayfinding,
author = {Rahman, Adil and Azim, Md Aashikur Rahman and Heo, Seongkook},
title = {Take My Hand: Automated Hand-Based Spatial Guidance for the Visually Impaired},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581415},
doi = {10.1145/3544548.3581415},
abstract = {Tasks that involve locating objects and then moving hands to those specific locations, such as using touchscreens or grabbing objects on a desk, are challenging for the visually impaired. Over the years, audio guidance and haptic feedback have been a staple in hand navigation based assistive technologies. However, these methods require the user to interpret the generated directional cues and then manually perform the hand motions. In this paper, we present automated hand-based spatial guidance to bridge the gap between guidance and execution, allowing visually impaired users to move their hands between two points automatically, without any manual effort. We implement this concept through FingerRover, an on-finger miniature robot that carries the user’s finger to target points. We demonstrate the potential applications that can benefit from automated hand-based spatial guidance. Our user study shows the potential of our technique in improving the interaction capabilities of people with visual impairments.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {544},
numpages = {16},
keywords = {accessibility, automated guidance, miniature guiding robot, spatial guidance, visual impairment},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{159kuribayashi2023pathfinder,
author = {Kuribayashi, Masaki and Ishihara, Tatsuya and Sato, Daisuke and Vongkulbhisal, Jayakorn and Ram, Karnik and Kayukawa, Seita and Takagi, Hironobu and Morishima, Shigeo and Asakawa, Chieko},
title = {PathFinder: Designing a Map-less Navigation System for Blind People in Unfamiliar Buildings},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580687},
doi = {10.1145/3544548.3580687},
abstract = {Indoor navigation systems with prebuilt maps have shown great potential in navigating blind people even in unfamiliar buildings. However, blind people cannot always benefit from them in every building, as prebuilt maps are expensive to build. This paper explores a map-less navigation system for blind people to reach destinations in unfamiliar buildings, which is implemented on a robot. We first conducted a participatory design with five blind people, which revealed that intersections and signs are the most relevant information in unfamiliar buildings. Then, we prototyped PathFinder, a navigation system that allows blind people to determine their way by detecting and conveying information about intersections and signs. Through a participatory study, we improved the interface of PathFinder, such as the feedback for conveying the detection results. Finally, a study with seven blind participants validated that PathFinder could assist users in navigating unfamiliar buildings with increased confidence compared to their regular aid.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {41},
numpages = {16},
keywords = {intersection detection, orientation and mobility, sign recognition, visual impairment},
location = {Hamburg, Germany},
series = {CHI '23},
}

@Article{160kang2001multiobjectnav,
author={Kang, Dong-Oh
and Kim, Sung-Hun
and Lee, Heyoung
and Bien, Zeungnam},
title={Multiobjective Navigation of a Guide Mobile Robot for the Visually Impaired Based on Intention Inference of Obstacles},
journal={Autonomous Robots},
year={2001},
month={Mar},
day={01},
volume={10},
number={2},
pages={213-230},
abstract={Different from ordinary mobile robots used in a well-structured industrial workspace, a guide mobile robot for the visually impaired should be designed in consideration of multiple moving obstacles of various types and with different speeds while it adaptively maintains a certain distance from the user. Here, the moving obstacles mostly refer to pedestrians in intentional motions. Thus, navigation of the guide robot can be facilitated if the intention of each obstacle detected can be known in advance.},
issn={1573-7527},
doi={10.1023/A:1008990105090},
url={https://doi.org/10.1023/A:1008990105090}
}

@INPROCEEDINGS{161kulyukin2005robocart,
author={Kulyukin, V. and Gharpure, C. and Nicholson, J.},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
title={RoboCart: toward robot-assisted navigation of grocery stores by the visually impaired}, 
year={2005},
volume={},
number={},
pages={2845-2850},
publisher={IEEE},
address={Edmonton, AB, Canada},
abstract={This paper presents RoboCart, a proof-of-concept prototype of a robotic shopping assistant for the visually impaired. The purpose of RoboCart is to help visually impaired customers navigate a typical grocery store and carry purchased items. The current hardware and software components of the system are presented. For localization, RoboCart relies on RFID tags deployed at various locations in the store. For navigation, RoboCart relies on laser range finding. Experiences with deploying RoboCart in a real grocery store are described. The current status of the system and its limitations are outlined.},
keywords={Navigation;Computer science;Laboratories;Hardware;Dogs;Indoor environments;Robot sensing systems;Paper technology;Software prototyping;Prototypes;service robotics},
doi={10.1109/IROS.2005.1545107},
ISSN={2153-0866},
month={Aug},}

@INPROCEEDINGS{162gallo2010multimodalcane,
author={Gallo, S. and Chapuis, D. and Santos-Carreras, L. and Kim, Y. and Retornaz, P. and Bleuler, H. and Gassert, R.},
booktitle={3rd IEEE RAS \& EMBS International Conference on Biomedical Robotics and Biomechatronics}, 
title={Augmented white cane with multimodal haptic feedback}, 
year={2010},
volume={},
number={},
pages={149-155},
publisher={IEEE},
address={Tokyo, Japan},
abstract={This paper proposes an instrumented handle with multimodal augmented haptic feedback, which can be integrated into a conventional white cane to extend the haptic exploration range of visually impaired users. The information extracted from the environment through a hybrid range sensor is conveyed to the user in an intuitive manner over two haptic feedback systems. The first renders impulses that imitate the impact of the real cane with a distant obstacle. In combination with the range sensors, this system allows to “touch” and explore remote objects, thus compensating for the limited range of the conventional white cane without altering its intuitive usage. The impulses are generated by storing kinetic energy in a spinning inertia wheel, which is released by abruptly braking the wheel. Furthermore, a vibrotactile interface integrated into the ergonomic handle conveys the distance to obstacles to the user. Three vibrating motors located along the index finger and hand are activated in different spatiotemporal patterns to induce a sense of distance through apparent movement. The realized augmented white cane not only increases the safety of the user by detecting obstacles from a further distance and alerting about those located at the head level, but also allows the user to build extended spatial mental models by increasing the sensing range, thereby allowing anticipated decision making and thus more natural navigation.},
keywords={Electric shock;Wheels;Force;Torque;Haptic interfaces;DC motors;Sensors},
doi={10.1109/BIOROB.2010.5628066},
ISSN={2155-1782},
month={Sep.},}

@INPROCEEDINGS{163ando2010forceblinker2,
author={Ando, Takeshi and Tsukahara, Ryota and Seki, Masatoshi and Fujie, Masakatsu G.},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
title={Mechanism and evaluation of a haptic interface “Force Blinker 2” for navigation of the visually impaired}, 
year={2010},
volume={},
number={},
pages={4680-4685},
publisher={IEEE},
address={Taipei, Taiwan},
abstract={In the navigation of the visually impaired, an external input such as force or sound is required about the direction of travel to reach a particular target position. We develop a new haptic interface called “Force Blinker 2” to navigate the visually impaired. In Force Blinker 2, rotating weights and repulsive magnets are used to reduce the force generated to the direction opposite the traveling direction, which caused false recognition in the previous system, “Force Blinker 1.” In Force Blinker 2, the rotational radius of the weight varies depending on the velocity of the rotational weight. Ten visually impaired subjects evaluated Force Blinker 2 by comparing it with Force Blinker 1, a fixed radius type interface. The directions presented by Force Blinker 2 were correctly recognized at a rate of approximately 85%, which is approximately a 10% improvement over the rate by Force Blinker 1.},
keywords={Force;Haptic interfaces;Navigation;Rails;Magnetic flux;Prototypes;Mathematical model},
doi={10.1109/IROS.2010.5651077},
ISSN={2153-0866},
month={Oct},}

@INPROCEEDINGS{164pradeep2010vision,
author={Pradeep, Vivek and Medioni, Gerard and Weiland, James},
booktitle={IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops}, 
title={Robot vision for the visually impaired}, 
year={2010},
volume={},
number={},
pages={15-22},
publisher={IEEE},
address={San Francisco, California, United States},
abstract={We present a head-mounted, stereo-vision based navigational assistance device for the visually impaired. The head-mounted design enables our subjects to stand and scan the scene for integrating wide-field information, compared to shoulder or waist-mounted designs in literature which require body rotations. In order to extract and maintain orientation information for creating a sense of egocentricity in blind users, we incorporate visual odometry and feature based metric-topological SLAM into our system. Using camera pose estimates with dense 3D data obtained from stereo triangulation, we build a vicinity map of the user's environment. On this map, we perform 3D traversability analysis to steer subjects away from obstacles in the path. A tactile interface consisting of microvibration motors provides cues for taking evasive action, as determined by our vision processing algorithms. We report experimental results of our system (running at 10 Hz) and conduct mobility tests with blindfolded subjects to demonstrate the usefulness of our approach over conventional navigational aids like the white cane.},
keywords={Robot vision systems;Navigation;Simultaneous localization and mapping;Cameras;Layout;Data mining;Performance analysis;Micromotors;System testing;Vibration control},
doi={10.1109/CVPRW.2010.5543579},
ISSN={2160-7516},
month={June},}

@inproceedings{165galatas2011eyedog,
author = {Galatas, Georgios and McMurrough, Christopher and Mariottini, Gian Luca and Makedon, Fillia},
title = {eyeDog: an assistive-guide robot for the visually impaired},
year = {2011},
isbn = {9781450307727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141622.2141691},
doi = {10.1145/2141622.2141691},
abstract = {Visually impaired people can navigate unfamiliar areas by relying on the assistance of other people, canes, or specially trained guide dogs. Guide dogs provide the impaired person with the highest degree of mobility and independence, but require expensive training and selective breeding. In this paper we describe the design and development of a prototype assistive-guide robot (eyeDog) that provides the visually impaired person with autonomous vision-based navigation and laser-based obstacle avoidance capabilities. This kind of assistive-guide robot has several advantages, such as robust performance and reduced cost and maintenance. The main components of our system are the Create robotic platform (from iRobot), a net-book, an on-board USB webcam and a LIDAR unit. The camera is used as the primary exteroceptive sensor for the navigation task; the frames captured by the camera are processed in order to robustly estimate the position of the vanishing point associated to the road/corridor where the eyeDog needs to move. The controller will then steer the robot until the vanishing point and the image center coincide. This condition guarantees the robot to move parallel to the direction of the road/corridor. While moving, the robot uses the LIDAR for obstacle avoidance.},
booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {58},
numpages = {8},
keywords = {Hough transform, RANSAC, autonomous assistive robot, vision-based navigation},
location = {Heraklion, Crete, Greece},
series = {PETRA '11}
}

@INPROCEEDINGS{166kaiser2012wearablenav,
author={Kaiser, Esteban Bayro and Lawo, Michael},
booktitle={IEEE/ACIS 11th International Conference on Computer and Information Science}, 
title={Wearable Navigation System for the Visually Impaired and Blind People}, 
year={2012},
volume={},
number={},
pages={230-233},
publisher={IEEE},
address={Shanghai, China},
abstract={A wearable navigation system for visually impaired and blind people in unknown indoor and outdoor environment is presented. This system will map and track the position of the pedestrian during the exploration of the new environment. In order to build this system the well known Simultaneous Localization and Mapping (SLAM) from mobile robotics will implemented. Once a map is created the user can be guided efficiently through it. The user will be equipped with a short range laser, an inertial measurement unit (IMU), a wearable computer for processing purpose and a bone head phon. This system does not intent to replace the use of the white cane. Its purpose is to gather contextual information to aid the user to navigate},
keywords={Simultaneous localization and mapping;Mobile robots;Global Positioning System;Measurement by laser beam;Buildings;Lasers;Wearable computing;Pedestrian Navigation;Visually Impaired and Blind People},
doi={10.1109/ICIS.2012.118},
ISSN={},
month={May},}

@INPROCEEDINGS{167wei2013fuzzy,
author={Wei, Yuanlong and Kou, Xiangxin and Lee, Mincheol},
booktitle={13th International Conference on Control, Automation and Systems (ICCAS 2013)}, 
title={Development of a guide-dog robot system for the visually impaired by using fuzzy logic based human-robot interaction approach}, 
year={2013},
volume={},
number={},
pages={136-141},
publisher={IEEE},
address={Gwangju, South Korea},
abstract={This paper presents a development of guide-dog robot system for visually impaired. Based on a hall-sensor joystick and ultrasonic sensors, a “smart rope” system is designed for the human-robot interaction. In this system, multiple functions are provided for the self-walking in urban system, such as following, navigation and obstacle avoidance. To distinguish between small involuntary force and the intended navigational movement, a fuzzy logic control method is applied to improve the accuracy for the “smart rope” system manipulation. To compensate the lack of visual sense of visually impaired, a smart phone with camera is utilized as the robot vision, in order to detect the traffic lights and the zebra crossing. A fast vision recognition approach is provided based on Adaboosting and Template matching combined algorithm. For the evaluation of proposed method, an integrated system is implemented to the mobile robot platform. The performance of both interactive system and vision system are analyzed after the experiment in the urban environment. System's accuracy, usefulness and adaptability are verified. The experimental results showed that this new designed guide-dog robot system is suitable and effective to assist the visually impaired for the self-walking.},
keywords={Robot sensing systems;Frequency control;Acoustics;Force;Visually impaired;Guide-dog robot;Hall-sensor joystick;ultrasonic sensor;Fuzzy logic control;Adaboosting;Template matching;Traffic lights;Zebra crossing},
doi={10.1109/ICCAS.2013.6703877},
ISSN={2093-7121},
month={Oct},}

@INPROCEEDINGS{168ni2013vibrotacticlevoice,
author={Ni, Dejing and Wang, Lu and Ding, Yu and Zhang, Jun and Song, Aiguo and Wu, Juan},
booktitle={IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
title={The design and implementation of a walking assistant system with vibrotactile indication and voice prompt for the visually impaired}, 
year={2013},
volume={},
number={},
pages={2721-2726},
publisher={IEEE},
address={Shenzhen, China},
abstract={This paper presents a comprehensive navigation system to assist the visually impaired in safely walking in their living areas. The system can provide the visually impaired vibrotactile hints of navigation and free directions during outdoor walking. The cluttered environmental information is acquired by Kinect cameras and ultrasonic sensors in real time. All the direction and obstacle avoidance instructions are transformed into a series of vibrating stimulus and actuated by the vibration belt. Meanwhile, GPS and speech recognition modules are used for navigation. Users can set destination through speech recognition module and get their current location by GPS module. With the help of Google terminal, the system can provide a wide range of path navigation to users. Three experiments are conducted to testify the effectiveness of the system. The results show that the method of detecting environment and indication way is effective.},
keywords={Sensors;Cameras;Vibrations;Legged locomotion;Acoustics;Global Positioning System},
doi={10.1109/ROBIO.2013.6739885},
ISSN={},
month={Dec},}

@INPROCEEDINGS{169pyun2013cane,
author={Pyun, Rosali and Kim, Yeongmi and Wespe, Pascal and Gassert, Roger and Schneller, Stefan},
booktitle={IEEE 13th International Conference on Rehabilitation Robotics (ICORR)}, 
title={Advanced Augmented White Cane with obstacle height and distance feedback}, 
year={2013},
volume={},
number={},
pages={1-6},
publisher={IEEE},
address={Seattle},
location={Seattle, Washington, United States},
abstract={The white cane is a widely used mobility aid that helps visually impaired people navigate the surroundings. While it reliably and intuitively extends the detection range of ground-level obstacles and drop-offs to about 1.2 m, it lacks the ability to detect trunk and head-level obstacles. Electronic Travel Aids (ETAs) have been proposed to overcome these limitations, but have found minimal adoption due to limitations such as low information content and low reliability thereof. Although existing ETAs extend the sensing range beyond that of the conventional white cane, most of them do not detect head-level obstacles and drop-offs, nor can they identify the vertical extent of obstacles. Furthermore, some ETAs work independent of the white cane, and thus reliable detection of surface textures and drop-offs is not provided. This paper introduces a novel ETA, the Advanced Augmented White Cane, which detects obstacles at four vertical levels and provides multi-sensory feedback. We evaluated the device in five blindfolded subjects through reaction time measurements following the detection of an obstacle, as well as through the reliability of dropoff detection. The results showed that our aid could help the user successfully detect an obstacle and identify its height, with an average reaction time of 410 msec. Drop-offs were reliably detected with an intraclass correlation > 0.95. This work is a first step towards a low-cost ETA to complement the functionality of the conventional white cane.},
keywords={Acoustics;Infrared sensors;Navigation;Reliability;Ultrasonic variables measurement;Legged locomotion;Vibrations;Electronic Travel Aid (ETA);ultrasonic sensor;infrared sensor;vibrotactile display;head-level obstacle;drop-off detection},
doi={10.1109/ICORR.2013.6650358},
ISSN={1945-7901},
month={June},}

@INPROCEEDINGS{170sekiguchi2014wearablenav,
author={Sekiguchi, Mari and Ishiwata, Koichi and Fuchida, Masataka and Nakamura, Akio},
booktitle={The 23rd IEEE International Symposium on Robot and Human Interactive Communication}, 
title={Development of a wearable system for navigating the visually impaired in the indoor environment - a prototype system for fork detection and navigation -}, 
year={2014},
volume={},
number={},
pages={549-554},
publisher={IEEE},
address={Edinburgh, United Kingdom},
location={Edinburgh, United Kingdom},
abstract={We propose a wearable guide system for supporting the visually impaired. A wearable device equipped with a small laser range sensor and a gyroscope as sensors is developed. The sensors are attached to the user's chest. The range sensor is utilized to obtain distance information to the wall in the horizontal cross-sectional plane in front of the user. The gyroscope is adopted to estimate user's direction utilized in the fork pattern classification. The system classifies passage forks of the indoor environment into 7 patterns; left turn, straight, right turn, T-junction (left), T-junction (dead end), T-junction (right), crossroads. Based on the fork classification and prepared environmental topological map, the system instructs the user appropriate direction by voice at the fork. Experimental trials show the basic validity of the proposed system. Among 10 eye-masked subjects who join the experiment, the only two persons can reach the destination without the voice announcement. On the other hand, 8 of 10 subjects who wear the proposed wearable device can reach the destination.},
keywords={Gyroscopes;Robot sensing systems;Indoor environments;Global Positioning System;Measurement by laser beam},
doi={10.1109/ROMAN.2014.6926310},
ISSN={1944-9437},
month={Aug},}

@ARTICLE{171park2015telehapticmuseum,
author={Park, Chung Hyuk and Ryu, Eun-Seok and Howard, Ayanna M.},
journal={IEEE Transactions on Haptics}, 
title={Telerobotic Haptic Exploration in Art Galleries and Museums for Individuals with Visual Impairments}, 
year={2015},
volume={8},
number={3},
pages={327-338},
abstract={This paper presents a haptic telepresence system that enables visually impaired users to explore locations with rich visual observation such as art galleries and museums by using a telepresence robot, a RGB-D sensor (color and depth camera), and a haptic interface. The recent improvement on RGB-D sensors has enabled real-time access to 3D spatial information in the form of point clouds. However, the real-time representation of this data in the form of tangible haptic experience has not been challenged enough, especially in the case of telepresence for individuals with visual impairments. Thus, the proposed system addresses the real-time haptic exploration of remote 3D information through video encoding and real-time 3D haptic rendering of the remote real-world environment. This paper investigates two scenarios in haptic telepresence, i.e., mobile navigation and object exploration in a remote environment. Participants with and without visual impairments participated in our experiments based on the two scenarios, and the system performance was validated. In conclusion, the proposed framework provides a new methodology of haptic telepresence for individuals with visual impairments by providing an enhanced interactive experience where they can remotely access public places (art galleries and museums) with the aid of haptic modality and robotic telepresence.},
keywords={Haptic interfaces;Three-dimensional displays;Robot sensing systems;Streaming media;Visualization;Image color analysis;Haptic telepresence;assistive robotics;3D haptic rendering;visual impairment;video encoding;depth sensors;multimedia streaming;Haptic telepresence;assistive robotics;3D haptic rendering;visual impairment;video encoding;depth sensors;multimedia streaming},
doi={10.1109/TOH.2015.2460253},
ISSN={2329-4051},
month={July},}

@INPROCEEDINGS{172zhang2016sixdofnavaid,
author={Zhang, He and Ye, Cang},
booktitle={IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
title={An indoor navigation aid for the visually impaired}, 
year={2016},
volume={},
number={},
pages={467-472},
publisher={IEEE},
address={Qingdao, China},
abstract={This paper presents a 6-DOF pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired (VI). The PE method involves two graph SLAM processes to reduce the accumulative pose error of device. In the first step, the floor plane is extracted from the 3D camera's point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch and Z errors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the existing state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a VI person in an indoor environment. The system uses the estimated pose and floorplan to locate the device user in a building and guide the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.},
keywords={Simultaneous localization and mapping;Cameras;RNA;Three-dimensional displays;Navigation;Feature extraction;Floors},
doi={10.1109/ROBIO.2016.7866366},
ISSN={},
month={Dec},}

@INPROCEEDINGS{173garrote2018walkerintent,
author={Garrote, Luís and Paulo, João and Perdiz, João and Peixoto, Paulo and Nunes, Urbano J.},
booktitle={27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}, 
title={Robot-Assisted Navigation for a Robotic Walker with Aided User Intent}, 
year={2018},
volume={},
number={},
pages={348-355},
publisher={IEEE},
address={Nanjing, China},
abstract={This paper presents an approach to robot-assisted navigation on a mobility assistance context, by learning from the user while helping him navigate efficiently and safely in complex environments. Assistive robots such as robotic walkers provide the ability to support a user's body weight on the upper limbs while walking. However, walkers can add an extra layer of distress due to their specific manipulation constraints. For the users of such devices, lack of dexterous upper limb control can be a considerable problem as it means they may be unable to operate these devices efficiently; users may also have visual impairments that reduce their navigational efficiency. The proposed approach uses a Reinforcement Learning (RL) model and a dynamic window-based local motion planning algorithm. It aims to learn needed corrections in the motion command based on the surrounding environment and the user's intent. The proposed solution handles corrections and guides the user through the environment without collisions while learning and aiding the user when he is unable to operate the device efficiently. The RL based approach was tested in indoor scenarios with a robotic walker platform showing preliminary promising results.},
keywords={Navigation;Legged locomotion;Robot kinematics;Collision avoidance;Robot sensing systems},
doi={10.1109/ROMAN.2018.8525674},
ISSN={1944-9437},
month={Aug},}

@INPROCEEDINGS{174chen2017cane,
author={Chen, Qingtian and Khan, Muhammad and Tsangouri, Christina and Yang, Christopher and Li, Bing and Xiao, Jizhong and Zhu, Zhigang},
booktitle={IEEE 7th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)}, 
title={CCNY Smart Cane}, 
year={2017},
volume={},
number={},
pages={1246-1251},
publisher={IEEE},
address={Honolulu},
location={Honolulu, Hawaii, United States},
abstract={This paper presents SmartCane - the CCNY Smart Cane system, a robotic white cane and mobile device navigation software for visually impaired people. The system includes software for Google Tango devices that utilizes simultaneous localization and mapping (SLAM) to plan a path and guide a visually impaired user to waypoints within indoor environments. A control panel is mounted on the standard white cane that enables visually impaired users to communicate with the navigation software and is additionally used to provide navigation instructions via haptic feedback. Based on the motion-tracking and localization capabilities of the Google Tango, the SmartCane is able to generate a safe path to the destination waypoint indicated by the user.},
keywords={Navigation;Software;Google;Indoor environments;Mobile handsets;Simultaneous localization and mapping;Indoor assistive navigation;SLAM;Tango device;blind and visual impairment},
doi={10.1109/CYBER.2017.8446303},
ISSN={},
month={July},}

@ARTICLE{175mancini2018walkingassistance,
author={Mancini, Adriano and Frontoni, Emanuele and Zingaretti, Primo},
journal={IEEE Transactions on Intelligent Transportation Systems}, 
title={Mechatronic System to Help Visually Impaired Users During Walking and Running}, 
year={2018},
volume={19},
number={2},
pages={649-660},
publisher={IEEE},
abstract={Ambient assisted living and intelligent transportation systems are becoming strongly coupled. There is the necessity of improving the quality of life by developing inclusive mobility solutions for impaired people. In this paper, we focus on a monocular vision-based system to assist people during walking, jogging, and running in outdoor environments. The impaired user is guided along a path represented by a lane or line on a dedicated runway. We developed a set of image processing algorithms to extract lines/lanes to follow. The embedded system is based on a small camera and a board that is responsible for processing the images and communicating with the developed haptic device. The haptic device is formed by a set of two gloves equipped with vibration motors that drive the user to the right direction. The vibration sequences are generated according to a robotic-like controller, considering the user as a two wheel steering robot, where the rotational and translation velocity can be controlled. The results obtained show that the overall system is able to detect the right path and to provide the right stimuli to the user, by means of the gloves, up to a speed over 10 km/h.},
keywords={Navigation;Vibrations;Legged locomotion;Haptic interfaces;Visualization;Cameras;Impaired people;visual navigation;embedded systems;monocular vision},
doi={10.1109/TITS.2017.2780621},
ISSN={1558-0016},
month={Feb},}

@INPROCEEDINGS{176yang2018intersectionnav,
author={Yang, Kailun and Cheng, Ruiqi and Bergasa, Luis M. and Romera, Eduardo and Wang, Kaiwei and Long, Ningbo},
booktitle={IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
title={Intersection Perception Through Real-Time Semantic Segmentation to Assist Navigation of Visually Impaired Pedestrians}, 
year={2018},
volume={},
number={},
pages={1034-1039},
publisher={IEEE},
address={Kuala Lumpur, Malaysia},
abstract={Intersection navigation comprises one of the major ingredient of Intelligent Transportation Systems (ITS) for Visually Impaired Pedestrians (VIP), who are the most vulnerable road users that should be protected with a high priority in metropolitan areas. Robotic vision-based assistive technologies sprung up over the past few years, which focused on specific scene objects using monocular detectors or depth sensors. These dividual solutions have reached impressive detectable range and accuracy with relatively short running time, and enhanced the intersection perception to a large degree. However, simultaneously enabling all detectors incurs a long delay and becomes computationally prohibitive on wearable embedded systems. In this work, we propose to seize CNN-based per-pixel semantic segmenter to cover navigational perception needs in a unified way. This is not only critical to perceive crosswalk position (where to cross roads), traffic light signal (when to cross roads), but also to analyze the states of other pedestrians and vehicles (whether safe to cross roads). At the centroid of our unification proposal is a deep learning architecture, aspired to attain efficient and robust semantic understanding. A comprehensive variety of experiments demonstrates the advanced accuracy over state-of-art algorithms/segmenters while maintaining high inference speed on a real-world navigation assistance system.},
keywords={Navigation;Semantics;Roads;Real-time systems;Computer architecture;Robots;Detectors},
doi={10.1109/ROBIO.2018.8665211},
ISSN={},
month={Dec},}

@INPROCEEDINGS{177bruno2019robotguidedog,
author={Bruno, Diego Renan and de Assis, Marcelo Henrique and Osório, Fernando Santos},
booktitle={Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE)}, 
title={Development of a Mobile Robot: Robotic Guide Dog for Aid of Visual Disabilities in Urban Environments}, 
year={2019},
volume={},
number={},
pages={104-108},
publisher={IEEE},
address={Rio Grande, Brazil},
abstract={The general objective of this work is to develop a mobile robotic platform that is able to avoid obstacles for the benefit of visually impaired people. The idea is that this platform is a robot "guide dog" ~for visually impaired persons, and animal guide dogs like these have very high costs (on average 40 thousand dollars - Our current robotic prototype has a cost of 400 dollars), also need a long time for training (more than 1 year), being therefore inaccessible to many visually impaired persons. Our work then turns to social robotics, where we have a big problem with disabled people and who do not get freedom to move around in urban spaces without the help of a caregiver. By collecting data from ultrasound sensors, managing a possible path to the target point in an autonomous manner, using a computer vision system to recognize and treat the obstacles encountered, representing these in the form of audio to the user. The prototype was developed to serve as a platform for research in the development of navigational algorithms and computer vision systems of the automation laboratory of the (omitted for blind review). The robot presented good results in tests with navigation algorithms and computer vision that were applied to validate this platform in tests with visually impaired people in IDVC - (Catanduva Institute for the Visually Impaired).},
keywords={Robot sensing systems;Mobile robots;Computer vision;Genetic algorithms;Dogs;Deep Learning;Mobile Robotics;Vision in robotics and automation},
doi={10.1109/LARS-SBR-WRE48964.2019.00026},
ISSN={2643-685X},
month={Oct},}

@inproceedings{178limprayoon2021summon,
author = {Limprayoon, Jirachaya "Fern" and Pareek, Prithu and Tan, Xiang Zhi and Steinfeld, Aaron},
title = {Robot Trajectories When Approaching a User with a Visual Impairment},
year = {2021},
isbn = {9781450383066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441852.3476538},
doi = {10.1145/3441852.3476538},
abstract = {Mobile robots have been shown to be helpful in guiding users in complex indoor spaces. While these robots can assist all types of users, current implementations often rely on users visually rendezvousing with the robot, which may be a challenge for people with visual impairments. This paper describes a proof of concept for a robotic system that addresses this kind of short-range rendezvous for users with visual impairments. We propose to use a lattice graph-based Anytime Repairing A* (ARA*) planner as a global planner to discourage the robot from turning in place at its goal position, making its path more human-like and safer. We also interviewed an Orientation \& Mobility (O&M) Specialist for their thoughts on our planner. They observed that our planner produces less obtrusive trajectories to the user than the ROS default global planner and recommended that our system should allow the robot to approach the person from the side as opposed to the front as it currently does. In the future, we plan to test our system with users in-person to better validate our assumptions and find additional pain points.},
booktitle = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {84},
numpages = {4},
keywords = {approach trajectory, people with visual impairments, rendezvous, robot navigation},
location = {Virtual Event, United States of America},
series = {ASSETS '21}
}

@inproceedings{179hackbarth2021walker,
author = {Hackbarth, Johannes and Jacob, Caspar},
title = {Walker - An Autonomous, Interactive Walking Aid},
year = {2021},
isbn = {9781450383066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441852.3476552},
doi = {10.1145/3441852.3476552},
abstract = {In this paper, we describe ongoing work about a robotic walker-frame that was designed to aid patients in an orthopaedic rehabilitation clinic. The so-called Walker is able to autonomously drive to patients and then changes into a more traditional walking-frame, i.e. one that has to be pushed by the patient, but it can still help by giving navigation instructions. Walker was designed with a multi-modal user interface in such a way that it can also be used by visually, hearing or speaking impaired people.},
booktitle = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {96},
numpages = {3},
keywords = {assistance robots, hardware design, human-robot interaction, verbal and non-verbal},
location = {Virtual Event, United States of America},
series = {ASSETS '21}
}

@Article{180li2021imu-mag,
author={Li, Min
and Ammanabrolu, Jayanth},
title={Indoor way-finding method using IMU and magnetic tensor sensor measurements for visually impaired users},
journal={International Journal of Intelligent Robotics and Applications},
year={2021},
month={Jun},
day={01},
volume={5},
number={2},
pages={264-282},
abstract={This paper introduces a new indoor way-finding method for the visually impaired person (VIP) by utilizing the naturally-generated inertial and geomagnetic information. Reliable and accurate indoor orientation and localization are provided by newly designed sensor fusion algorithms, which take advantage of inertial and geomagnetic information and overcome the inherent problems of the naturally-generated signals, such as low signal-to-noise ratio (SNR) and high environmental sensitivity. Geomagnetic information compensates the sensor drift and accumulative error of the inertial sensors whereas the inertial sensors help to correct the orientation-related errors and drift of the magnetic fields. A parameter derived from the magnetic tensor is introduced for obstacle avoidance and object/destination approach, especially when the relatively large localization uncertainty exists. With a prototype developed based on the system design, several experiments under different indoor scenarios demonstrate that the proposed indoor-way finding method can guide the VIPs and avoid obstacles indoor efficiently and accurately.},
issn={2366-598X},
doi={10.1007/s41315-021-00163-6},
url={https://doi.org/10.1007/s41315-021-00163-6}
}

@ARTICLE{181moon2022pathprediction,
author={Moon, Hee-Seung and Seo, Jiwon},
journal={IEEE Access}, 
title={Sample-Efficient Training of Robotic Guide Using Human Path Prediction Network}, 
year={2022},
volume={10},
number={},
pages={104996-105007},
publisher={IEEE},
abstract={Training a robot that engages with people is challenging; it is expensive to directly involve people in the training process, which requires numerous data samples. This paper presents an alternative approach for resolving this problem. We propose a human path prediction network (HPPN) that generates a user’s future trajectory based on sequential robot actions and human responses using a recurrent-neural-network structure. Subsequently, an evolution-strategy-based robot training method using only the virtual human movements generated using the HPPN is presented. It is demonstrated that our proposed method permits sample-efficient training of a robotic guide for visually impaired people. By collecting only 1.5 K episodes from real users, we were able to train the HPPN and generate more than 100 K virtual episodes required for training the robot. The trained robot precisely guided blindfolded participants along a target path. Furthermore, using virtual episodes, we investigated a new reward design that prioritizes human comfort during the robot’s guidance without incurring additional costs. This sample-efficient training method is expected to be widely applicable to future robots that interact physically with humans.},
keywords={Robots;Training;Navigation;Robot vision systems;Trajectory;Human-robot interaction;Cameras;Recurrent neural networks;Blind navigation;evolution strategy;human–robot interaction;recurrent neural network;robotic guide},
doi={10.1109/ACCESS.2022.3210932},
ISSN={2169-3536},
month={},}

@Article{182gill2022tactiledisplay,
AUTHOR = {Gill, Satinder and Pawluk, Dianne T. V.},
TITLE = {Design of a “Cobot Tactile Display” for Accessing Virtual Diagrams by Blind and Visually Impaired Users},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {12},
ARTICLE-NUMBER = {4468},
NUMPAGES = {19},
URL = {https://www.mdpi.com/1424-8220/22/12/4468},
PubMedID = {35746250},
ISSN = {1424-8220},
ABSTRACT = {Access to graphical information plays a very significant role in today’s world. Access to this information can be particularly limiting for individuals who are blind or visually impaired (BVIs). In this work, we present the design of a low-cost, mobile tactile display that also provides robotic assistance/guidance using haptic virtual fixtures in a shared control paradigm to aid in tactile diagram exploration. This work is part of a larger project intended to improve the ability of BVI users to explore tactile graphics on refreshable displays (particularly exploration time and cognitive load) through the use of robotic assistance/guidance. The particular focus of this paper is to share information related to the design and development of an affordable and compact device that may serve as a solution towards this overall goal. The proposed system uses a small omni-wheeled robot base to allow for smooth and unlimited movements in the 2D plane. Sufficient position and orientation accuracy is obtained by using a low-cost dead reckoning approach that combines data from an optical mouse sensor and inertial measurement unit. A low-cost force-sensing system and an admittance control model are used to allow shared control between the Cobot and the user, with the addition of guidance/virtual fixtures to aid in diagram exploration. Preliminary semi-structured interviews, with four blind or visually impaired participants who were allowed to use the Cobot, found that the system was easy to use and potentially useful for exploring virtual diagrams tactually.},
DOI = {10.3390/s22124468},}

@INPROCEEDINGS{183kayukawa2022feelingstudy/motiv,
author={Kayukawa, Seita and Sato, Daisuke and Murata, Masayuki and Ishihara, Tatsuya and Kosugi, Akihiro and Takagi, Hironobu and Morishima, Shigeo and Asakawa, Chieko},
booktitle={31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
title={How Users, Facility Managers, and Bystanders Perceive and Accept a Navigation Robot for Visually Impaired People in Public Buildings}, 
year={2022},
volume={},
number={},
pages={546-553},
publisher={IEEE},
address={Napoli, Italy},
abstract={Autonomous navigation robots have a considerable potential to offer a new form of mobility aid to people with visual impairments. However, to deploy such robots in public buildings, it is imperative to receive acceptance from not only robot users but also people that use the buildings and managers of those facilities. Therefore, we conducted three studies to investigate the acceptance and concerns of our prototype robot, which looks like a regular suitcase. First, an online survey revealed that people could accept the robot navigating blind users. Second, in the interviews with facility managers, they were cautious about the robot’s camera and the privacy of their customers. Finally, focus group sessions with legally blind participants who experienced the robot navigation revealed that the robot may cause trouble when it collides with those who may not be aware of the user’s blindness. Still, many participants liked the design of the robot which assimilated into the surroundings.},
keywords={Privacy;Navigation;Buildings;Robot vision systems;Visual impairment;Blindness;Cameras},
doi={10.1109/RO-MAN53752.2022.9900717},
ISSN={1944-9437},
month={Aug},}

@InProceedings{184defazio2023quadrupedresponsivelocomotion,
title = 	 {Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control},
author =       {DeFazio, David and Hirota, Eisuke and Zhang, Shiqi},
booktitle = 	 {Proceedings of The 7th Conference on Robot Learning},
pages = 	 {2184--2194},
year = 	 {2023},
editor = 	 {Tan, Jie and Toussaint, Marc and Darvish, Kourosh},
volume = 	 {229},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {06--09 Nov},
publisher =    {PMLR},
address = {Atlanta, United States},
pdf = 	 {https://proceedings.mlr.press/v229/defazio23a/defazio23a.pdf},
url = 	 {https://proceedings.mlr.press/v229/defazio23a.html},
abstract = 	 {Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human.},
}

@inproceedings{185balatti2024adaptiveimpedance,
title={Robot-assisted navigation for visually impaired through adaptive impedance and path planning},
author={Balatti, Pietro and Ozdamar, Idil and Sirintuna, Doganay and Fortini, Luca and Leonori, Mattia and Gandarias, Juan M and Ajoudani, Arash},
booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
pages={2310--2316},
year={2024},
organization={IEEE},
publisher={IEEE},
address = {Yokohama, Japan},
}

@inproceedings{186agrawal2023shelfhelp,
author = {Agrawal, Shivendra and Nayak, Suresh and Naik, Ashutosh and Hayes, Bradley},
title = {ShelfHelp: Empowering Humans to Perform Vision-Independent Manipulation Tasks with a Socially Assistive Robotic Cane},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The ability to shop independently, especially in grocery stores, is important for maintaining a high quality of life. This can be particularly challenging for people with visual impairments (PVI). Stores carry thousands of products, with approximately 30,000 new products introduced each year in the US market alone, presenting a challenge even for modern computer vision solutions. Through this work, we present a proof-of-concept socially assistive robotic system we call ShelfHelp, and propose novel technical solutions for enhancing instrumented canes traditionally meant for navigation tasks with additional capability within the domain of shopping. ShelfHelp includes a novel visual product locator algorithm designed for use in grocery stores and a novel planner that autonomously issues verbal manipulation guidance commands to guide the user during product retrieval. Through a human subjects study, we show the system's success in locating and providing effective manipulation guidance to retrieve desired products with novice users. We compare two autonomous verbal guidance modes achieving comparable performance to a human assistance baseline and present encouraging findings that validate our system's efficiency and effectiveness and through positive subjective metrics including competence, intelligence, and ease of use.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1514–1523},
numpages = {10},
keywords = {Markov decision process, assistive robotics, computer vision, human-robot interaction, manipulation guidance, planner},
location = {London, UK},
series = {AAMAS '23},
}

@article{187song2023mixedreality,
title = {Mixture reality-based assistive system for visually impaired people},
journal = {Displays},
volume = {78},
pages = {102449},
year = {2023},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102449},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223000823},
author = {Jucheng Song and Jixu Wang and Shuliang Zhu and Haidong Hu and Mingliang Zhai and Jiucheng Xie and Hao Gao},
keywords = {Mixed reality, SLAM, Navigation technology, Visually impaired, Assistive system},
abstract = {Individuals with visual impairments often face challenges in their daily lives, particularly in terms of independent mobility. To address this issue, we present a mixed reality-based assistive system for visually impaired individuals, which comprises a Microsoft Hololens2 device and a website and utilizes a simultaneous localization and mapping (SLAM) algorithm to capture various large indoor scenes in real-time. This system incorporates remote multi-person assistance technology and navigation technology to aid visually impaired individuals. To evaluate the effectiveness of our system, we conducted an experiment in which several participants completed a large indoor scene maintenance task. Our experimental results demonstrate that the system is robust and can be utilized in a wide range of indoor environments. Additionally, the system enhances environmental perception and enables visually impaired individuals to navigate independently, thus facilitating successful task completion.}
}

@INPROCEEDINGS{188tan2021flyingguidedog,
author={Tan, Haobin and Chen, Chang and Luo, Xinyu and Zhang, Jiaming and Seibold, Constantin and Yang, Kailun and Stiefelhagen, Rainer},
booktitle={IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
title={Flying Guide Dog: Walkable Path Discovery for the Visually Impaired Utilizing Drones and Transformer-based Semantic Segmentation}, 
year={2021},
volume={},
number={},
pages={1123-1128},
publisher={IEEE},
address={Sanya, China},
abstract={Lacking the ability to sense ambient environments effectively, blind and visually impaired people (BVIP) face difficulty in walking outdoors, especially in urban areas. Therefore, tools for assisting BVIP are of great importance. In this paper, we propose a novel "flying guide dog" prototype for BVIP assistance using drone and street view semantic segmentation. Based on the walkable areas extracted from the segmentation prediction, the drone can adjust its movement automatically and thus lead the user to walk along the walkable path. By recognizing the color of pedestrian traffic lights, our prototype can help the user to cross a street safely. Furthermore, we introduce a new dataset named Pedestrian and Vehicle Traffic Lights (PVTL), which is dedicated to traffic light recognition. The result of our user study in real-world scenarios shows that our prototype is effective and easy to use, providing new insight into BVIP assistance.},
keywords={Legged locomotion;Semantics;Urban areas;Prototypes;Dogs;Resists;Transformers},
doi={10.1109/ROBIO54168.2021.9739520},
ISSN={},
month={Dec},}

@inproceedings{189mirsky2021grandchallenge/motiv,
author = {Mirsky, Reuth and Stone, Peter},
title = {The Seeing-Eye Robot Grand Challenge: Rethinking Automated Care},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Automated care systems are becoming more tangible than ever: recent breakthroughs in robotics and machine learning can be used to address the need for automated care created by the increasing aging population. However, such systems require overcoming several technological, ethical, and social challenges. One inspirational manifestation of these challenges can be observed in the training of seeing-eye dogs for visually impaired people. A seeing-eye dog is not just trained to obey its owner, but also to "intelligently disobey": if it is given an unsafe command from its handler, it is taught to disobey it or even insist on a different course of action. This paper proposes the challenge of building a seeing-eye robot, as a thought-provoking use-case that helps identify the challenges to be faced when creating behaviors for robot assistants in general. Through this challenge, this paper delineates the prerequisites that an automated care system will need to have in order to perform intelligent disobedience and to serve as a true agent for its handler.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {28–33},
numpages = {6},
keywords = {surrogacy, service robots, intelligent disobedience, grand challenge, automated care},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@ARTICLE{190peng2023viiahand,
author={Peng, Chunhao and Yang, Dapeng and Zhao, Deyu and Cheng, Ming and Dai, Jinghui and Jiang, Li},
journal={IEEE Robotics and Automation Letters}, 
title={Viiat-Hand: A Reach-and-Grasp Restoration System Integrating Voice Interaction, Computer Vision, Auditory and Tactile Feedback for Non-Sighted Amputees}, 
year={2024},
volume={9},
number={10},
pages={8674-8681},
abstract={For non-sighted and visually impaired (BVI) amputees, the combined loss of vision and grasping abilities turns the seemingly simple task of reaching and grasping into a significant challenge. This letter introduces a novel multi-sensory prosthesis system designed for BVI amputees to assist in perception, navigation, and grasping tasks. The system integrates voice interaction, environmental perception, grasp guidance, collaborative control, and auditory/tactile feedback. Specifically, it processes user commands, provides environmental data via auditory/tactile channels, and manages collaborative control of grasp gestures and wrist angles for stable object handling. The prototype, viiat-hand, was experimentally tested with eight non-disabled and four non-sighted subjects performing reach-and-grasp tasks, showing that users could accurately reach (average time:15.24 s) and securely grasp objects (average time:17.23 s) in an indoor setting. The system also proved to be user-friendly, requiring minimal training for users to become adept.},
keywords={Prosthetics;Cameras;Wrist;Collaboration;Task analysis;Vibrations;Navigation;Prosthetic hand;visual impairment;computer vision;auditory aid;human-machine interaction},
doi={10.1109/LRA.2024.3448218},
ISSN={2377-3766},
month={Oct},}

@article{191due2023robotvdog,
author = {Brian L. Due},
title = {Guide dog versus robot dog: assembling visually impaired people with non-human agents and achieving assisted mobility through distributed co-constructed perception},
journal = {Mobilities},
volume = {18},
number = {1},
pages = {148--166},
year = {2023},
publisher = {Routledge},
doi = {10.1080/17450101.2022.2086059},
URL = { https://doi.org/10.1080/17450101.2022.2086059 },
eprint = { https://doi.org/10.1080/17450101.2022.2086059 },
abstract = { Guide dogs are sense-able agents that can assist Visually Impaired Persons (VIP) to achieve mobility. But could a guide dog be replaced by a robot dog? Based on video recordings and ethnomethodological ‘conversation analysis’ of VIPs who are mobile in a street environment with a remotely operated robodog or a guide dog, respectively, this paper shows the multisensory and semiotic capacities of non-human agents as assistants in navigational activities. It also highlights the differences between their type of agency and sense-ability, and thus their different roles in situations of assisted mobility and disability mobility. This paper contributes to research in assisted and disability mobility between humans and non-humans by showing how they work not as individual agents, but as ‘VIP + guide dog’ and ‘VIP + robodog + operator’ assemblages, and by demonstrating that these assemblages distribute and co-construct the practical perception of the material world which is necessary for accomplishing mobility. }
}

@InProceedings{192yin2023leggedforceestimation,
author="Yin, Yunpeng
and Gao, Feng
and Xiao, Yuguang
and Yang, Limin
and Fan, Zehua",
editor="Yang, Huayong
and Liu, Honghai
and Zou, Jun
and Yin, Zhouping
and Liu, Lianqing
and Yang, Geng
and Ouyang, Xiaoping
and Wang, Zhiyong",
title="Force-Estimation Based Interaction of Legged Robots through Whole-Body Dynamics",
booktitle="Intelligent Robotics and Applications",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="506--517",
abstract="External force estimation and adaptation is an assistive technology for legged robots. It enables the robot to accommodate to impacts or interferences and maintain its balance. This technology can also be used for the interaction between the robots and humans when the estimation of force is reliant. This paper presents this new interaction idea for the legged robots in the scenario of guiding visually impaired individuals. The external force is estimated through the whole-body dynamics without any extra force sensors, which enables the robot to handle the unexpected external disturbances or user manipulations in a same manner. We first demonstrate this force-estimation based interaction on a hexapod robot, using a rigid stick handler as the interface. This interaction technology not only has the advantage of being more adaptable to the environments, it also can help the robot to adjust its motions to follow the user's intention of movements.",
isbn="978-981-99-6495-6"
}

@ARTICLE{193liao2023dronerunningguide,
author={Liao, Zhenyu and Luces, Jose V. Salazar and Ravankar, Ankit A. and Hirata, Yasuhisa},
journal={IEEE Robotics and Automation Letters}, 
title={Running Guidance for Visually Impaired People Using Sensory Augmentation Technology Based Robotic System}, 
year={2023},
volume={8},
number={9},
pages={5323-5330},
publisher={IEEE},
abstract={Participating in sports is of great significance to people's physical and mental well-being. While physical activity is commonplace for healthy individuals, it presents challenges for those with visual impairments, as they can not rely on visual cues to perceive essential information related to sports participation, such as their surroundings. Many related studies including our previous work for assisting users in doing sports using sensory augmentation technology, which couples haptic feedback with people's desired movements, are proposed for this challenge. On the basis of these studies, we propose a system for guiding visually impaired users running outdoors using a drone-based robotic system to locate a user and a track, calculate desired moving directions, and provide haptic feedback to the user. We conduct an experiment to explore how accurately people can recognize the directions conveyed by the proposed guidance method. Subjects were asked to select their felt directions on a tablet while running on a treadmill at 6.5 km/h and 7.5 km/h. The results show subjects could recognize the cued directions with an average resolution of $\mathbf {19.8^{\circ }}$ and $\mathbf {19.6^{\circ }}$ at different speeds, respectively, and there is no significant difference exist between the two speeds. In addition, we guide users in realistic running scenarios on sports tracks. Subjects in this experiment wore an eye mask to simulate the visually impaired. They were instructed to run by following the perceived directions conveyed by haptic feedback. According to the results, they could run within a specific track 81% of the time with the proposed system.},
keywords={Sports;Robot sensing systems;Haptic interfaces;Drones;Target tracking;Visualization;Vibrations;Human-centered robotics;physically assistive devices;wearable robotics},
doi={10.1109/LRA.2023.3294718},
ISSN={2377-3766},
month={Sep.},}

@INPROCEEDINGS{194liao2021runningguide,
author={Liao, Zhenyu and Salazar, Jose and Hirata, Yasuhisa},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Robotic Guidance System for Visually Impaired Users Running Outdoors Using Haptic Feedback}, 
year={2021},
volume={},
number={},
pages={8325-8331},
publisher={IEEE},
address={Prague, Czech Republic},
abstract={For the visually impaired people, some outdoor activities like running or soccer are difficult, due to not being able to clearly see the environment. Recently, multiple researchers have contributed to help the visually impaired people run outdoors using robotic systems with different types of feedback, such as auditory feedback and haptic feedback. They discovered that using robotic systems can be an effective way to guide visually impaired people while exercising outdoors. In this paper, we propose a method to guide the visually impaired people to do sports outside using a robotic system with haptic feedback, and we evaluate the feasibility of the proposed system through experiments with blindfolded users running outdoors. In the running guidance task, the position of the runner is determined from the visual feed of a drone, and haptic feedback produced on the users’ left lower leg is used to convey to the runner the directions in which to move to remain on a specific path. Additionally, we compared the performance of users under different haptic feedback modalities in the running task. The three compared modalities are: producing vibration only during the swing phase, only during stance phase or producing vibration continuously. The experimental results for a running task showed that our system enabled users to remain inside a specified track 93% of the total running time, while the ratio decreased to 79%, 77%, and 61% when receiving vibrations during only swing phase, during only stance phase, and without using any feedback respectively. We also observed users felt safer while running blindfolded by using the proposed method.},
keywords={Vibrations;Legged locomotion;Visualization;Navigation;Robot vision systems;Haptic interfaces;Task analysis},
doi={10.1109/IROS51168.2021.9636567},
ISSN={2153-0866},
month={Sep.},}

@INPROCEEDINGS{195ichikawa2022voicesystem,
author={Ichikawa, Reiya and Zhang, Bin and Lim, Hun-ok},
booktitle={8th International Symposium on System Security, Safety, and Reliability (ISSSR)}, 
title={Voice Expression System of Visual Environment for a Guide Dog Robot}, 
year={2022},
volume={},
number={},
pages={191-192},
publisher={IEEE},
address={Chongqing, China},
abstract={In this paper, a voice expression system of visual environment for a guide dog robot is proposed. This system enables the robot to recognize objects and scenes in front of the robot by CNN (Convolutional Neural Network) and generate captions of the scenes by LSTM (Long-Short-Term Memory) network. Then the robot expresses the recognized visual scene by voice, which is generated by speech synthesis. The guide dog robot can guide the visually impaired person safely to the desired destination, as well as entertain the user by expressing various visual information through voice. The system is composed of object recognition, scene caption, and speech synthesis. The effectiveness of this system is confirmed through experiments conducted with our guide dog robot.},
keywords={Visualization;Urban areas;Dogs;Speech recognition;Safety;Speech synthesis;Convolutional neural networks;Guide dog robot;Visual scene recognition;Voice expression system},
doi={10.1109/ISSSR56778.2022.00041},
ISSN={},
month={Oct},}

@inproceedings{196murai2024wearablenav,
author = {Murai, Yasuyuki and Ota, Yumiko and Tatsumi, Hisayuki},
title = {Prototyping of a walking assistance system for visually impaired people using AI and LiDAR},
year = {2024},
isbn = {9798400709111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629296.3629364},
doi = {10.1145/3629296.3629364},
abstract = {This research focuses on the development of an AI-based system designed to provide walking support for individuals with visual impairments. Depending on the specific characteristics of their disability, visually impaired individuals may encounter challenges when walking in a straight line. To address this issue, we are working on a system that incorporates a small body-worn camera to capture movement direction, identify safe walking areas from the recorded images, and guide users toward those regions. The current system in development utilizes AI to detect pedestrian position information from the camera images, establishing a "walkable area" based on this data to ensure user safety. Voice notifications are employed to communicate movement directions and other relevant information to the user. In this report, we propose integrating LiDAR technology to gather comprehensive environmental information, enhancing the safety of the walking support system. We employ Simultaneous Localization and Mapping (SLAM) techniques to create an environment map using the collected LiDAR data. The report outlines the progress made in developing a prototype system capable of providing walking support through the utilization of the environment map and the defined walkable area.},
booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers},
pages = {423–428},
numpages = {6},
keywords = {Deep Learning, LiDAR, Veering tendency, Visually impaired},
location = {Barcelona, Spain},
series = {ICETC '23}
}

@INPROCEEDINGS{197shih2018dlwv2glove,
author={Shih, Meng-Li and Chen, Yi-Chun and Tung, Chia-Yu and Sun, Cheng and Cheng, Ching-Ju and Chan, Liwei and Varadarajan, Srenivas and Sun, Min},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={DLWV2: A Deep Learning-Based Wearable Vision-System with Vibrotactile-Feedback for Visually Impaired People to Reach Objects}, 
year={2018},
volume={},
number={},
pages={1-9},
publisher={IEEE},
address={Madrid, Spain},
abstract={We develop a Deep Learning-based Wearable Vision-system with Vibrotactile-feedback (DLWV2)to guide Blind and Visually Impaired (BVI)people to reach objects. The system achieves high accuracy in object detection and tracking in 3-D using an extended deep learning-based 2.5-D detector and a 3-D object tracker with the ability to track 3-D object locations even outside the camera field-of-view. We train our detector with a large number of images with 2.5-D object ground-truth (i.e., 2-D object bounding boxes and distance from the camera to objects). A novel combination of HTC Vive Tracker with our system enables us to automatically obtain the ground-truth labels for training while requiring very little human effort to set up the system. Moreover, our system processes frames in real-time through a client-server computing platform such that BVI people can receive realtime vibrotactile guidance. We conduct a thorough user study on 12 BVI people in new environments with object instances which are unseen during training. Our system outperforms the non-assistive guiding strategy with statistic significance in both time and the number of contacting irrelevant objects. Finally, the interview with BVI users confirms that our system with distance-based vibrotactile feedback is mostly preferred, especially for objects requiring gentle manipulation such as a bottle with water inside.},
keywords={Cameras;Target tracking;Detectors;Training;Real-time systems;Object detection},
doi={10.1109/IROS.2018.8593711},
ISSN={2153-0866},
month={Oct},}

@INPROCEEDINGS{198zhu2019edge,
author={Zhu, Jinhui and Chen, Yinong and Zhang, Mei and Chen, Qiang and Guo, Ying and Min, Huaqing and Chen, Zhilie},
booktitle={IEEE 14th International Symposium on Autonomous Decentralized System (ISADS)}, 
title={An Edge Computing Platform of Guide-dog Robot for Visually Impaired}, 
year={2019},
volume={},
number={},
pages={1-7},
publisher={IEEE},
address={Utrecht, Netherlands},
abstract={Information processing of guide dog robot requires expensive computing resources to meet real-time performance. We propose an edge computing framework based on Intel Up-Squared board and neural compute stick. Image processing and real-time control are performed on the framework. In addition, the voice recognition and commanding are also implemented, which are processed on Amazon cloud service. Vision, speech, and control services are integrated by ASU VIPLE (Visual IoT/Robotics Programming Language Environment). A prototype is developed, which implements the guide dog’s obstacle avoidance, traffic sign recognition and following, and voice interaction with human.},
keywords={Dogs;Mobile robots;Robot sensing systems;Wheelchairs;Edge computing;Cameras;Guide dog robot;Visually Impaired;Edge Computing;Neural Compute Stick;Internet of thing;ASU VIPLE;Embedded system},
doi={10.1109/ISADS45777.2019.9155620},
ISSN={2640-7485},
month={April},}

@INPROCEEDINGS{199capi2012system,
author={Capi, Genci},
booktitle={IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
title={Development of a new robotic system for assisting and guiding visually impaired people}, 
year={2012},
volume={},
number={},
pages={229-234},
address={Guangzhou, China},
publisher={IEEE},
abstract={This paper presents a new intelligent robotic system to assist or guide visually impaired people in unknown indoor environments. The goal is to employ the robot public buildings such as hospitals, offices, etc. The robotic system, which is equipped with a visual sensor, laser range finders, speaker, gives visually impaired people information about the environment around them. The laser data are analyzed using the clustering technique, making it possible to detect obstacles, steps and stairs. In addition, the robot can be switched to the guide mode. By selecting the target location, the robot starts guiding the user toward the goal utilizing the line on the ground or evolved neural controllers. The PC analysis the sensors data and send information to the visually impaired people by natural language or beep signal. The usefulness of the proposed system is examined experimentally.},
keywords={},
doi={10.1109/ROBIO.2012.6490971},
ISSN={},
month={Dec},}

@INPROCEEDINGS{200ogawa2014system,
author={Ogawa, Hironori and Tobita, Kazuteru and Sagayama, Katsuyuki and Tomizuka, Masayoshi},
booktitle={IEEE Symposium on Computational Intelligence in Robotic Rehabilitation and Assistive Technologies (CIR2AT)}, 
title={A guidance robot for the visually impaired: System description and velocity reference generation}, 
year={2014},
volume={},
number={},
pages={9-15},
publisher={IEEE},
address={Orlando, Florida, United States},
abstract={This paper presents a guidance robot for the visually impaired along with generation of velocity reference signals. The guidance robot has an interface grip with a force sensor on the top. The robot can be operated intuitively through the interface grip, and at the same time, the position and the behavior of the robot can be provided to the user. An underactuated mobile mechanism is used. It can be moved in X and Y directions and is unconstrained in turning direction. The rotation center of the robot is determined by the user's position. Environmental sensors are equipped for avoiding obstacles and hazards. In addition, the robot has redundant anti-falling systems for safety. The systems work independently from the environmental sensors. The robot is lightweight, foldable, and easy to carry. A versatile time-optimal reference generation algorithm is proposed for the robot's velocity reference. The algorithm can set the bounds for the acceleration, jerk, and snap. Moreover, initial conditions and final conditions of the velocity, acceleration, and jerk can be specified. The switching surfaces are obtained with these parameters in the three-dimensional phase space, which allows the determination of the snap command for easy motion pattern generation. These parameters are changeable online.},
keywords={Robot sensing systems;Acceleration;Switches;Mobile robots;Wheels},
doi={10.1109/CIRAT.2014.7009735},
ISSN={},
month={Dec},}

@INPROCEEDINGS{201kang2001cane,
author={Sung-Jae Kang and Young Ho and In Hyuk Moon},
booktitle={Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)}, 
title={Development of an intelligent guide-stick for the blind}, 
year={2001},
volume={4},
number={},
pages={3208-3213 vol.4},
publisher={IEEE},
address={Seoul, Korea},
abstract={An intelligent guide stick for the blind was developed. It consists of an ultrasound displacement sensor, two DC motors, and a micro-controller. The total weight is 4.0kg, and the width and the height of the guide stick are 24 cm and 85 cm, respectively. Computer simulations were performed in order to find the traces of the guide stick at three different paths using an in-house Visual C/sup ++/ software. Actual experiments were also performed to compare with the computer simulation results. The difference between the actual experiment and the simulation was 1.19 cm in the straight path. However, the difference alter the first 90/spl deg/ turn was 9.3 cm and became 11.9 cm after the second 90/spl deg/ turn. Nevertheless, the intelligent guide stick followed the path of the road successfully avoiding the obstacle. The intelligent guide stick will help the blind travel with providing more convenient means of life.},
keywords={Intelligent sensors;Sensor arrays;Ultrasonic imaging;Robot sensing systems;Path planning;Roads;Shape;Acoustic sensors;Global Positioning System;Biomedical engineering},
doi={10.1109/ROBOT.2001.933112},
ISSN={1050-4729},
month={May},}

@misc{202mehrizi2021quadrupedal,
title={Quadrupedal Robotic Guide Dog with Vocal Human-Robot Interaction}, 
author={Kavan Mehrizi},
year={2021},
eprint={2111.03718},
archivePrefix={arXiv},
primaryClass={cs.HC}
}

@article{203knol1988dogneeds,
author = {B. W. Knol, C. Roozendaal, L. van den Bogaard and J. Bouw},
title = {The suitability of dogs as guide dogs for the blind: Criteria and testing procedures},
journal = {Veterinary Quarterly},
volume = {10},
number = {3},
pages = {198--204},
year = {1988},
publisher = {Taylor \& Francis},
doi = {10.1080/01652176.1988.9694171},
note ={PMID: 3051646},
URL = { https://doi.org/10.1080/01652176.1988.9694171 },
eprint = { https://doi.org/10.1080/01652176.1988.9694171 },
abstract = { Criteria and testing procedures with regard to the suitability of dogs as guide dogs for the blind were developed on the basis of a literature study and own observations. A profile of the guide dog comprising physical characteristics, skillfulness, behaviour, and obedience was drawn up. As a rule, the testing procedures concern health and skills of the dogs. In the skill test some elements of the behavioural and obedience test were included. The final evaluation is based on the results of physical examination and the skill test, unless testing of behaviour and/or obedience appears necessary as well. A method for evaluating the performances of the dogs as objectively as possible is described. Some implications of using and testing guide dogs are discussed. },}

@Article{205meliones2022ultrasonic,
AUTHOR = {Meliones, Apostolos and Filios, Costas and Llorente, Jairo},
TITLE = {Reliable Ultrasonic Obstacle Recognition for Outdoor Blind Navigation},
JOURNAL = {Technologies},
VOLUME = {10},
YEAR = {2022},
NUMBER = {3},
ARTICLE-NUMBER = {54},
NUMPAGES = {31},
URL = {https://www.mdpi.com/2227-7080/10/3/54},
ISSN = {2227-7080},
ABSTRACT = {A reliable state-of-the-art obstacle detection algorithm is proposed for a mobile application that will analyze in real time the data received by an external sonar device and decide the need to audibly warn the blind person about near field obstacles. The proposed algorithm can equip an orientation and navigation device that allows the blind person to walk safely autonomously outdoors. The smartphone application and the microelectronic external device will serve as a wearable that will help the safe outdoor navigation and guidance of blind people. The external device will collect information using an ultrasonic sensor and a GPS module. Its main objective is to detect the existence of obstacles in the path of the user and to provide information, through oral instructions, about the distance at which it is located, its size and its potential motion and to advise how it could be avoided. Subsequently, the blind can feel more confident, detecting obstacles via hearing before sensing them with the walking cane, including hazardous obstacles that cannot be sensed at the ground level. Besides presenting the micro-servo-motor ultrasonic obstacle detection algorithm, the paper also presents the external microelectronic device integrating the sonar module, the impulse noise filtering implementation, the power budget of the sonar module and the system evaluation. The presented work is an integral part of a state-of-the-art outdoor blind navigation smartphone application implemented in the MANTO project.},
DOI = {10.3390/technologies10030054},}

@ARTICLE{206hamed2020cooplocomotion,
author={Hamed, Kaveh Akbari and Kamidi, Vinay R. and Ma, Wen-Loong and Leonessa, Alexander and Ames, Aaron D.},
journal={IEEE Robotics and Automation Letters}, 
title={Hierarchical and Safe Motion Control for Cooperative Locomotion of Robotic Guide Dogs and Humans: A Hybrid Systems Approach}, 
year={2020},
volume={5},
number={1},
pages={56-63},
abstract={This letter presents a hierarchical control strategy based on hybrid systems theory, nonlinear control, and safety-critical systems to enable cooperative locomotion of robotic guide dogs and visually impaired people. We address high-dimensional and complex hybrid dynamical models that represent collaborative locomotion. At the high level of the control scheme, local and nonlinear controllers, based on the virtual constraints approach, are designed to induce exponentially stable dynamic gaits. The local controller for the leash is assumed to be a nonlinear controller that keeps the human in a safe distance from the dog while following it. At the lower level, a real-time quadratic programming (QP) is solved for modifying the local controllers of the robot as well as the leash to avoid obstacles. In particular, the QP framework is set up based on control barrier functions (CBFs) to compute optimal control inputs that guarantee safety while being close to the local controllers. The stability of the complex periodic gaits is investigated through the Poincaré return map. To demonstrate the power of the analytical foundation, the control algorithms are transferred into an extensive numerical simulation of a complex model that represents cooperative locomotion of a quadrupedal robot, referred to as Vision 60, and a human model. The complex model has 16 continuous-time domains with 60 state variables and 20 control inputs.},
keywords={Dogs;Legged locomotion;Robot kinematics;Safety;Adaptation models;Legged robots;motion control;dynamics},
doi={10.1109/LRA.2019.2939719},
ISSN={2377-3766},
month={Jan},}

@INPROCEEDINGS{207sivacoumare2021assistant,
author={Sivacoumare, Ajaykumaar and S, Sheethal Maria and Satheesh, Sarga and Athul, T and V, Manikandan and Vinopraba, T.},
booktitle={IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society}, 
title={AI Quadruped Robot Assistant for the Visually Impaired}, 
year={2021},
volume={},
number={},
pages={1-5},
address={Toronto, Ontario, Canada},
publisher={IEEE},
abstract={Legged robots are more versatile than wheeled robots and offer several advantages- for one, they can traverse complex terrain. However, they are expensive and more complex than their wheeled counterparts. This paper proposes a low-cost prototype developed using a spider-type acrylic chassis, Raspberry Pi 4B, Pi camera module, and ESP32 microcontroller. The robot is equipped with an object detector, a face detector and recognizer, a speech recognizer, a Chat-bot and offers different Human-Machine Interface (HMI) methods; suited to assist the visually impaired. For efficient operation and coordination of the programs, the Robot Operating System (ROS) has been used. Hence, users can interact with the robot’s object detector and face recognizer through speech or text. The Raspberry Pi is used for running the programs, and ESP32 is dedicated to controlling the robot.},
keywords={Legged locomotion;Service robots;Microcontrollers;Face recognition;Robot kinematics;Robot vision systems;Speech recognition;face recognition;object detection;quadruped robot;robot operating system;voice recognition},
doi={10.1109/IECON48115.2021.9589508},
ISSN={2577-1647},
month={Oct},}

@INPROCEEDINGS{208wei2013smartrope,
author={Wei, Yuanlong and Kou, Xiangxin and Lee, Min Cheol},
booktitle={IEEE/ASME International Conference on Advanced Intelligent Mechatronics}, 
title={Smart rope and vision based guide-dog robot system for the visually impaired self-walking in urban system}, 
year={2013},
volume={},
number={},
pages={698-703},
address={Wollongong, Australia},
publisher={IEEE},
abstract={This paper presents a development of guide-dog robot system for visually impaired. Based on a hall-sensor joystick and ultrasonic sensors, a “smart rope” system is designed for the human-robot interaction. In this system, multiple functions are provided for the self-walking in urban system, such as following, navigation and obstacle avoidance. To distinguish between small involuntary force and the intended navigational movement, a fuzzy logic control method is applied to improve the accuracy for the “smart rope” system manipulation. To compensate the lack of visual sense of visually impaired, a smart phone with camera is utilized as the robot vision, in order to detect the traffic lights and the zebra crossing. A fast vision recognition approach is provided based on Adaboosting and Template matching combined algorithm. For the evaluation of proposed method, an integrated system is implemented to the mobile robot platform. The performance of both interactive system and vision system are analyzed after the experiment in the urban environment. System's accuracy, usefulness and adaptability are verified. The experimental results showed that this new designed guide-dog robot system is suitable and effective to assist the visually impaired for the self-walking.},
keywords={Robot sensing systems;Acoustics;Mobile robots;Force;Fuzzy logic;Visually impaired;Guide-dog robot;Hall-sensor joystick;ultrasonic sensor;Fuzzy logic control;Adaboosting;Template matching;Traffic lights;Zebra crossing},
doi={10.1109/AIM.2013.6584174},
ISSN={2159-6255},
month={July},}

@INPROCEEDINGS{209chang2020zebracrossing,
author={Chang, Wan-Jung and Su, Jian-Ping and Chen, Liang-Bi and Chen, Ming-Che and Hsu, Chia-Hao and Yang, Ching-Hsiang and Sie, Cheng-You and Chuang, Cheng-Hsin},
booktitle={IEEE International Conference on Consumer Electronics (ICCE)}, 
title={An AI Edge Computing Based Wearable Assistive Device for Visually Impaired People Zebra-Crossing Walking}, 
year={2020},
volume={},
number={},
pages={1-2},
publisher={IEEE},
address={Las Vegas, United States of America},
abstract={This paper proposes an artificial intelligence (AI) edge computing-based wearable assistive device for assisting zebra-crossing walking of the visually impaired people. The proposed assistive device is composed of a pair of smart glasses, a walking cane, and a waist-mounted box. When the visually impaired pedestrian prepares to cross the zebra-crossing, thus the visually impaired pedestrian will receive the current traffic light signal message. The visually impaired guidance voice service will be provided via Bluetooth (BT). Moreover, when the walking offset occurs on the zebra-crossing, then the visually impaired pedestrian will be reminded through the voice prompt of the earphone.},
keywords={Legged locomotion;Headphones;Deep learning;Roads;Conferences;Assistive devices;Artificial intelligence},
doi={10.1109/ICCE46568.2020.9043132},
ISSN={2158-4001},
month={Jan},}

@inproceedings{211yatani2012spacesense,
author = {Yatani, Koji and Banovic, Nikola and Truong, Khai},
title = {SpaceSense: representing geographical information to visually impaired people using spatial tactile feedback},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2207734},
doi = {10.1145/2207676.2207734},
abstract = {Learning an environment can be challenging for people with visual impairments. Braille maps allow their users to understand the spatial relationship between a set of places. However, physical Braille maps are often costly, may not always cover an area of interest with sufficient detail, and might not present up-to-date information. We built a handheld system for representing geographical information called SpaceSense, which includes custom spatial tactile feedback hardware-multiple vibration motors attached to different locations on a mobile touch-screen device. It offers high-level information about the distance and direction towards a destination and bookmarked places through vibrotactile feedback to help the user maintain the spatial relationships between these points. SpaceSense also adapts a summarization technique for online user reviews of public and commercial venues. Our user study shows that participants could build and maintain the spatial relationships between places on a map more accurately with SpaceSense compared to a system without spatial tactile feedback. They pointed specifically to having spatial tactile feedback as the contributing factor in successfully building and maintaining their mental map.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {415–424},
numpages = {10},
keywords = {vibrotactile feedback, users with visual impairments, touch screens, handheld devices, geographical information representation, assistive technology},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@Article{212velazquez2018outdoornav,
AUTHOR = {Velázquez, Ramiro and Pissaloux, Edwige and Rodrigo, Pedro and Carrasco, Miguel and Giannoccaro, Nicola Ivan and Lay-Ekuakille, Aimé},
TITLE = {An Outdoor Navigation System for Blind Pedestrians Using GPS and Tactile-Foot Feedback},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {578},
NUMPAGES = {15},
URL = {https://www.mdpi.com/2076-3417/8/4/578},
ISSN = {2076-3417},
ABSTRACT = {This paper presents a novel, wearable navigation system for visually impaired and blind pedestrians that combines a global positioning system (GPS) for user outdoor localization and tactile-foot stimulation for information presentation. Real-time GPS data provided by a smartphone are processed by dedicated navigation software to determine the directions to a destination. Navigational directions are then encoded as vibrations and conveyed to the user via a tactile display that inserts into the shoe. The experimental results showed that users were capable of recognizing with high accuracy the tactile feedback provided to their feet. The preliminary tests conducted in outdoor locations involved two blind users who were guided along 380–420 m predetermined pathways, while sharing the space with other pedestrians and facing typical urban obstacles. The subjects successfully reached the target destinations. The results suggest that the proposed system enhances independent, safe navigation of blind pedestrians and show the potential of tactile-foot stimulation in assistive devices.},
DOI = {10.3390/app8040578}
}

@INPROCEEDINGS{219muhammad2012soundnav,
author={Muhammad, M. S. and Thoo, Y. W. and Masra, S. M. W.},
booktitle={IEEE Colloquium on Humanities, Science and Engineering (CHUSER)}, 
title={Sound navigation aid system for the vision impaired}, 
year={2012},
volume={},
number={},
pages={288-293},
publisher={IEEE},
address={Kinabalu, Malaysia},
abstract={Visually impaired or blind people are someone who lost their ability to see. They usually depend on guide cane or guide dog to assist them navigates their environment. However, guide cane requires the blind to scan the surrounding manually with their hand while guide dog is expensive and has a short life span. There are many researches on creating better navigation aid for blind people. Instead of using vibration or alarm to notify the distance of the obstacles, this research proposed the application of sound notification for the blind. This system is developed using MATLAB r2010a with the integrated webcam of the laptop and a laser pointer. The laser pointer acts as a reference point so that object's distance can be calculated from the captured image by the webcam. This system manages to calculate distance for any color of object under any level of brightness condition during day and night time. It also possesses human head movement ability to turn left and right, up and down. This system managed to detect the obstacles up to a distance of 432 centimeters, which is sufficient to prevent the blind from bumping into the obstacles. The calculated distance is then converted to sound in order to notify the blind how far the object is away from them.},
keywords={blind;navigation aid;range finder;sound imaging},
doi={10.1109/CHUSER.2012.6504326},
ISSN={},
month={Dec},}

@article{220fernando2023routeplanning/survey,
author = {Nimalika Fernando, David A. McMeekin and Iain Murray},
title = {Route planning methods in indoor navigation tools for vision impaired persons: a systematic review},
journal = {Disability and Rehabilitation: Assistive Technology},
volume = {18},
number = {6},
pages = {763--782},
year = {2023},
publisher = {Taylor \& Francis},
doi = {10.1080/17483107.2021.1922522},
note ={PMID: 34043928},
URL ={https://doi.org/10.1080/17483107.2021.1922522},
eprint = {https://doi.org/10.1080/17483107.2021.1922522},
abstract = { Route planning is key support, which can be provided by navigation tools for the blind and visually impaired (BVI) persons. No comprehensive analysis has been reported in the literature on this topic. The main objective of this study is to examine route planning approaches used by indoor navigation tools for BVI persons with respect to the route determination criteria, how user context is integrated, algorithms adopted, and the representation of the environment for route planning. A systematic review was conducted covering the period 1982–July 2018 and thirteen databases. From the 1709 articles that resulted from the initial search, 131 were selected for the study. Route length was the sole factor used to determine the best route in the majority of the studies. Routes with less obstacles, less turns, more landmarks and close to walls were selected by the other studies. User variations were considered in few studies. Unique needs of the BVI persons were addressed by few novel algorithms which had integrated multiple parameters and BVI-friendly route modelling. Differences between the navigation capabilities of the sighted and the BVI community were not a major concern when deciding the optimum routes in the majority of BVI indoor navigation tools. How to trade-off between factors affecting optimum routes, and how to model suitable routes in complex buildings needs to be studied further, looking from the user perspective.Implications for rehabilitationNavigation differences between sighted and blind and vision-impaired (BVI) communities are not concerned frequently when planning routes in indoor navigation tools of BVI persons.Selecting routes avoiding areas difficult to traverse, close to walls, having landmarks, and less turns are some approaches used to address the unique needs of the BVI community.Assisting for recovery from veering and real-time obstacle detouring are useful features offered by these tools.Identifying and prioritizing different factors contributing to better routes, concerning user variations, and adopting multi-objective route optimization will help to develop improved route planning methodologies for BVI indoor navigation tool Navigation differences between sighted and blind and vision-impaired (BVI) communities are not concerned frequently when planning routes in indoor navigation tools of BVI persons. Selecting routes avoiding areas difficult to traverse, close to walls, having landmarks, and less turns are some approaches used to address the unique needs of the BVI community. Assisting for recovery from veering and real-time obstacle detouring are useful features offered by these tools. Identifying and prioritizing different factors contributing to better routes, concerning user variations, and adopting multi-objective route optimization will help to develop improved route planning methodologies for BVI indoor navigation tool }
}

@inproceedings{221zhao2019arstairnav,
author = {Zhao, Yuhang and Kupferstein, Elizabeth and Castro, Brenda Veronica and Feiner, Steven and Azenkot, Shiri},
title = {Designing AR Visualizations to Facilitate Stair Navigation for People with Low Vision},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347906},
doi = {10.1145/3332165.3347906},
abstract = {Navigating stairs is a dangerous mobility challenge for people with low vision, who have a visual impairment that falls short of blindness. Prior research contributed systems for stair navigation that provide audio or tactile feedback, but people with low vision have usable vision and don't typically use nonvisual aids. We conducted the first exploration of augmented reality (AR) visualizations to facilitate stair navigation for people with low vision. We designed visualizations for a projection-based AR platform and smartglasses, considering the different characteristics of these platforms. For projection-based AR, we designed visual highlights that are projected directly on the stairs. In contrast, for smartglasses that have a limited vertical field of view, we designed visualizations that indicate the user's position on the stairs, without directly augmenting the stairs themselves. We evaluated our visualizations on each platform with 12 people with low vision, finding that the visualizations for projection-based AR increased participants' walking speed. Our designs on both platforms largely increased participants' self-reported psychological security.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {387–402},
numpages = {16},
keywords = {visualization, low vision, augmented reality, accessibility},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@Article{222angelopoulos2019arnavdepth,
author={Angelopoulos, Anastasios Nikolas
and Ameri, Hossein
and Mitra, Debbie
and Humayun, Mark},
title={Enhanced Depth Navigation Through Augmented Reality Depth Mapping in Patients with Low Vision},
journal={Scientific Reports},
year={2019},
month={Aug},
day={02},
volume={9},
number={1},
pages={11230},
abstract={Patients diagnosed with Retinitis Pigmentosa (RP) show, in the advanced stage of the disease, severely restricted peripheral vision causing poor mobility and decline in quality of life. This vision loss causes difficulty identifying obstacles and their relative distances. Thus, RP patients use mobility aids such as canes to navigate, especially in dark environments. A number of high-tech visual aids using virtual reality (VR) and sensory substitution have been developed to support or supplant traditional visual aids. These have not achieved widespread use because they are difficult to use or block off residual vision. This paper presents a unique depth to high-contrast pseudocolor mapping overlay developed and tested on a Microsoft Hololens 1 as a low vision aid for RP patients. A single-masked and randomized trial of the AR pseudocolor low vision aid to evaluate real world mobility and near obstacle avoidance was conducted consisting of 10 RP subjects. An FDA-validated functional obstacle course and a custom-made grasping setup were used. The use of the AR visual aid reduced collisions by 50{\%} in mobility testing (p{\thinspace}={\thinspace}0.02), and by 70{\%} in grasp testing (p{\thinspace}={\thinspace}0.03). This paper introduces a new technique, the pseudocolor wireframe, and reports the first significant statistics showing improvements for the population of RP patients with mobility and grasp.},
issn={2045-2322},
doi={10.1038/s41598-019-47397-w},
url={https://doi.org/10.1038/s41598-019-47397-w}
}

@inproceedings{223gamage2023assistivedesire/survey,
author = {Gamage, Bhanuka and Do, Thanh-Toan and Price, Nicholas Seow Chiang and Lowery, Arthur and Marriott, Kim},
title = {What do Blind and Low-Vision People Really Want from Assistive Smart Devices? Comparison of the Literature with a Focus Study},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608955},
doi = {10.1145/3597638.3608955},
abstract = {Over the last decade there has been considerable research into how artificial intelligence (AI), specifically computer vision, can assist people who are blind or have low-vision (BLV) to understand their environment. However, there has been almost no research into whether the tasks (object detection, image captioning, text recognition etc.) and devices (smartphones, smart-glasses etc.) investigated by researchers align with the needs and preferences of BLV people. We identified 646 studies published in the last two and a half years that have investigated such assistive AI techniques. We analysed these papers to determine the task, device and participation by BLV individuals. We then interviewed 24 BLV people and asked for their top five AI-based applications and to rank the applications found in the literature. We found only a weak positive correlation between BLV participants’ perceived importance of tasks and researchers’ focus and that participants prefer conversational agent interface and head-mounted devices.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {30},
numpages = {21},
keywords = {augmented reality, mobile application, recognition, smart devices, virtual reality, wearable},
location = {New York, New York, United States of America},
series = {ASSETS '23}
}

@INPROCEEDINGS{224gui2019point-to-tell,
author={Gui, Wenjun and Li, Bingyu and Yuan, Shuaihang and Rizzo, John-Ross and Sharma, Lakshay and Feng, Chen and Tzes, Anthony and Fang, Yi},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={An assistive low-vision platform that augments spatial cognition through proprioceptive guidance: Point-to-Tell-and-Touch}, 
year={2019},
volume={},
number={},
pages={3817-3822},
publisher={IEEE},
address={Macau, China},
abstract={Spatial cognition, as gained through the sense of vision, is one of the most important capabilities of human beings. However, for the visually impaired (VI), lack of this perceptual capability poses great challenges in their life. Therefore, we have designed Point-to-Tell-and-Touch, a wearable system with an ergonomic human-machine interface, for assisting the VI with active environmental exploration, with a particular focus on spatial intelligence and navigation to objects of interest in an alien environment. Our key idea is to link visual signals, as decoded synthetically, to the VI’s proprioception for more intelligible guidance, in addition to vision-to-audio assistance, i.e., finger pose, as indicated by pointing, is used as “proprioceptive laser pointer” to target an object in that line of sight. The whole system consists of two features, Point-to-Tell and Point-to-Touch, both of which can work independently or cooperatively. The Point-to-Tell feature contains a camera with a novel one-stage neural network tailored for blind-centered object detection and recognition, and a headphone telling the VI the semantic label and distance from the pointed object. the Point-to-Touch, the second feature, leverages a vibrating wrist band to create a haptic feedback tool that supplements the initial vectorial guidance provided by the first stage (hand pose being direction and the distance being the extent, offered through audio cues). Both platform features utilize proprioception or joint position sense. Through hand pose, the VI end user knows where he or she is pointing relative to their egocentric coordinate system and we are able to use this foundation to build spatial intelligence. Our successful indoor experiments demonstrate the proposed system to be effective and reliable in helping the VI gain spatial cognition and explore the world in a more intuitive way.},
keywords={},
doi={10.1109/IROS40897.2019.8967647},
ISSN={2153-0866},
month={Nov},}

@PhdThesis{225kundu2019monocularnav,
author={Kundu, Rupam},
title={A Single Camera based Localization and Navigation Assistance for The Visually Impaired in Indoor Environments},
year={2019},
publisher={The Ohio State University},
school={The Ohio State University},
keywords={Indoor navigation, Indoor tracking, Blind people navigation, Visually impaired navigation assistance},
abstract={Detecting the presence of objects for spatial orientation and avoiding them for mobility is extremely difficult for People with  Visual Disabilities. 285 million people worldwide are estimated to  have a visual disability, of which 39 million are blind and another  246 million have low vision. Many day-to-day tasks that the rest of  the population consider routine, are in fact excessively complex for  these individuals to perform. It is difficult to have a perception of  space and to be able to navigate both indoors and outdoors safely,  confidently and effectively without visual information about the  surroundings. The use of smartphones is popular among the people with  visual disabilities for simple navigational assistance such as  finding the current location, and reading out directions which  typically leverage the availability of GPS in outdoor environments.  But when it comes to indoor environments there is a deficit of such  GPS-based accurate location services and this makes it more  challenging to localize themselves with respect to the environment  and navigate from one location to another. Existing solutions rely on  the user to scan the environment (e.g., Ultracane, Smartcane) and  build and maintain a mental model of the environment. Additionally,  due to their heavy computational needs and power-hungry sensors  (e.g., stereo camera, lidar and radar), existing solutions require  significant amount of energy leading to short usable periods between  chargings unless heavy battery packs are carried. I present two  solutions. The first one is a lightweight single camera-based  solution for Localization {\&}amp; Navigation (VisualLoc) and the second  is an inexpensive, power-efficient depth-mapping solution  (CaneScanner) for assisting Visually Impaired in Indoor  Environments. Together these two tools help to achieve the  following:  1) find one{\&}{\#}x2019;s relative position (distance {\&}amp;  direction) with respect to the objects in the environment and  subsequently keeping track of these self-to-object spatial  relationship as they transform during locomotion; and,  2) sense  objects within user{\&}{\#}x2019;s vicinity to efficiently, thereby helping  them to navigate, safely and effectively from one{\&}{\#}x2019;s  position  to another desired position by avoiding the obstacles. VisualLoc  provides positioning and tracking services by using a mobile smart  camera such as in google glasses or smartphones which can be attached  to the body of a visually impaired individual.  Existing Radio  Frequency (RF) or Visual Light Communication (VLC) based indoor  tracking solutions can provide location and orientation only when  there are dense deployments of Access Points (APs) or VLC bulbs  (anchor points) in the user{\&}{\#}x2019;s field of view. VisualLoc has   two components: (1) Vision-fix: It finds the absolute coordinates and  orientation of the user{\&}apos;s camera leveraging Visual Light  Communication bulbs. (2) Vision-track: It tracks the coordinates and  orientation of the user{\&}apos;s camera when no anchor points is in  line of sight. VisualLoc deployed in an indoor college building  provides a median localization accuracy of 28 cm for Vision-fix and  49 cm for Vision-track. VisualLoc is the first implementation which  is designed from the perspective of sparse deployment. CaneScanner is designed to find the depth-map of an indoor  environment using a white cane, the most widely accepted tool by the  visually impaired community,  equipped with a single camera and  inertial sensors.  CaneScanner has three components: (i) an algorithm  for providing tracking services, (ii) a robust depth mapping  algorithm for creating accurate 3D depth-map, (iii) a power saving  mode to optimize the camera usage and overall processing cost of  CaneScanner. CaneScanner is the first work which addresses the low  power, long range depth sensing needs of people with visual  disabilities. The overarching goal of this thesis is to enable   PVDs to be better equipped to navigate and interact with their  immediate surroundings using technological breakthroughs in the  design of lightweight, power-efficient and inexpensive solutions.},
url={http://rave.ohiolink.edu/etdc/view?acc_num=osu154593040067708}
}

@INPROCEEDINGS{226wise2012status/survey,
author={Wise, Elyse and Li, Binghao and Gallagher, Thomas and Dempster, Andrew G. and Rizos, Chris and Ramsey-Stewart, Euan and Woo, Daniel},
booktitle={International Conference on Indoor Positioning and Indoor Navigation (IPIN)}, 
title={Indoor navigation for the blind and vision impaired: Where are we and where are we going?}, 
year={2012},
volume={},
number={},
pages={1-7},
publisher={IEEE},
address={Sydney, NSW, Australia},
abstract={Despite over a decade of intensive research and development, the problem of delivering an effective indoor navigation system to the blind and vision impaired (BVI) remains largely unsolved. In an attempt to strengthen and improve future research efforts, we define a set of criteria for evaluating the success of a potential navigation device. In order to give complete coverage, the Requirements Analysis has been broken down into the subcategories of positioning accuracy, robustness, seamlessness of integration with varying environments and the nature of information that is outputted to a BVI user. We then apply this framework to a number of existing navigation solutions for the BVI, drawing upon the notable achievements that have been made thus far and also the crucial issues that remain unresolved or are yet to receive attention. It was found that these key issues, which existing designs fail to overcome, can be attributed to the need for a new focus and user centred design attitude - one which incorporates universal design, recognises the uniqueness of its audience and understands the challenges associated with the systems/devices intended environment.},
keywords={Navigation;Robustness;Accuracy;Complexity theory;indoor navigation;blind vision impaired;human-computer interaction;user requirements;universal design;location information},
doi={10.1109/IPIN.2012.6418894},
ISSN={},
month={Nov},}

@inproceedings{227nicolau2009blobby,
author = {Nicolau, Hugo and Jorge, Joaquim and Guerreiro, Tiago},
title = {Blobby: how to guide a blind person},
year = {2009},
isbn = {9781605582474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1520340.1520541},
doi = {10.1145/1520340.1520541},
abstract = {For the majority of blind people, walking in unknown places is a very difficult, or even impossible, task to perform, when without help. The adoption of the white cane is the main aid to a blind user's mobility. However, the major difficulties arise in the orientation task. The lack of reference points and the inability to access visual cues are its main causes. We aim to overcome this issue allowing users to walk through unknown places, by receiving a familiar and easily understandable feedback. Our preliminary contributions are in understanding, through user studies, how blind users explore an unknown place, their difficulties, capabilities and needs. We also analyzed how these users create their own mental maps, verbalize a route and communicate with each other. Structuring and generalizing this information, we were able to create a prototype that generates familiar and adequate instructions, behaving like a blind companion, one with similar capabilities that understands his "friend" and speaks the same language. We evaluated the system with the target population, validating our approach and orientation guidelines, while gathering overall user satisfaction.},
booktitle = {CHI '09 Extended Abstracts on Human Factors in Computing Systems},
pages = {3601–3606},
numpages = {6},
keywords = {orientation, mobile, instructions, familiar, evaluation, blind, accessibility},
location = {Boston, Massachusetts, United States of America},
series = {CHI EA '09}
}

@inproceedings{228kulyukin2008blindleading,
author = {Kulyukin, Vladimir and Nicholson, John and Ross, David and Marston, James and Gaunet, Florence},
year = {2008},
month = {01},
pages = {54-59},
title = {The Blind Leading the Blind: Toward Collaborative Online Route Information Management by Individuals with Visual Impairments.},
booktitle={Association for the Advancement of Artificial Intelligence (AAAI) Spring Symposium: Social Information Processing},
publisher={AAAI},
address={Palo Alto, California, United States},
url={https://cdn.aaai.org/Symposia/Spring/2008/SS-08-06/SS08-06-011.pdf},
}

@article{229schinazi2016congenital,
author = {Schinazi, Victor R. and Thrash, Tyler and Chebat, Daniel-Robert},
title = {Spatial navigation by congenitally blind individuals},
journal = {WIREs Cognitive Science},
volume = {7},
number = {1},
pages = {37-58},
doi = {https://doi.org/10.1002/wcs.1375},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1375},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.1375},
abstract = {Spatial navigation in the absence of vision has been investigated from a variety of perspectives and disciplines. These different approaches have progressed our understanding of spatial knowledge acquisition by blind individuals, including their abilities, strategies, and corresponding mental representations. In this review, we propose a framework for investigating differences in spatial knowledge acquisition by blind and sighted people consisting of three longitudinal models (i.e., convergent, cumulative, and persistent). Recent advances in neuroscience and technological devices have provided novel insights into the different neural mechanisms underlying spatial navigation by blind and sighted people and the potential for functional reorganization. Despite these advances, there is still a lack of consensus regarding the extent to which locomotion and wayfinding depend on amodal spatial representations. This challenge largely stems from methodological limitations such as heterogeneity in the blind population and terminological ambiguity related to the concept of cognitive maps. Coupled with an over-reliance on potential technological solutions, the field has diffused into theoretical and applied branches that do not always communicate. Here, we review research on navigation by congenitally blind individuals with an emphasis on behavioral and neuroscientific evidence, as well as the potential of technological assistance. Throughout the article, we emphasize the need to disentangle strategy choice and performance when discussing the navigation abilities of the blind population. WIREs Cogn Sci 2016, 7:37–58. doi: 10.1002/wcs.1375 This article is categorized under: Psychology > Learning Neuroscience > Cognition Neuroscience > Plasticity},
year = {2016},
}

@PhdThesis{230folska2012in-blind-sight,
author={Folska, Claudia L.},
editor={Lee, Yuk},
title={In blind sight: Wayfinding in the absence of vision},
series={ProQuest Dissertations and Theses},
year={2012},
publisher={University of Colorado at Denver},
address={United States -- Colorado},
pages={657},
school={University of Colorado at Denver},
keywords={Social sciences; Communication and the arts; Psychology; Architecture; Blind; Design; Neuroplasticity; Planning; Wayfinding; Cognitive psychology; Urban planning; 0633:Cognitive psychology; 0999:Urban planning; 0729:Architecture},
abstract={This exploratory research investigated real-world human navigation and the role of neural plasticity in the absence of vision. The University of Colorado at Denver and Health Sciences Human Research Subjects Committee and the Internal Review Board both approved this research;  and informed, written consent was given by each participant prior to his or her interview. Thirty-nine blind participants were interviewed and asked to draw sketch maps of a routine route at the Colorado Center for the Blind. The findings of this research were striking. All participants drew very similar maps regardless of their onset of blindness. Also, the results of this investigation suggest a preference for relying on tactile modality for extracting environmental information, rather than relying on auditory information. A Multi-Sensory Cognitive Efficiency Model is proposed to explain the multidimensional approach to extracting data from the built environment in the absence of vision. Finally, findings from this research are applicable to land-use development surrounding Transit-Oriented Developments.},
note={Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works. Last updated - 2023-03-04. Ph.D. 3522738},
isbn={978-1-267-55052-1},
url={https://media.proquest.com/media/hms/ORIG/2/ryKBK?_a=ChgyMDI0MDUyODA1MDMyNDMyNjo5MjQ5NjUSBTk1OTc5GgpPTkVfU0VBUkNIIg4xNDkuMTAyLjI1NC4xNyoFMTg3NTAyCjEwMzc4MTMwODA6DURvY3VtZW50SW1hZ2VCATBSBk9ubGluZVoCRlRiA1BGVGoAcgB6AIIBJVAtMTAxMDI2OC0xNDU1My1DVVNUT01FUi1udWxsLTcwOTY5MTKSAQZPbmxpbmXKAXVNb3ppbGxhLzUuMCAoTWFjaW50b3NoOyBJbnRlbCBNYWMgT1MgWCAxMF8xNV83KSBBcHBsZVdlYktpdC81MzcuMzYgKEtIVE1MLCBsaWtlIEdlY2tvKSBDaHJvbWUvMTI1LjAuMC4wIFNhZmFyaS81MzcuMzbSARZEaXNzZXJ0YXRpb25zICYgVGhlc2VzmgIHUHJlUGFpZKoCK09TOkVNUy1NZWRpYUxpbmtzU2VydmljZS1nZXRNZWRpYVVybEZvckl0ZW3KAhNEaXNzZXJ0YXRpb24vVGhlc2lz0gIBWfICAPoCAU6CAwNXZWKKAxxDSUQ6MjAyNDA1MjgwNTAyMzg1MjI6NjY3MzI0&_s=KvoX2%2B5Pnf%2FCjSWJ7exFcA18o68%3D},
language={English},
}

@InProceedings{231emerson2017accessingbuilt,
author="Wall Emerson, Robert",
editor="Antona, Margherita
and Stephanidis, Constantine",
title="Outdoor Wayfinding and Navigation for People Who Are Blind: Accessing the Built Environment",
booktitle="Universal Access in Human--Computer Interaction. Human and Technological Environments",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="320--334",
abstract="People who are blind use a range of tools and training to optimize their travel in the built environment. However, changes in the built environment have brought new challenges for blind mobility. Developments in technology, whether used by the person who is blind or designed within the built environment offer ways to overcome mobility challenges. Technology used by a person who is blind includes GPS units, ultrasonic detectors, and smartphone applications that offer travel system information. Technology designed to increase accessibility to the built environment includes accessible pedestrian signals, smart paint, talking signs, autonomous vehicles, integrated travel systems, and devices that communicate between the pedestrian and the built environment.",
isbn="978-3-319-58700-4"
}

@techreport{232swobodzinski2022pedestrian,
title={Pedestrian Wayfinding Under Consideration of Visual Impairment, Blindness, and Deafblindness: A Mixed-Method Investigation Into Individual Experiences and Supporting Elements},
author={Swobodzinski, Martin and Parker, Amy T and Schaller, Elizabeth and Snow, Denise},
year={2022},
url={https://archives.pdx.edu/ds/psu/38772},
doi={10.15760/trec.282},
location={Portland, Oregon},
address={Portland, Oregon},
institution = {Portland State University},
}

@inproceedings{233hwang2024handler-dog-interaction,
author = {Hwang, Hochul and Jung, Hee-Tae and Giudice, Nicholas A and Biswas, Joydeep and Lee, Sunghoon Ivan and Kim, Donghyun},
title = {Towards Robotic Companions: Understanding Handler-Guide Dog Interactions for Informed Guide Dog Robot Design},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642181},
doi = {10.1145/3613904.3642181},
abstract = {Dog guides are favored by blind and low-vision (BLV) individuals for their ability to enhance independence and confidence by reducing safety concerns and increasing navigation efficiency compared to traditional mobility aids. However, only a relatively small proportion of BLV individuals work with dog guides due to their limited availability and associated maintenance responsibilities. There is considerable recent interest in addressing this challenge by developing legged guide dog robots. This study was designed to determine critical aspects of the handler-guide dog interaction and better understand handler needs to inform guide dog robot development. We conducted semi-structured interviews and observation sessions with 23 dog guide handlers and 5 trainers. Thematic analysis revealed critical limitations in guide dog work, desired personalization in handler-guide dog interaction, and important perspectives on future guide dog robots. Grounded on these findings, we discuss pivotal design insights for guide dog robots aimed for adoption within the BLV community.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {596},
numpages = {20},
keywords = {Accessibility, Individuals with Disabilities \& Assistive Technologies, Interview, Robot},
location = {Honolulu, Hawaii, United States of Amerca},
series = {CHI '24}
}

@article{234passini1988info,
author = {R. Passini and J. Delisle and C. Langlois and G. Proulx},
title ={Wayfinding Information for Congenitally Blind Individuals},
journal = {Journal of Visual Impairment \& Blindness},
volume = {82},
number = {10},
pages = {425-429},
year = {1988},
doi = {10.1177/0145482X8808201008},
URL = { https://doi.org/10.1177/0145482X8808201008 },
eprint = { https://doi.org/10.1177/0145482X8808201008 },
abstract = { The paper reports on a wayfinding study aimed at identifying the information needs of the congenitally totally blind population. A route-finding experiment in a complex architectural setting was undertaken with a group of 15 congenitally totally blind and a matched control group of 15 sighted subjects. The experiment showed that, compared to the sighted control group, the blind persons planned the journey in more detail, requiring for this purpose additional environmental information. During the journey, they formulated significantly more decisions and used significantly more units of information than the sighted control. Furthermore, the nature of the information used and its source were also different for the two groups. A cognitive mapping exercise, on the other hand, showed the blind to perform virtually as well as the sighted person. },
}

@article{235passini1988without-vision,
author = {Romedi Passini and Guyltne Proulx},
title ={Wayfinding without Vision: An Experiment with Congenitally Totally Blind People},
journal = {Environment and Behavior},
volume = {20},
number = {2},
pages = {227-252},
year = {1988},
doi = {10.1177/0013916588202006},
URL = { https://doi.org/10.1177/0013916588202006 },
eprint = { https://doi.org/10.1177/0013916588202006 },
abstract = { In a wayfinding experiment a group of 15 congenitally totally blind subjects and a matched control group of 15 sighted subjects were guided through a complex architectural setting. After two guided tours they had to make the journey on their own. An analysis of their decision making showed that the blind subjects tended to prepare theirjourney in more detail, that they made significantly more decisions during the journey, and relied on significantly more units of information. The nature of the decisions made and the information used by the two groups showed some interesting differences: About half of the decisions made by the visually impaired group were not part of the sighted group's repertory; furthermore, certain environmental features important to one group were totally ignored by the other. According to a cognitive mapping exercise and a posttest questionnaire, a third of the congenitally totally blind subjects were not only capable of representing the route taken without errors, but demonstrated as well a general understanding of the spatial attributes of the setting. Although the blind group as a whole tended to make more errors, the overall performance compares favorable with the sighted control group. The findings are discussed in terms of wayfinding theory and design implications. }
}

@InProceedings{236silva2022nav,
author="Silva, Malki-{\c{c}}edheq Benjamim C.
and Bispo, B. C.
and Silva, C. M.
and Cunha, N. A.
and Santos, E. A. B.
and Rodrigues, M. A. B.",
editor="Bastos-Filho, Teodiano Freire
and de Oliveira Caldeira, Eliete Maria
and Frizera-Neto, Anselmo",
title="Assisted Navigation System for the Visually Impaired",
booktitle="XXVII Brazilian Congress on Biomedical Engineering",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="1439--1444",
abstract="Visual impairment can be congenital, or acquired, and is classified as low vision or blindness. In both cases, the person passes through daily difficulties, such as the ability to recognize objects and people, which implies insecurity and mobility problems. Several assistive technologies have been developed over the years (walking sticks, guided dogs, and electronic devices that assist in mobility) aiming to contribute to the integration of the visually impaired into society and guarantee more independence. This work proposes a wearable echolocation system for obstacle detection through wireless sensing modules, which uses a microcontroller with information that allows determining the distance between obstacles and the user that is wearing the device. Moreover, it provides the user with tactile and auditory feedback related to this distance, enabling him to perceive the approach of objects or living beings, even without the ability to interpret visual signals. For this purpose, an experimental protocol was developed, which was carried out by visually impaired volunteers, with the approval of the ethics committee for studies with human beings (CAEE: 09844219.9.0000.5208), to assess usability and validate the prototype. The validation of the equipment was done through the statistical analysis of the results obtained from questionnaires applied to the volunteers submitted to the experiments. The results showed that 83.33{\%} of the participants said they were at least satisfied with the mobility assistance provided by the prototype and 66.67{\%} were at least satisfied with the confidence in the mobility promoted by the navigation assistant system developed in the first use of the equipment. The developed prototype was validated and its applicability as a navigation assistant for the visually impaired was verified. Ensuring easy use and freedom of movement, enabling improvement in the mobility of the individual.",
isbn="978-3-030-70601-2"
}

@InProceedings{237chaudary2017teleguide,
author="Chaudary, Babar
and Paajala, Iikka
and Keino, Eliud
and Pulli, Petri",
editor="Giokas, Kostas
and Bokor, Laszlo
and Hopfgartner, Frank",
title="Tele-guidance Based Navigation System for the Visually Impaired and Blind Persons",
booktitle="eHealth 360{\textdegree}",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="9--16",
abstract="The design and development of tele-assistance services have taken a great consideration in the domain of healthcare lately. With the growing proportion of dependent people (ageing, disabled users) in the society, tele-assistance and tele-monitoring platforms will play a significant role to provide an efficient and economical remote care. It will allow aged or disabled persons to maintain their independence and lessen the burden and cost of care by caregivers. The concept of proposed tele-guidance system is based on the idea that a blind pedestrian can be assisted by spoken instructions from a remote caregiver who receives a live video stream from a camera carried by the visually impaired persons (VIP). The ICT based assistive tools have acceptance issues by visually impaired persons. It is important while designing navigation tools for the VIP to keep in view the factors that restrain them from the adoption of assistive technology. This paper presents a tele-guidance based navigation assistance system for the VIP and blind persons and reports a qualitative study about attitude of VIP towards technological navigation assistance.",
isbn="978-3-319-49655-9"
}

@Article{238ishikawa2021spatial,
author={Ishikawa, Toru},
title={Spatial thinking, cognitive mapping, and spatial awareness},
journal={Cognitive Processing},
year={2021},
month={Sep},
day={01},
volume={22},
number={1},
pages={89-96},
abstract={This article looks at wayfinding and spatial orientation as important everyday spatial thinking skills and discusses why some people have difficulty with the skills and how one can assist people with difficulty in navigation. It first clarifies the characteristics of human spatial cognition and behavior and the tendency of spatial knowledge to be distorted and fragmented in the environment. In particular, it emphasizes the existence of large individual differences in the skill of cognitive mapping, namely the accuracy of metric and configurational understanding of the environment. The article then looks at difficulties associated with the use of maps and description of spatial relations. Given these difficulties, the article discusses the possibilities of assisting people with mobile navigation tools and improving the skill of cognitive mapping by training in spatial orientation. Implications for the development of user-adapted and context-aware navigation assistance and the significance of research from an individual differences perspective are finally discussed.},
issn={1612-4790},
doi={10.1007/s10339-021-01046-1},
url={https://doi.org/10.1007/s10339-021-01046-1},
}

@Article{239theodorou2023insight,
author={Theodorou, Paraskevi
and Meliones, Apostolos},
title={Gaining insight for the design, development, deployment and distribution of assistive navigation systems for blind and visually impaired people through a detailed user requirements elicitation},
journal={Universal Access in the Information Society},
year={2023},
month={Aug},
day={01},
volume={22},
number={3},
pages={841-867},
abstract={The autonomy, independence, productivity and, in general, quality of life of people with visual impairments often rely significantly on their ability to use new assistive technologies. In particular, their ability to navigate by foot, use means of transport and visit indoor spaces may be greatly enhanced by the use of assistive navigation systems. In this paper, a detailed analysis of user needs and requirements is presented concerning the design and development of assisting navigation systems for blind and visually impaired people (BVIs). To this end, the elicited user needs and requirements from interviews with the BVIs are processed and classified into seven main categories. Interestingly, one of the categories represents the requirements of the BVIs to be trained on the use of the mobile apps that would be included in an assistive navigation system. The need of the BVIs to be confident in their ability to safely use the apps revealed the requirement that training versions of the apps should be available. These versions would be able to simulate real-world conditions during the training process. The requirements elicitation and classification reported in this paper aim to offer insight into the design, development, deployment and distribution of assistive navigation systems for the BVIs.},
issn={1615-5297},
doi={10.1007/s10209-022-00885-9},
url={https://doi.org/10.1007/s10209-022-00885-9}
}

@Article{240wheeler2020personalized,
author={Wheeler, Bradley
and Syzdykbayev, Meirman
and Karimi, Hassan A.
and Gurewitsch, Raanan
and Wang, Yanbo},
title={Personalized accessible wayfinding for people with disabilities through standards and open geospatial platforms in smart cities},
journal={Open Geospatial Data, Software and Standards},
year={2020},
month={Jun},
day={16},
volume={5},
number={1},
pages={2},
abstract={Of the many features that smart cities offer, safe and comfortable mobility of pedestrians within the built environment is of particular importance. Safe and comfortable mobility requires that the built environments of smart cities be accessible to all pedestrians, mobility abled and mobility impaired, given their various mobility needs and preferences. This, coupled with advanced technologies such as wayfinding applications, pedestrians can get assistance in finding the best pathways at different locations and times. Wayfinding applications comprise two components, a database component containing accessibility data, and appropriate algorithms that can utilize accessibility data to meet the mobility needs and preferences of all individuals. While wayfinding applications that provide accessibility on both permanent (e.g., steps) and temporary (e.g., snow) pathways are becoming available, there is a gap in current solutions. There are two elements in the gap, one is that the accessibility data used for finding accessible pathways for people with disabilities are not compliant to the widely agreed upon and available standards, another is that the accessibility data are not available in free and open platforms so that they can be used by developers to develop personalized wayfinding applications and services. To fill this gap, in this paper, we propose a new extension in CityGML with accessibility data. We demonstrate the benefits of the new extension by testing various route options within a city. These route options clearly show the differences between commonly (shortest and fastest) requested and produced pathways and accessible pathways that are feasible and preferred by people who are mobility impaired, such as wheelchair users.},
issn={2363-7501},
doi={10.1186/s40965-020-00075-5},
url={https://doi.org/10.1186/s40965-020-00075-5},
}

@INPROCEEDINGS{241apostolopoulos2012integratedlocalization,
author={Apostolopoulos, Ilias and Fallah, Navid and Folmer, Eelke and Bekris, Kostas E.},
booktitle={IEEE International Conference on Robotics and Automation}, 
title={Integrated online localization and navigation for people with visual impairments using smart phones}, 
year={2012},
volume={},
number={},
pages={1322-1329},
publisher={IEEE},
address={Saint Paul, Minnesota, USA},
abstract={Indoor localization and navigation systems for individuals with visual impairments (VI) typically rely upon extensive augmentation of the physical space or heavy, expensive sensors; thus, few systems have been adopted. This work describes a system able to guide people with VI through buildings using inexpensive sensors, such as accelerometers, which are available in portable devices like smart phones. The method takes advantage of feedback from the human user, who confirms the presence of landmarks. The system calculates the user's location in real time and uses it to provide audio instructions on how to reach the desired destination. Previous work suggested that the accuracy of the approach depended on the type of directions and the availability of an appropriate transition model for the user. A critical parameter for the transition model is the user's step length. The current work investigates different schemes for automatically computing the user's step length and reducing the dependency of the approach to the definition of an accurate transition model. Furthermore, the direction provision method is able to use the localization estimate and adapt to failed executions of paths by the users. Experiments are presented that evaluate the accuracy of the overall integrated system, which is executed online on a smart phone. Both people with visual impairments, as well as blindfolded sighted people, participated in the experiments. The experiments included paths along multiple floors, that required the use of stairs and elevators.},
keywords={Sensors;Estimation;Smart phones;Tiles;Navigation;Visualization},
doi={10.1109/ICRA.2012.6225093},
ISSN={1050-4729},
month={May},}

@inproceedings{242fiannaca2014headlock,
author = {Fiannaca, Alexander and Apostolopoulous, Ilias and Folmer, Eelke},
title = {Headlock: a wearable navigation aid that helps blind cane users traverse large open spaces},
year = {2014},
isbn = {9781450327206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661334.2661453},
doi = {10.1145/2661334.2661453},
abstract = {Traversing large open spaces is a challenging task for blind cane users, as such spaces are often devoid of tactile features that can be followed. Consequently, in such spaces cane users may veer from their intended paths. Wearable devices have great potential for assistive applications for users who are blind as they typically feature a camera and support hands and eye free interaction. We present HEADLOCK; a navigation aid for an optical head-mounted display that helps blind users traverse large open spaces by letting them lock onto a salient landmark across the space, such as a door, and then providing audio feedback to guide the user towards the landmark. A user study with 8 blind users evaluated the usability and effectiveness of two types of audio feedback (sonification and text-to-speech) for guiding a user across an open space to a doorway. Qualitative results are reported, which may inform the design of assistive wearable technology for users who are blind.},
booktitle = {Proceedings of the 16th International ACM SIGACCESS Conference on Computers \& Accessibility},
pages = {19–26},
numpages = {8},
keywords = {head-mounted display, mobility, navigation, sonification., veering, visual impairment, wearable computing},
location = {Rochester, New York, USA},
series = {ASSETS '14}
}

@misc{243wahab2011smart,
title={Smart Cane: Assistive Cane for Visually-impaired People}, 
author={Mohd Helmy Abd Wahab and Amirul A. Talib and Herdawatie A. Kadir and Ayob Johari and A. Noraziah and Roslina M. Sidek and Ariffin A. Mutalib},
year={2011},
eprint={1110.5156},
archivePrefix={arXiv},
primaryClass={cs.SY}
}

@INPROCEEDINGS{244zhang2015slam,
author={Zhang, Xiaochen and Li, Bing and Joseph, Samleo L. and Xiao, Jizhong and Sun, Yi and Tian, Yingli and Muñoz, J. Pablo and Yi, Chucai},
booktitle={IEEE International Conference on Systems, Man, and Cybernetics}, 
title={A SLAM Based Semantic Indoor Navigation System for Visually Impaired Users}, 
year={2015},
volume={},
number={},
pages={1458-1463},
address={Hong Kong, China},
publisher={IEEE},
abstract={This paper proposes a novel assistive navigation system based on simultaneous localization and mapping (SLAM) and semantic path planning to help visually impaired users navigate in indoor environments. The system integrates multiple wearable sensors and feedback devices including a RGB-D sensor and an inertial measurement unit (IMU) on the waist, a head mounted camera, a microphone and an earplug/speaker. We develop a visual odometry algorithm based on RGB-D data to estimate the user's position and orientation, and refine the orientation error using the IMU. We employ the head mounted camera to recognize the door numbers and the RGB-D sensor to detect major landmarks such as corridor corners. By matching the detected landmarks against the corresponding features on the digitalized floor map, the system localizes the user, and provides verbal instruction to guide the user to the desired destination. The software modules of our system are implemented in Robotics Operating System (ROS). The prototype of the proposed assistive navigation system is evaluated by blindfolded sight persons. The field tests confirm the feasibility of the proposed algorithms and the system prototype.},
keywords={Floors;Navigation;Semantics;Visualization;Simultaneous localization and mapping;Cameras;assistive navigation;semantic path planning;SLAM;wearable device},
doi={10.1109/SMC.2015.258},
ISSN={},
month={Oct},}

@ARTICLE{245bamdad2024slamvip/survey,
author={Bamdad, Marziyeh and Scaramuzza, Davide and Darvishy, Alireza},
journal={IEEE Access}, 
title={SLAM for Visually Impaired People: A Survey}, 
year={2024},
volume={12},
number={},
pages={130165-130211},
abstract={In recent decades, several assistive technologies have been developed to improve the ability of blind and visually impaired (BVI) individuals to navigate independently and safely. At the same time, simultaneous localization and mapping (SLAM) techniques have become sufficiently robust and efficient to be adopted in developing these assistive technologies. We present the first systematic literature review of 54 recent studies on SLAM-based solutions for blind and visually impaired people, focusing on literature published from 2017 onward. This review explores various localization and mapping techniques employed in this context. We systematically identified and categorized diverse SLAM approaches and analyzed their localization and mapping techniques, sensor types, computing resources, and machine-learning methods. We discuss the advantages and limitations of these techniques for blind and visually impaired navigation. Moreover, we examine the major challenges described across studies, including practical challenges and considerations that affect usability and adoption. Our analysis also evaluates the effectiveness of these SLAM-based solutions in real-world scenarios and user satisfaction, providing insights into their practical impact on BVI mobility. The insights derived from this review identify critical gaps and opportunities for future research activities, particularly in addressing the challenges presented by dynamic and complex environments. We explain how SLAM technology offers the potential to improve the ability of visually impaired individuals to navigate effectively. Finally, we present future opportunities and challenges in this domain.},
keywords={Navigation;Simultaneous localization and mapping;Planning;Surveys;Object recognition;Location awareness;Systematic literature review;Visual impairment;Navigation;SLAM;systematic literature review;visually impaired},
doi={10.1109/ACCESS.2024.3454571},
ISSN={2169-3536},
month={},}

@ARTICLE{durrantwhyte2006slam,
author={Durrant-Whyte, H. and Bailey, T.},
journal={IEEE Robotics \& Automation Magazine}, 
title={Simultaneous localization and mapping: part I}, 
year={2006},
volume={13},
number={2},
pages={99-110},
keywords={Simultaneous localization and mapping;Mobile robots;Robotics and automation;History;Artificial intelligence;Navigation;Vehicles;Buildings;Bayesian methods;Particle filters},
doi={10.1109/MRA.2006.1638022}}

@inproceedings{246kyrarini2023robotspwvi/survey,
author = {Kyrarini, Maria and Zand, Manizheh and Kodur, Krisha},
title = {Assistive Robots for Persons with Visual Impairments: Current Research and Open Challenges},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596593},
doi = {10.1145/3594806.3596593},
abstract = {Robots have the potential to support persons with different abilities, including individuals with blindness or visual impairments (BVI). This paper aims to investigate the current developments in robotic research that focus on BVI robotic users and identifying what are the open challenges. This paper addresses the following research questions; (1) What research has been performed in the past two years on robotic systems that help persons with visual impairments? and (2) What are the challenges the assistive robots for persons with visual impairments face in real-world environments? Answering these questions will bring us a step closer to inclusive and accessible robots.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {413–416},
numpages = {4},
keywords = {accessibility, blind, human-robot interaction, robotics, robots, visually impaired},
location = {Corfu, Greece},
series = {PETRA '23}
}

@INPROCEEDINGS{247massiceti2024explainingclip,
author={Massiceti, Daniela and Longden, Camilla and Słowik, Agnieszka and Wills, Samuel and Grayson, Martin and Morrison, Cecily},
booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Explaining CLIP's Performance Disparities on Data from Blind/Low Vision Users}, 
year={2024},
volume={},
number={},
pages={12172-12182},
publisher={IEEE},
address={Seattle},
location={Seattle, WA, USA},
abstract={Large multi-modal models (LMMs) hold the potential to usher in a new era of automated visual assistance for people who are blind or low vision (BLV). Yet, these models have not been systematically evaluated on data captured by BLV users. We address this by empirically assessing CLIP, a widely-used LMM likely to underpin many assistive technologies. Testing 25 CLIP variants in a zero-shot classification task, we find that their accuracy is 15 percentage points lower on average for images captured by BLV users than web-crawled images. This disparity stems from CLIP's sensitivities to 1) image content (e.g. not recognizing disability objects as well as other objects); 2) image quality (e.g. not being robust to lighting variation); and 3) text content (e.g. not recognizing objects described by tactile adjectives as well as visual ones). We delve deeper with a textual analysis of three common pre-training datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability con-tent is rarely mentioned. We then provide three examples that illustrate how the performance disparities extend to three downstream models underpinned by CLIP: OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5 images can mitigate CLIP's quality-of-service disparities for BLV users in some scenarios, which we discuss alongside a set of other possible mitigations.},
keywords={Image quality;Visualization;Image recognition;Sensitivity;Text recognition;Prevention and mitigation;Lighting;CLIP;vision-language;blind;low vision;zero-shot classification;fairness;accessibility;multi-modal},
doi={10.1109/CVPR52733.2024.01157},
ISSN={2575-7075},
month={June}
}

@article{248sheffield2022many,
title={How many Braille readers? Policy, politics, and perception},
author={Sheffield, Rebecca M and D'Andrea, Frances M and Morash, Valerie and Chatfield, Sarah},
journal={Journal of Visual Impairment \& Blindness},
volume={116},
number={1},
pages={14--25},
year={2022},
publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
